<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION ü¶Å Part III - Chunkwise Parallel from of LION | Arshia Afzal </title> <meta name="author" content="Arshia Afzal"> <meta name="description" content="Arshia Afzal personal page. "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION ü¶Å Part III - Chunkwise Parallel from of LION",
            "description": "",
            "published": "May 31, 2024",
            "authors": [
              
              {
                "author": "Arshia Afzal",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Elias Abad Rocamora",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Leyla Naz Candogan",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Pol Puigdemont Plana",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Tonin",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yongtao Wu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Volkan Cevher",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "All authors are with EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arshia</span> Afzal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION ü¶Å Part III - Chunkwise Parallel from of LION</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#lion-chunk">LION-Chunk</a> </div> <ul> <li> <a href="#lion-d-chunk">LION-D Chunk</a> </li> <li> <a href="#lion-s-chunk">LION-S Chunk</a> </li> </ul> </nav> </d-contents> <ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li>Part III - Chunkwise Parallel from of LION</li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - Results</a></li> </ol> <p>Since we have now established the LION theorem, which maps full linear attention into a bidirectional RNN in <a href="/blog/2024/mamba2-part2-theory/">Part II</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet and GLA, as well as the SSD algorithm in Mamba2, a chunkwise parallel form of full linear attention could be an effective solution. Additionally, in models like Hydra, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking full linear attention, particularly for LION-S and LION-D, where the decay factor is fixed and the mask $\mathbf{M}$ follows a Toeplitz structure? But it is important that this chunkwise form is particularly useful for <strong>inference</strong>, since during training, as we said, full linear attention will provide the highest throughput, especially for short sequences, which is the case for bidirectional tasks. The aim of chunking full attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does not require chunking during training, unlike SSMs such as Hydra.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>Chunking Full Linear Attention is simpler than for causal Linear Attention since there is no intra-chunk. Considering the chunks for queries, keys and values as $\mathbf{Q}<em>{[i]} , \mathbf{K}</em>{[i]} , \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}$ with chunk size being $C$ and total number of $N=\frac{L}{C}$ chunks, we can chunk the full Linear Attention as:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \quad \mathbf{Y}_{[i]} = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. The chunk hidden states $\mathbf{C}<em>{[ij]}$ and $\mathbf{S}</em>{[ij]}$ iterate over $j$, with the final output for chunk (i) computed using their last values at $j = N$. The chunk mask $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full attention mask from, defined as:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Now that this has been stated and proven, we will describe how to construct the <strong>chunkwise mask</strong> from the attention mask $\mathbf{M}$, particularly for the <strong>fixed</strong> and <strong>selective</strong> masks in our framework. The chunkwise form of the mask for chunk of $i$ and $j$ is annotated as $\mathbf{M}_{[ij]}$.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <p>For the fixed mask, we have:</p> \[\begin{aligned} \mathbf{M}_{[ij]} = \begin{cases} \lambda^{|i-j|} (\frac{1}{\mathbf{L}}\hspace{1mm}\mathbf{L}^\top)&amp; \text{if } i&gt;j, \\ \lambda^{|i-j|} (\mathbf{L}\hspace{1mm}\frac{1}{\mathbf{L}^\top}) &amp; \text{if } i&lt;j, \\ \mathbf{\Gamma} &amp; \text{if } i = j, \end{cases} \quad \mathbf{L}_i = \lambda^i, \quad \mathbf{\Gamma}_{ij} = \lambda^{|i-j|} \end{aligned}\] <table> <tbody> <tr> <td>with $\mathbf{L} \in \mathbb{R}^C$ and $\mathbf{\Gamma} \in \mathbb{R}^{C\times C}$ being the vector and matrix used for creating the mask $\mathbf{M}<em>{[ij]}$ and they are only depending on the decay parameter $\lambda$ and the chunk size $C$. For the fixed decay mask, a simpler case of the general selective mask, $\mathbf{L}^F$ and $\mathbf{L}^B$ are identical and simplify to $\mathbf{L}_i = \lambda^i$. Since the full mask follows $\mathbf{M}</em>{ij} = \lambda^{</td> <td>i-j</td> <td>}$, the chunkwise mask for $i, j$ can be written as:</td> </tr> </tbody> </table> \[\mathbf{M}_{[ij]} = \mathbf{L}_{[i]} \frac{1}{\mathbf{L}_{[j]}} = \lambda^{|i-j|} \mathbf{L}_{[0]} \frac{1}{\mathbf{L}_{[0]}}.\] <p>Similarly, for the upper triangular part:</p> \[\mathbf{M}_{[ij]} = \lambda^{|i-j|} \frac{1}{\mathbf{L}^\top_{[0]}} \mathbf{L}_{[0]}.\] <table> <tbody> <tr> <td>For diagonal chunks, the mask remains a fixed matrix $\mathbf{\Gamma} \in \mathbb{R}^{C \times C}$, where $\mathbf{\Gamma}_{ij} = \lambda^{</td> <td>i-j</td> <td>}$, representing a smaller version of the full fixed decay mask $\mathbf{M} \in \mathbb{R}^{L \times L}$ with $\mathbf{M}_{ij} = \lambda^{</td> <td>i-j</td> <td>}$.</td> </tr> </tbody> </table> <h3 id="lion-s-chunk">LION-S Chunk</h3> <p>We first partition the SSM (semiseparable) matrix into blocks of size $\mathtt{Q} \times \mathtt{Q}$. Then, we use the properties of semiseparable matrices to factorize each off-diagonal block, which is low rank.</p> <ol> <li>(<em>Orange</em>) Each diagonal block is a smaller semiseparable matrix; we can compute this multiplication however we like; in particular, using the quadratic (attention-like) form of SSD.</li> <li>(<em>Green</em>) There are only $\mathtt{T} / \mathtt{Q}$ total different green blocks because many of them are shared. These can be computed with a batched matmul.</li> <li>(<em>Yellow</em>) Notice that the yellow terms themselves form a 1-semiseparable matrix; in other words, this step is equivalently to an SSM scan (on some modified $A$ factors)!</li> <li>(<em>Blue</em>) Similar to green, these can be computed with a batched matmul.</li> </ol> <h3 id="ssd-algorithm-chunking-and-state-passing">SSD Algorithm: Chunking and State Passing</h3> <p>An alternative interpretation of the algorithm involves reasoning about how the SSM operates on the actual sequence. We first split the sequence of input into blocks (or chunks) of size $\mathtt{Q}$. The steps then have the interpretation</p> <ol> <li> <strong>Intra-chunk outputs</strong>: compute the local output of each chunk (<em>what is the output per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li> <strong>Chunk states</strong>: compute the final state of each chunk (<em>what is the final state per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li> <strong>Pass states</strong>: compute a recurrence on all of the chunks‚Äô final states ‚Äì using any desired algorithm, e.g. parallel or sequential scan (<em>what is the actual final state per chunk taking into account all previous inputs?</em>)</li> <li> <strong>Output states</strong>: for each chunk, given its true initial state (computed in Step 3), compute the contribution to the output just from the initial state</li> </ol> <p>Either way, we see that most of the algorithm (Step 1, 2, and 4) leverages matmuls (and hence tensor cores), and also can be computed completely in parallel! Only Step 3 requires a scan, but it operates on a much shorter sequence and usually only takes a small fraction of the time of the full algorithm.</p> <h3 id="special-cases">Special Cases</h3> <p>We note that special cases of this algorithm have been seen before. In particular RetNet<d-cite key="sun2023retentive"></d-cite>, which we showed in Part II to be a special case of SSD, mention a ‚Äúchunkwise‚Äù algorithm which computes the quadratic form on a chunk of the input one-at-a-time and passes the final state to the next chunk. This turns out to be essentially equivalent to the SSD algorithm specialized to a restricted case (i.e. a decay matrix mask $L$). Our derivation comes from a different direction‚Äîthe block matrix decomposition‚Äîwhich also makes it more obvious how to parallelize this algorithm and make it really fast in practice.</p> <p>Other forms of ‚Äúchunkwise‚Äù recurrences have recently become popular, such as in <a href="https://arxiv.org/abs/2312.06635" rel="external nofollow noopener" target="_blank">Gated Linear Attention (GLA)</a><d-cite key="yang2024gated"></d-cite>.</p> <h2 id="the-code">The Code</h2> <p>In the ‚ÄúMinimal SSD‚Äù code that we provide in the paper and the <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py" rel="external nofollow noopener" target="_blank">code release</a>, we delineate each of these four steps. As promised, this algorithm is not only faster but also much easier to implement than the original selective scan of Mamba, coming in at just around 25 lines of code!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span>
</code></pre></div></div> <h2 id="the-details">The Details</h2> <p>Let‚Äôs talk about a couple of additional details in the implementation (these don‚Äôt even appear in the full paper, so pay attention!) that unpack some of the choices in this reference code.</p> <h3 id="the-ssm-scan">The SSM Scan</h3> <p>In the above code, we utilized the connection between scalar SSM recurrences</p> \[h_{t+1} = A_t h_t + B_t x_t\] <p>and matrix multiplication by 1-semiseparable matrices</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix}\] <p>which we covered in Part II (and Section 3.2.2 of the paper). In this minimal implementation, we compute Step 3 of the algorithm, which is computing a scalar SSM by <em>any</em> algorithm of our choice, by explicitly materializing a 1-SS matrix and doing dense matrix multiplication.</p> <p>We use this version for several reasons:</p> <ol> <li>Code-wise, it‚Äôs simpler to materialize and multiply by this matrix than to actually implement a parallel associative scan</li> <li>Because of the block decomposition of the SSM matrix, the sequence length $\mathtt{T}$ is reduced by a factor of $\approx 100$ ‚Äì so doing the scan in time $O(\mathtt{T}^2)$ instead of $O(\mathtt{T})$ isn‚Äôt too bad</li> <li>We have to materialize a 1-SS matrix anyways for Step 1 of the algorithm (the diagonal blocks), so might as well reuse the code ¬Ø\_(„ÉÑ)_/¬Ø</li> </ol> <p>While this example code is simpler and reasonably efficient on GPU (and probably TPU as well!), it‚Äôs no longer truly linear at long sequences. Our more optimized Triton implementation does replace the 1-SS multiplication in Step 3 with an actual associative scan.</p> <h3 id="stability">Stability</h3> <h4 id="attempt-1-ratios-of-cumprods">Attempt 1: Ratios of cumprods</h4> <p>The first naive attempt may be to notice that the entries of this matrix are cumulative products</p> \[a_{i:j}^\times = a_i \times \cdots \times a_{j-1} = \frac{a_{i:\mathtt{T}}^\times}{a_{j:\mathtt{T}}^\times}\] <p>However, this runs into severe numerical issues because these products can get really tiny (imagine $a_t \approx 0.9$ and powering it up for a sequence length $\mathtt{T}$ in the thousands!)</p> <h4 id="fix-1-the-segment-sum-segsum-operation">Fix 1: The Segment Sum (<code class="language-plaintext highlighter-rouge">segsum</code>) Operation</h4> <p>The second attempt would be to do all of this in log-space, because all the $a_t$ are positive; so the products become additions, and instead of <code class="language-plaintext highlighter-rouge">cumprod</code>s to deal with we have <code class="language-plaintext highlighter-rouge">cumsum</code>s instead. Then in order to compute the 1-SS matrix, we just have to compute the sums $\log a_i + \dots + \log a_{j-1}$ for every <em>segment</em> $[i:j]$. We call this the <strong>segment sum (segsum)</strong> primitive, analogous to cumulative sum (cumsum).</p> <h4 id="attempt-2-differences-of-cumsums">Attempt 2: Differences of cumsums</h4> <p>The obvious way to do this again is using the same idea as above, but in log space</p> \[a_{i:j}^\times = \exp\left( \log a_i + \cdots + \log a_{j-1} \right) = \left( (\log a)_{i:\mathtt{T}}^+ - (\log a)_{j:\mathtt{T}}^+ \right)\] <p>where we compute a single cumulative sum of $a$ along the time axis, and then compute all pairwise differences. In code, we can do this with</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum_unstable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>
</code></pre></div></div> <p>(and then the 1-semiseparable matrix is just the exponential of this output).</p> <p>Sums/differences are a lot more stable than products/quotients, so this should work ‚Äì right?</p> <h4 id="fix-2-remove-all-subtractions">Fix 2: Remove All Subtractions</h4> <p>Unfortunately, it turns out this still doesn‚Äôt work. The values of this 1-SS matrix roughly represent the SSM dynamics, which are very sensitive to these values of $a_t$, so we have to be very precise. And even in log space, these cumsums can be fairly large, which runs into <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation" rel="external nofollow noopener" target="_blank">catastrophic cancellation</a> when subtracted. So we really have to find a way to compute this matrix with only additions, while still vectorizing everything‚Ä¶</p> <h4 id="attempt-3-stable-segsum">Attempt 3: Stable Segsum</h4> <p>This leads to the helper function in the reference SSD code. Instead of computing a single cumsum and then subtracting, we find a way to use a batch of independent cumsums that immediately produces the right answer without subtraction.</p> <p>These details do matter! Without the right implementation of these primitives, the basic SSD algorithm produces NaNs immediately during training (even with FP32).</p> <h3 id="discretization">Discretization</h3> <p>This lineage of structured state space models developed from <a href="https://arxiv.org/abs/2111.00396" rel="external nofollow noopener" target="_blank">S4</a> and <a href="https://arxiv.org/abs/2110.13985" rel="external nofollow noopener" target="_blank">its</a> <a href="https://arxiv.org/abs/2008.07669" rel="external nofollow noopener" target="_blank">predecessors</a> which were viewed as continuous-time systems.<d-cite key="gu2023thesis"></d-cite><d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2021combining"></d-cite><d-cite key="gu2020hippo"></d-cite></p> <p>In Mamba, however, we don‚Äôt really view the SSM as continuous anymore. In fact, as mentioned in the Discussion (Section 5) of the <a href="https://arxiv.org/abs/2312.00752" rel="external nofollow noopener" target="_blank">original paper</a>, Mamba trades off with S4 on modeling different types of data:</p> <ul> <li>S4 is a continuous-time model that excels at modeling continuous data, e.g. perceptual signals such as audio waveforms and pixel-level vision.</li> <li>Mamba is a discrete-time model that excels at modeling discrete data, e.g. tokenized data such as language.</li> </ul> <p>However, the parameterization of Mamba still used the same discretization step as in prior structured SSMs, where there is another parameter $\Delta$ being modeled. We do this because the discretization step has other side effects such as properly normalizing the activations <d-cite key="gu2023train"></d-cite><d-cite key="orvieto2023resurrecting"></d-cite> which is important for performance.</p> <p>The initializations and parameterizations from the previous <a href="https://arxiv.org/abs/2206.12037" rel="external nofollow noopener" target="_blank">theory on structured SSMs</a> still work out-of-the-box, so why fix what‚Äôs not broken?</p> <p>Despite this, we‚Äôre pretty sure that the discretization step isn‚Äôt really necessary for Mamba. In the Mamba-2 paper, we chose to work directly with the ‚Äúdiscrete parameters‚Äù $A$ and $B$, which in all previous structured SSM papers (including Mamba-1) were denoted $(\bar{A}, \bar{B})$ and defined through an additional transformation</p> \[\begin{align*} \bar{A} &amp;= \exp(e^{\Delta A}) \\ \bar{B} &amp;= (\exp(e^{\Delta A}) - I) A^{-1} B \end{align*}\] <p>This doesn‚Äôt pose any problems: to use the continuous SSM parameterization, simply transform the parameters through the above formulas before plugging into the SSD code above.</p> <p>In the full Mamba-2 code, we also kept the same parameterization and discretization step as in Mamba‚Äîagain, why fix what‚Äôs not broken?‚Äîbut hypothesize that ‚Äúdiscrete-centric‚Äù variants (such as the <em>gamma normalization</em> of <a href="https://arxiv.org/abs/2303.06349" rel="external nofollow noopener" target="_blank">LRU</a><d-cite key="orvieto2023resurrecting"></d-cite> and <a href="https://arxiv.org/abs/2402.19427" rel="external nofollow noopener" target="_blank">Griffin</a><d-cite key="de2024griffin"></d-cite>) should work equally well.</p> <blockquote class="block-tip"> <h4 id="is-discretization-necessary">Is Discretization Necessary?</h4> <p>It‚Äôs useful for other structured SSMs, but perhaps not needed for Mamba. But it‚Äôs just a simple invertible transformation, so use either discrete or continuous parameterizations as you like!</p> </blockquote> <h2 id="whats-next">What‚Äôs Next</h2> <p>In the <a href="/blog/2024/mamba2-part4-systems/">final part of this series</a>, we‚Äôll continue talking about the implementation of Mamba-2, but on a more macroscopic level; about the entire neural network, instead of just details of the core SSD layer.</p> <p>We‚Äôll also talk about the actual speed of the algorithm covered in this post.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Arshia Afzal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>