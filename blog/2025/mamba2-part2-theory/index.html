<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION ü¶Å Part 2 - Bi-directional RNN | Arshia Afzal </title> <meta name="author" content="Arshia Afzal"> <meta name="description" content="Deriving equivalent bidirectional RNN for linear Attention"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://arshiaafzal.github.io/blog/2025/mamba2-part2-theory/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION ü¶Å Part 2 - Bi-directional RNN",
            "description": "Deriving equivalent bidirectional RNN for linear Attention",
            "published": "February 20, 2025",
            "authors": [
              
              {
                "author": "Arshia Afzal",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Elias Abad Rocamora",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Leyla Naz Candogan",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Pol Puigdemont Plana",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Tonin",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yongtao Wu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Volkan Cevher",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "All authors are with EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arshia</span> Afzal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION ü¶Å Part 2 - Bi-directional RNN</h1> <p>Deriving equivalent bidirectional RNN for linear Attention</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#finding-bidirectional-rnn-equal-to-full-linear-attention">Finding Bidirectional RNN Equal to Full Linear Attention</a> </div> <div> <a href="#some-important-details-of-our-rnn">Some Important details of our RNN</a> </div> <div> <a href="#different-masks-of-lion">Different Masks of LION</a> </div> <ul> <li> <a href="#lion">LION-üî•</a> </li> <li> <a href="#lion-d">LION-D</a> </li> <li> <a href="#lion-s">LION-S</a> </li> </ul> <div> <a href="#lion-attention-block">LION Attention Block</a> </div> </nav> </d-contents> <ol> <li><a href="/blog/2025/mamba2-part1-model/">Part 1 - Full Linear Attention</a></li> <li>Part 2 - Bi-directional RNN</li> <li><a href="/blog/2025/mamba2-part3-algorithm/">Part 3 - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2025/mamba2-part4-results/">Part 4 - Results</a></li> </ol> <p>In <a href="/blog/2025/mamba2-part1-model/">Part 1</a> of this series, we defined full linear attention with masking and scaling.<br> Similar to all linear transformers designed for causal sequence modeling, we aim to derive an RNN form for efficiency during inference.<br> In this section, we establish and theoretically demonstrate the equivalent bidirectional RNN for the Linear Transformer.</p> <h2 id="finding-bidirectional-rnn-equal-to-full-linear-attention">Finding Bidirectional RNN Equal to Full Linear Attention</h2> <p>Let‚Äôs start by separating the upper, lower, and diagonal elements of the attention matrix and the mask. Since the idea of a bidirectional RNN is to process the sequence in both the forward order (from first to last) and the reverse order (from last to first), these naturally correspond to the upper and lower parts of the attention matrix and mask.</p> <p>Ideally, we aim to construct an RNN that is equivalent to the masked and scaled Linear Attention. Let‚Äôs start by seperating upper and lower parts of the attention and mask:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_mask_color-480.webp 480w,/assets/img/att_mask_color-800.webp 800w,/assets/img/att_mask_color-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/att_mask_color.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><strong>Note:</strong> We made a strong effort to maintain a consistent color coding for this section of the blog post and throughout our paper :).</p> <ul> <li>Wherever you see a <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Yellow</span> color, it indicates the <strong>upper part of the matrix (non-causal)</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Red</span> color, it represents the <strong>diagonal elements</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Blue</span> color, it corresponds to the <strong>lower triangular (causal) part</strong>.</li> </ul> <p>Let‚Äôs seperate the attention into upper and lower parts:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_sep-480.webp 480w,/assets/img/att_sep-800.webp 800w,/assets/img/att_sep-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/att_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This formulation represents both the causal and non-causal forms of attention. Ideally, we aim to model each triangular part using an RNN.Similarly, we can also separate the mask in the same way:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mask_sep-480.webp 480w,/assets/img/mask_sep-800.webp 800w,/assets/img/mask_sep-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/mask_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Let‚Äôs also write the scaling part of the masked attention $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ as:</p> \[\begin{aligned} \mathbf{Y} = \big(\text{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V} = (\mathbf{C}^{-1}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}))\mathbf{V}, \hspace{1mm} \mathbf{C}_i = \mathbf{q}^{\top}_i\sum\limits_{j=1}^{L} \mathbf{M}_{ij}\mathbf{k}_j. \end{aligned}\] <p>Also, we can decompose the scaling matrix \(\mathbf{C}_i\) as:</p> \[\begin{aligned} \mathbf{C}_{i}= \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=1}^{i} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^F_i} + \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=i}^{L} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^B_i} \end{aligned}\] <p>Now we replace tha bove scaling matrix $\mathbf{C}$ in the output of the attention form of $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ .Interestingly, many terms naturally cancel out with each other.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/proofC-480.webp 480w,/assets/img/proofC-800.webp 800w,/assets/img/proofC-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/proofC.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>This results in only the forward and backward directions of the RNN remaining. As observed, the forward path aligns with causal linear attention with masking. Now, we need to demonstrate that the backward path follows the same RNN structure in the reverse direction. We can simply flip the upper triangular matrices using the <a href="https://en.wikipedia.org/wiki/Exchange_matrix" rel="external nofollow noopener" target="_blank">exchange matrix</a> \(\mathbf{J}_L\) and the function \(F(X) = \mathbf{J}_L X \mathbf{J}_L\):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flip-480.webp 480w,/assets/img/flip-800.webp 800w,/assets/img/flip-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/flip.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Cool! Now, both the upper part (equivalent to the RNN in the forward direction) and the lower part (equivalent to the RNN in the backward direction) can be formulated as RNNs. This is exactly what we need to construct our bidirectional RNN equivalent to full linear attention.</p> <blockquote class="block-tip"> <p><strong>LION: Reccurence form</strong></p> \[\begin{aligned} \mathbf{S}_i^{F/B} &amp;= \lambda_i \mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\ \mathbf{z}^{F/B}_i &amp;= \lambda_i \mathbf{z}^{F/B}_{i-1} + \mathbf{k}_i, \\ c^{F/B}_i &amp; = \mathbf{q}_i^{\top} \mathbf{z}^{F/B}_{i} - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}, \\ \mathbf{y}^{F/B}_i &amp;= \mathbf{q}_i^{\top} \mathbf{S}^{F/B}_i - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2} \mathbf{v}_i, \\ out&amp;put: \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i}. \\ \end{aligned}\] </blockquote> <p>The terms $\frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}$ and $\mathbf{k}_i}{2}$ are subtracted to avoid double counting. This bi-directional RNN is equivalent to scaled and masked linear attention described in previous section of this blogpost.</p> <h2 id="some-important-details-of-our-rnn">Some Important details of our RNN</h2> <blockquote> <p>Only the states \(c^{F/B}_i\) and \(\mathbf{y}^{F/B}_i\) are stored per token, resulting in \(\mathcal{O}(Ld)\) memory usage. In contrast, naively storing full matrix-valued hidden states would require \(\mathcal{O}(Ld^2)\), which becomes infeasible for large models.</p> </blockquote> <blockquote> <p>Forward and backward recurrences run independently, completing in \(L\) time steps with \(L\) memory units, compared to \(2L\) in the naive approach.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/memory-480.webp 480w,/assets/img/memory-800.webp 800w,/assets/img/memory-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/memory.png" width="100%" height="auto" title="Memory Allocation of LION in RNN form" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">Memory allocation in LION during Forward and Backward recurrences.</figcaption> </figure> <p>All in one we can visulaize our framework nicely like:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frlion-480.webp 480w,/assets/img/frlion-800.webp 800w,/assets/img/frlion-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/frlion.png" width="100%" height="auto" title="LION" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> <figcaption class="caption">LION ü¶Å: Our framework for training in parallel using Full Linear Attention which also supports the efficient bi-directional RNN format.</figcaption> </figure> <h2 id="different-masks-of-lion">Different Masks of LION</h2> <p>Now that we have created our framework let‚Äôs see what are the choices of the decay factor \(\lambda_i\) and how they resemble the famous linear Transformer models. Let‚Äôs set:</p> <blockquote> <p>$\lambda_i=1$ this results in mighty simple Linear Transformer <d-cite key="katharopoulos2020transformers"></d-cite> which we refrer to as <span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">LION-üî• </span> which is LION-Lit resembling Linear Transformer.</p> </blockquote> <blockquote> <p>$\lambda_i=\lambda$ this results in mighty RetNet <d-cite key="sun2023retentive"></d-cite> which we refrer to as <span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">LION-D </span></p> </blockquote> <blockquote> <p>$\lambda_i=\sigma(\mathbf{W}\mathbf{x}_i)$ being input dependent, and bi-directional Linear Transformer inspired by selectivity of Mamba2 <d-cite key="dao2024transformers"></d-cite> which we refrer to as <span style="background-color: rgb(255, 233, 211) ; padding: 3px; color:black">LION-S </span></p> </blockquote> <p>We evaluate all above models, extended to bidirectional sequence modeling using LION, on several bidirectional tasks. Also as all Linear Transformers use feature mapping $\phi(.)$ to queries and keys we also applied SILU shifted $\phi(x)=$ <code class="language-plaintext highlighter-rouge">(SILU(x)+0.5)/(norm(SILU(x)+0.5))</code> non-linear activation function. Let‚Äôs delve deep in each of these models in LION framework.</p> <h3 id="lion-">LION-üî•</h3> <p>LION-üî• is an extension of the very first Linear Transformer. Without any masking, the bidirectional parallel form can be simply written as:</p> \[\mathbf{Y} = Scale(\mathbf{Q} \mathbf{K}^\top )\mathbf{V}\] <p>and the RNN form of the above parallel full linear attention is simply the RNN form mentioned above in this section in green box just by simply not using any mask.</p> <h3 id="lion-d">LION-D</h3> <p>By fixing \(\lambda_i = \lambda\), the mask \(\mathbf{M}\) has the form:</p> \[\begin{align} \mathbf{M}_{ij} = \lambda^{|i-j|}, \quad \mathbf{D}_{ij} = |i-j|\log(\lambda), \quad \mathbf{M} = \exp(\mathbf{D}). \notag \end{align}\] <p>\(\mathbf{M}\) above is a Toeplitz mask <d-cite key="qin2023toeplitz"></d-cite> and therefore, creating the decay mask can be made even faster using simple PyTorch commands. To ensure numerical stability, we bound the parameter \(\lambda\) using the <strong>sigmoid function</strong>, setting \(\lambda = \sigma(a)\). Without this constraint, the scalar \(\lambda^L\) could become excessively large, leading to instability. Additionally, as we all know, summation is generally more numerically stable than multiplication. Therefore, in some cases, instead of multiplying a matrix repeatedly, we can leverage summation for improved stability. However, in practice, for <strong>RetNet-style masks</strong> with a fixed decay, multiplication remains stable. This allows for a more straightforward implementation when generating the mask in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Decay_Mask</span><span class="p">(</span><span class="n">a</span> <span class="p">,</span> <span class="n">L</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">L</span><span class="p">))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s">LION-S</h3> <p>Observing the structure of $\mathbf{M}$, its upper ($\mathbf{M}^B$) and lower ($\mathbf{M}^F$) triangular parts are rank-1 <a href="https://people.cs.kuleuven.be/~raf.vandebril/homepage/publications/papers_html/qrq_07/node16.html" rel="external nofollow noopener" target="_blank">semi-separable matrices</a> (cite), allowing for efficient computation via matrix multiplications.</p> <p>During training, the decay factors $\lambda_i$ are stacked into ${\lambda}^F \in \mathbb{R}^L$, and the cumulative product</p> \[\mathbf{L}^F = cumprod(\lambda^F) = \prod_{k=0}^{i} \lambda^F_k\] <p>is used to generate the lower triangular mask (\mathbf{M}^F). For the upper triangular mask $\mathbf{M}^B$, the input sequence is flipped, and the decay factors are computed as</p> \[\boldsymbol{\lambda}^B = \text{Flip}(\boldsymbol{\lambda}^F), \quad \mathbf{L}^B = cumprod(\boldsymbol{\lambda}^B).\] <p>The masks are then constructed as, $\mathbf{M}^F =$ <code class="language-plaintext highlighter-rouge">tril(LF@inv(LF)^T)</code> for the forward part and $\mathbf{M}^B =$ <code class="language-plaintext highlighter-rouge">triu(LB@inv(LB)^T)</code> for the backward part. where <code class="language-plaintext highlighter-rouge">tril(.)</code> and <code class="language-plaintext highlighter-rouge">trilu(.)</code> extract the lower and upper triangular parts of the input matrix respectively. The full mask is then obtained as</p> \[\mathbf{M} = \mathbf{M}^F + \mathbf{M}^B - \mathbf{I}.\] <p>To improve numerical stability, the selective scalar $\lambda_i$ is designed in exponential form</p> \[\lambda_i = e^{a_i}.\] <p>This results in the cumulative sum:</p> \[\mathbf{D}^F_{ij} = \begin{cases} \sum_{k=i}^{j+1} a_k, &amp; \text{if } i &gt; j, \\ \sum_{k=i+1}^{j} a_k, &amp; \text{if } i &lt; j, \\ 0, &amp; \text{if } i = j, \end{cases}\] \[\mathbf{M^F} = \exp(\mathbf{D^F}),\] <p>where $\exp(\cdot)$ is applied element-wise. The same process applies to $\mathbf{M}^B$ by flipping the input sequence order.</p> <p>Here, $\mathbf{D}^{F/B} = cumsum(\mathbf{a}^{F/B})$, where $\mathbf{a} \in \mathbb{R}^L$ contains the selective exponents $a_i$.</p> <p>Ensuring stability is crucial, as $\mathbf{L}^{F/B}$ can overflow or underflow when forming the full mask without chunking. To mitigate this, we define</p> \[a_i = \log(\sigma(\mathbf{W}_{a}^\top\mathbf{x}_i + b)),\] <p>where $\sigma(.)$ is the sigmoid function. This approach ensures numerical stability by bounding $a_i$ within the interval $[0,1]$.</p> <p><strong>Note:</strong> It is crucial that the activation function is a <strong>sigmoid</strong>, as other activations do not produce stable masks and can lead to NaN values in the loss function. To maintain stability, <strong>chunking</strong> is required during training. This issue has been specifically highlighted in the <strong>Mamba2</strong> <a href="https://goombalab.github.io/blog/2024/mamba2-part3-algorithm/" rel="external nofollow noopener" target="_blank">blog post</a>.<br> We provide a detailed explanation in the <strong>Results</strong> section of this blog post, where we discuss why using <strong>full attention</strong> is beneficial for achieving <strong>high throughput</strong> during training.</p> <p>The code for building the mask of LION-S is so simple and flexible even in Pytorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_causal_mask_lions</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">cumsum</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">,</span> <span class="mi">1</span><span class="o">/</span> <span class="p">(</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span> <span class="p">)</span>  <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Selective_Mask</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">vec_shape</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">vec</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">A_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">vec</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">vec</span><span class="p">.</span><span class="n">device</span><span class="p">)),</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,:,</span><span class="mi">1</span><span class="p">:].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">A_for</span> <span class="o">+</span> <span class="n">A_back</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <h2 id="lion-attention-block">LION Attention Block</h2> <p>We can formulate the parallel attention form of LION as shown below, supporting all three extensions of our main experiments:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Class</span> <span class="nc">LION_Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">qk_scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">proj_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">silunorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">Mask_type</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">qk_scale</span> <span class="ow">or</span> <span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="n">self</span><span class="p">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">=</span> <span class="n">Mask_type</span>

        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
            <span class="bp">None</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">non_lin</span> <span class="o">=</span> <span class="n">silu_shifted</span>
        <span class="n">self</span><span class="p">.</span><span class="n">silunorm</span> <span class="o">=</span> <span class="n">silunorm</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">a_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">a_i</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">a_i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Selective_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Decay_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">attn</span>

        <span class="c1"># Scaling
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">attn</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn</span> <span class="o">@</span> <span class="n">v</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <blockquote> <p><strong>Question:</strong> As seen above, the <strong>RNN</strong> is more efficient than the <strong>Transformer</strong> since it only requires storing the output for each token, resulting in a memory complexity of <strong>$\mathcal{O}(Ld)$</strong>, as opposed to storing the full attention matrix, which requires <strong>$\mathcal{O}(L^2 d)$</strong>. Can we achieve a balance between the speed of attention parallelism and the efficiency of an RNN?</p> </blockquote> <p>We will answer this question in our next section by introducing LION-Chunk.</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>In the next section of this series, we will describe how to apply a <strong>chunkwise parallel form</strong> for LION, allowing us to balance between the <em>RNN structure</em> and the <em>attention-based</em> formulation.</p> </li> <li> <p>We show the numercial results and experiments on <a href="https://www.image-net.org/" rel="external nofollow noopener" target="_blank">Imagenet</a> and <a href="https://paperswithcode.com/dataset/c4" rel="external nofollow noopener" target="_blank">C4</a> dataset :)</p> </li> </ul> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Arshia Afzal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>