<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION ü¶Å Part III - Chunkwise Parallel from of LION | Arshia Afzal </title> <meta name="author" content="Arshia Afzal"> <meta name="description" content="Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://arshiaafzal.github.io/blog/2025/lion-part3-chunk/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION ü¶Å Part III - Chunkwise Parallel from of LION",
            "description": "Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference",
            "published": "February 24, 2025",
            "authors": [
              
              {
                "author": "Arshia Afzal",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Elias Abad Rocamora",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Leyla Naz Candogan",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Pol Puigdemont Plana",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Tonin",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yongtao Wu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Mahsa Shoaran",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Volkan Cevher",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "All authors are with EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arshia</span> Afzal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION ü¶Å Part III - Chunkwise Parallel from of LION</h1> <p>Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#lion-chunk">LION-Chunk</a> </div> <ul> <li> <a href="#lion-s-chunk">LION-S Chunk</a> </li> <li> <a href="#lion-d-chunk">LION-D Chunk</a> </li> </ul> </nav> </d-contents> <p>[<a href="https://www.arxiv.org/abs/2502.16249" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/LIONS-EPFL/LION" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li><a href="/blog/2025/lion-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/blog/2025/lion-part2-theory/">Part II - Bi-directional RNN</a></li> <li>Part III - Chunkwise Parallel from of LION</li> <li><a href="/blog/2025/lion-part4-results/">Part IV - Results</a></li> </ol> <p>Since we have now established the LION framework, which maps Full Linear Attention into a bi-directional RNN in <a href="/blog/2025/lion-part2-theory/">Part II</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and Attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet <d-cite key="yang2024parallelizing"></d-cite> and GLA <d-cite key="yang2024gated"></d-cite>, as well as the SSD algorithm in Mamba2 <d-cite key="dao2024transformers"></d-cite>, a chunkwise parallel form of Full Linear Attention could be an effective solution. Additionally, in models like Hydra <d-cite key="hwang2025hydra"></d-cite>, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking Full Linear Attention, particularly for LION-S and LION-D, where the mask $\mathbf{M}$ structure is known? The aim of chunking Full Linear Attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does <strong>not</strong> require chunking during training allowing for higher throughput, especially for short sequences, when compared to other models such as Hydra <d-cite key="hwang2025hydra"></d-cite>. While in <strong>Gated Linear Attention (GLA)</strong> <d-cite key="yang2024gated"></d-cite>, <strong>DeltaNet</strong> <d-cite key="yang2024parallelizing"></d-cite>, and the <strong>SSD algorithm of Mamba2</strong> <d-cite key="dao2024transformers"></d-cite> <em>causal-specific</em> chunking methods are employed, we extend this to the non-causal case as well.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>The key idea of chunking is that instead of processing the entire sequence of length $L$, we divide it into $N$ subsequences of length $C$, where $N \times C = L$.<br> To achieve this, we start with the Full Linear Attention formulation:</p> \[\mathbf{Y} = (\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}) \mathbf{V}\] <p>we first chunk the queries, keys and values into submatrices</p> \[\mathbf{Q}_{[i]} , \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\] <p>Now, given the form \((\mathbf{A} \odot \mathbf{M})\), where \(\mathbf{A} = \mathbf{Q} \mathbf{K}^\top\) we can construct the chunkwise form in four parts</p> <ul> <li>Chunkwise \(\mathbf{A}_{[ij]}\)</li> <li>Chunkwise form for the scaling matrix $\mathbf{C}_{[ij]}$</li> <li>The chunked hidden state to shape the unscaled output $\mathbf{S}_{[i(j-1)]}$</li> <li>Finally the output of the chunk $i$ which is $\mathbf{Y}_{[i]}$</li> </ul> <p>using these chunked matrices we shape the full linear atteniton in chunk form as bellow:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[i(j-1)]} + \text{Sum} (\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; =\mathbf{S}_{[i(j-1)]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \\ \mathbf{Y}_{[i]} &amp; = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. And $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full maks $\mathbf{M}$ at chunk $ij$ like:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Let‚Äôs start with an example, chunking the Attention matrix $\mathbf{A}$ for a sequence of $L=9$ with $C=3$ chunk size in detail below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_chunk.svg" sizes="95vw"></source> <img src="/assets/img/att_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>Chunking simply involves computing the queries and keys for each boxed sub-matrix, as illustrated for the upper, lower, and diagonal chunks. For every Attention matrix chunk $[ij]$, the computation follows the same pattern, multiplying the corresponding queries and keys for that chunk.</p> <p>But does the same approach apply to Selective and Fixed masks?</p> <p>In reality, chunking the Attention mask is slightly different and even more critical than chunking Attention itself due to its unique structure. Below, we provide a detailed explanation of how to chunk the Attention mask for LION-D and LION-S.</p> <p>üöÄ <strong>Note:</strong> The chunking visualization and details of this part are exclusively on the blogpost version.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <p>Let‚Äôs start with the decay mask, as it is simpler and easier to visualize. For LION-D, the final mask is a Toeplitz mask constructed using the scalar decay factor $\lambda$. We can visualize how the mask is structured.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskdec_chunk.svg" sizes="95vw"></source> <img src="/assets/img/maskdec_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>The full mask of LION-D (or full RetNet mask) is constructed simply by the submatrix of $\Gamma$, which is a <a href="https://en.wikipedia.org/wiki/Toeplitz_matrix" rel="external nofollow noopener" target="_blank">Toeplitz matrix</a> itself. Regardless of where the chunk is located, whether in the upper or lower part of the mask matrix $\mathbf{M}$, it retains the same property of being a fraction of the Toeplitz matrix $\Gamma$ as bellow:</p> \[\mathbf{M}_{[ij]} = \Gamma \lambda^{|i-j|}\] <p>A pytorch implementation for LION-D Chunk Mask is provided below:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_decay_partial</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">start</span><span class="p">,</span> <span class="n">end</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">a</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">i</span><span class="p">,</span> <span class="n">j</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">"</span><span class="s">ij</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">e</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">i</span> <span class="o">-</span> <span class="n">j</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">]))</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="n">e</span>
    <span class="k">return</span> <span class="n">m</span>
</code></pre></div></div> <h3 id="lion-s-chunk">LION-S Chunk</h3> <p>The full mask of LION-S is more tricky than LION-D since the upper lower and the diagonal part of the mask are shaped differently:</p> <ul> <li>The <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Upper</span> part is influenced only by the decay factors applied from the end to the beginning of the sequence.</li> <li>The <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Diagonal</span> part incorporates contributions from both directions, spanning from the start to the end and from the end to the start.</li> <li>The <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Lower</span> part is influenced only by the decay factors applied from the beginning to the end of the sequence.</li> </ul> <p>Let‚Äôs visualize LION-S mask as well:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/masksel_chunk.svg" sizes="95vw"></source> <img src="/assets/img/masksel_chunk.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>For example, the chunk [1,3] has only the cumulative decay factors multiplied from the beginning up to the last three sequence elements, while the chunk [3,1] has only the decay factors multiplied from the end up to the first three sequence elements. This is the reason for using the matrices $\mathbf{L}^F$ and $\mathbf{L}^B$ to compute the cumulative products of the decay factors, progressing from the beginning to the end of the sequence and in reverse which can be created simply by <code class="language-plaintext highlighter-rouge">L^F = cumprod(a)</code> and <code class="language-plaintext highlighter-rouge">L^B = cumprod(flip(a))</code>.</p> <h3 id="the-code-for-lion-s-chunk-mask">The code for LION-S Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)[</span>
            <span class="p">...,</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span> <span class="p">:</span> <span class="p">(</span><span class="n">chunk_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_length</span>
        <span class="p">]</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">a</span><span class="p">,</span> <span class="n">diagonal</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mask_backward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span> <span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">a</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span>
        <span class="p">...,</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span> <span class="p">:</span> <span class="p">(</span><span class="n">chunk_index</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">,</span> <span class="p">:</span>
    <span class="p">]</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">a</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">mask_selective_partial</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span> <span class="n">chunk_index</span><span class="p">,</span> <span class="n">chunk_length</span><span class="p">):</span>
    <span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">a_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_forward</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">chunk_index</span><span class="p">,</span>
        <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">a_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_backward</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">:]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">chunk_index</span><span class="p">,</span>
        <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag_embed</span><span class="p">(</span>
        <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">b</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">l</span> <span class="o">-</span> <span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">)),</span>
        <span class="n">offset</span><span class="o">=-</span><span class="n">chunk_index</span> <span class="o">*</span> <span class="n">chunk_length</span><span class="p">,</span>
    <span class="p">)[...,</span> <span class="p">:</span> <span class="n">a_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">a_for</span> <span class="o">+</span> <span class="n">a_back</span> <span class="o">-</span> <span class="n">i</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">a_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now that we have all elements in place let‚Äôs see how these models are working in practice on real-world datasets for masked language modeling and image classification.</p> <h2 id="next-up">Next Up</h2> <p>In the <a href="/blog/2025/lion-part4-results/">final part of this series</a>, we present the advantages of using LION compared to other methods for training SSMs or Linear Transformers.</p> <p>We also present the trade-offs for different LION ü¶Å models and compare them with other well-known SSMs and Softmax Transformers.</p> <p><a href="/blog/2025/lion-part4-results/">Continue reading to Part IV - Results</a></p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Arshia Afzal. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>