<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION 🦁 Part I - Full Linear Attention | Arshia Afzal </title> <meta name="author" content="Arshia Afzal"> <meta name="description" content="Explaining the Full Linear Attention paradigm for bi-directional sequence modeling"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://arshiaafzal.github.io/blog/2025/lion-part1-model/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION 🦁 Part I - Full Linear Attention",
            "description": "Explaining the Full Linear Attention paradigm for bi-directional sequence modeling",
            "published": "February 24, 2025",
            "authors": [
              
              {
                "author": "Arshia Afzal",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Elias Abad Rocamora",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Leyla Naz Candogan",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Pol Puigdemont Plana",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Tonin",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yongtao Wu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Mahsa Shoaran",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Volkan Cevher",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "All authors are with EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arshia</span> Afzal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION 🦁 Part I - Full Linear Attention</h1> <p>Explaining the Full Linear Attention paradigm for bi-directional sequence modeling</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#tl-dr">TL;DR</a> </div> <div> <a href="#from-causal-to-full-linear-attention">From Causal to Full Linear Attention</a> </div> <div> <a href="#creating-scaled-and-masked-full-attention">Creating Scaled and Masked Full Attention</a> </div> </nav> </d-contents> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lion-480.webp 480w,/assets/img/lion-800.webp 800w,/assets/img/lion-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/lion.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>[<a href="https://www.arxiv.org/abs/2502.16249" rel="external nofollow noopener" target="_blank">Paper</a>] [<a href="https://github.com/LIONS-EPFL/LION" rel="external nofollow noopener" target="_blank">Code</a>]</p> <ol> <li>Part I - Full Linear Attention</li> <li><a href="/blog/2025/lion-part2-theory/">Part II - Bi-directional RNN</a></li> <li><a href="/blog/2025/lion-part3-chunk/">Part III - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2025/lion-part4-results/">Part IV - Results</a></li> </ol> <hr> <h1 id="tldr">TL;DR</h1> <p>Transformers with Linear Attention enable fast and parallel training. Moreover, they can be formulated as Recurrent Neural Networks (RNNs), for efficient linear-time inference. While extensively evaluated in causal sequence modeling, they have yet to be extended to the bi-directional setting. We introduce the <strong>LION</strong> framework, establishing new theoretical foundations for Linear Transformers in bi-directional sequence modeling. <strong>LION</strong> constructs a bi-directional RNN equivalent to full <strong>Linear Attention</strong>. This extends the benefits of Linear Transformers: parallel training and efficient inference into the bi-directional setting.</p> <div style="display: flex; align-items: flex-start;"> <div style="flex: 1; padding-right: 10px;"> <p> Existing memory-efficient bi-directional models employ more than 2x the training time of a Transformer. Our Linear Attention framework benefits from memory-efficient inference while maintaining the Transformer training speed. </p> <table> <tr> <th>Task</th> <th><span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">🦁-🔥 </span></th> <th><span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">🦁-D </span></th> <th><span style="background-color: rgb(255, 233, 211); padding: 3px; color:black">🦁-S </span></th> <th>Hydra</th> <th>Vim</th> </tr> <tr> <td>Vision</td> <td>$\times 0.73$</td> <td>$\times 1.39$</td> <td>$\times 1.46$</td> <td>$\times 2.51$</td> <td>$\times 10.86$</td> </tr> <tr> <td>MLM </td> <td>$\times 0.95$</td> <td>$\times 1.10$</td> <td>$\times 1.32$</td> <td>$\times 3.13$</td> <td>✗</td> </tr> </table> <div class="caption" style="color: #666666; margin-top: 1px;"> Training time (↓) relative to Transformer of the same scale </div> </div> <div style="flex: 0 0 50%;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig1_plot.svg" sizes="95vw"></source> <img src="/assets/img/fig1_plot.svg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Using <strong>LION</strong>, we cast three Linear Transformers to their bi-directional form:</p> <ul> <li> <strong>LION-️‍🔥</strong>, the bi-directional variant corresponding to <a href="https://arxiv.org/abs/2006.16236" rel="external nofollow noopener" target="_blank">LinearTransformer</a>.</li> <li> <strong>LION-D</strong>, extending <a href="https://arxiv.org/abs/2307.08621" rel="external nofollow noopener" target="_blank">RetNet</a>.</li> <li> <strong>LION-S</strong>, a Linear Transformer with a stable selective mask inspired by selectivity of SSMs like <a href="https://arxiv.org/abs/2405.21060" rel="external nofollow noopener" target="_blank">Mamba🐍</a>.</li> </ul> <p>By replacing the attention block with <strong>LION (-️‍🔥, -D, -S)</strong>, we achieve performance on bi-directional tasks that is comparable to Transformers and State-Space Models (SSMs) while improving training speed.</p> <hr> <p>Recently, Transformers with Linear Attention <d-cite key="katharopoulos2020transformers"></d-cite> and State Space Models <d-cite key="gu2023mamba"></d-cite> <d-cite key="gu2022efficiently"></d-cite> <d-cite key="dao2024transformers"></d-cite> (SSMs) have gained significant popularity for causal sequence modeling due to their ability to efficiently support both parallel training and RNN-like inference. These models have demonstrated impressive accuracy in causal tasks, particularly in causal language modeling. For bi-directional sequence modeling, SSMs, particularly Mamba <d-cite key="gu2023mamba"></d-cite>, have been evaluated in vision tasks along with architecture iterations like Vision Mamba <d-cite key="zhu2024vision"></d-cite> and Hydra <d-cite key="hwang2025hydra"></d-cite>. However, Transformers with Linear Attention have been less explored in the bi-directional setting.</p> <p>We are curious to explore whether Linear Attention Transformers, including the vanilla Linear Transformer <d-cite key="katharopoulos2020transformers"></d-cite> or RetNet <d-cite key="sun2023retentive"></d-cite> can perform effectively on bi-directional sequence modeling. More specifically, what modifications are needed to adapt them for tasks like image classification and masked language modeling? 😊</p> <p>Let’s break this down with three key questions:</p> <h3 id="question-1-applicability">Question 1 (Applicability)</h3> <p>Given that Linear Transformers can be formulated as RNNs, offering efficiency benefits during inference and enabling parallel training for causal sequence modeling, can they also provide similar advantages for bi-directional processing? If so, what would the parallel form be, and how would the equivalent bi-directional RNN be structured?</p> <h3 id="question-2-performance">Question 2 (Performance)</h3> <p>Can simple Linear Transformers, like Linear Transformer <d-cite key="katharopoulos2020transformers"></d-cite> or RetNet <d-cite key="sun2023retentive"></d-cite>, perform well on bi-directional tasks such as image classification or masked language modeling?</p> <h3 id="question-3-training-throughput">Question 3 (Training Throughput)</h3> <p>While bi-directional SSMs are performant, they tend to be difficult and slow to train compared to Transformers with Full Attention (e.g., ViT <d-cite key="dosovitskiy2020image"></d-cite> and BERT <d-cite key="devlin2018bert"></d-cite>). Can Linear Transformers match the accuracy and efficiency of bi-directional SSMs while maintaining the training throughput of Softmax Transformers?</p> <h2 id="from-causal-to-full-linear-attention">From Causal to Full Linear Attention</h2> <p>Let’s start with Linear Attention Recurrence:</p> \[\begin{aligned} &amp; S_i = S_{i-1} + k_i v^\top_i, \quad z_i = z_{i-1} + k_i, \\ &amp; Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Above is the RNN form of the Linear Attention which has the parallel form of:</p> \[\mathbf{Y} = Scale \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>and the mask \(\mathbf{M}^C\) is a lower triangular \(C\)ausal mask. Causal Linear Transformers are a class of models introduced following the development of the original Linear Transformer as shown above <d-cite key="katharopoulos2020transformers"></d-cite>. These models typically define a recurrence of the form:</p> \[\begin{aligned} S_i = \boldsymbol{\Lambda_i} \star S_{i-1} + \gamma_i k_i v^\top_i, \quad z_i = \boldsymbol{\Lambda_i} \star z_{i-1} + \gamma_i k_i, \\ Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Here, \(\boldsymbol{\Lambda_i}\) and \(\gamma_i\) are decay factors introduced after the Linear Transformer to enhance the performance and \(\star\) denotes an associative operator which depends on the specific model. (Spoiler alert ⚠️: the family of Linear Transformers has strong connections to SSMs, as explored in works like Deltanet <d-cite key="yang2024parallelizing"></d-cite> and Mamba2 <d-cite key="dao2024transformers"></d-cite> through state space duality (SSD) 😉). Many models apply a non-linear activation to queries and keys, such that \(\mathbf{k}_i = \phi(\mathbf{k}_i)\) and \(\mathbf{q}_i = \phi(\mathbf{q}_i)\). To avoid notation clutter, we omit explicitly writing \(\phi(.)\) everywhere assuming by default that queries and keys are already non-linearized. For simplicity, we consider \(\boldsymbol{\Lambda_i} = \lambda_i\) as a scalar and \(\gamma_i = 1\). We now present the general Scaled Linear Attention in the following form:</p> \[\begin{aligned} S_i &amp;= \lambda_i S_{i-1} + k_i v^\top_i,\\ z_i &amp;= \lambda_i z_{i-1} + k_i, \\ y_i &amp;= \frac{q^\top_i S_i}{q^\top_i z_i} \\ \end{aligned}\] <p>The first goal is to extend the Causal Linear Attention parallel form</p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>to a Scaled and Masked Full Linear Attention mechanism.</p> <h2 id="creating-scaled-and-masked-full-attention">Creating Scaled and Masked Full Attention</h2> <p>The first step is quite simple: the Masked and Scaled Attention can naturally take the following form, as suggested by its name:</p> <blockquote class="block-tip"> <p><strong>Full Linear Attention</strong></p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} \right)\] </blockquote> <p>The important part is how to well define the matrix \(\mathbf{M}\). A natural choice is to extend the causal mask \(\mathbf{M^C}\), where the causal mask between tokens \(i,j\) is given by \(\mathbf{M}^C_{ij} = \lambda_{j+1} \lambda_{j+2} \dots \lambda_i\), representing the product of all selective scalers between \(i\) and \(j\). In the bi-directional case, the full mask should preserve this desirable property. One can interpret the mask entries as a relative positional encoding between two tokens taking the following form:</p> \[\begin{aligned} \mathbf{M}_{ij} = \begin{cases} \Pi_{k=j}^{i-1}{\lambda_k}, &amp; i &gt; j \\ 1 &amp; i=j\\ \Pi_{k=i+1}^{j}{\lambda_k}, &amp; i &lt; j. \end{cases} \end{aligned}\] <p>To recap, the full output of Full Linear Attention can be presented as:</p> <p><span style="font-size: 0.7em;"> \(\mathbf{Y} = Scale \left( \underbrace{\left( \renewcommand*{\arraystretch} \begin{array}{ccccc} \mathbf{q}_1^{\top}\mathbf{k}_1 &amp; \mathbf{q}_1^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_1^{\top}\mathbf{k}_L \\ \mathbf{q}_2^{\top}\mathbf{k}_1 &amp; \mathbf{q}_2^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_2^{\top}\mathbf{k}_L\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{q}_L^{\top}\mathbf{k}_1 &amp; \mathbf{q}_L^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_L^{\top}\mathbf{k}_L\\ \end{array} \right)}_{\hspace{1mm} \mathbf{A} = \mathbf{Q} \mathbf{K}^{\top}} \odot \underbrace{ \left( \renewcommand*{\arraystretch} \begin{array}{ccccc} 1 &amp; \lambda_2 &amp; \lambda_2 \lambda_3 &amp; \cdots &amp; \lambda_2 \cdots \lambda_L \\ \lambda_1 &amp; 1 &amp; \lambda_3 &amp; \cdots &amp; \lambda_3 \cdots \lambda_L \\ \lambda_2 \lambda_1 &amp; \lambda_2 &amp; 1 &amp; \cdots &amp; \lambda_4 \cdots \lambda_L \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \lambda_{L-1} \cdots \lambda_1 &amp; \lambda_{L-1} \cdots \lambda_2 &amp; \lambda_{L-1} \cdots \lambda_3 &amp; \cdots &amp; 1 \\ \end{array} \right) }_{\hspace{1mm} \mathbf{M}} \right) \left( \renewcommand*{\arraystretch} \begin{array}{c} \mathbf{v}_1^\top \\ \mathbf{v}_2^\top \\ \mathbf{v}_3^\top \\ \vdots \\ \mathbf{v}_L^\top \\ \end{array} \right)\) </span></p> <p>The equation above represents the Full <strong>Linear</strong> Attention in parallel form. Now that we have established Full Linear Attention for bi-directional sequence modeling, it’s time to derive its equivalent bi-directional RNN.</p> <h3 id="an-important-question"><strong>An Important Question:</strong></h3> <blockquote> <p><strong>Question:</strong> Is it worth training with Full Attention on bi-directional tasks considering it has quadratic complexity with sequence length \(O(L^2)\)?</p> </blockquote> <p>The answer is <strong>yes</strong>! Unlike causal language modeling, for bi-directional tasks such as Vision ($L=196$) and Masked Language Modeling (MLM) ($L=128$), sequence lengths used in practice are relatively short. This means that we can usually fit Full Attention in memory enalbing higher throughput without a significant trade-off in complexity.</p> <p>We believe that architectures designed for causal tasks can really benefit from modifications to adapt them to the bi-directional domain.</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>We introduce our framework, <strong>LION</strong>, which derives an equivalent bi-directional RNN for Full Linear Attention.</p> </li> <li> <p>Within this framework, we demonstrate how different Linear Transformers can be extended to their bi-directional counterparts.</p> </li> <li> <p>We explore the construction of stable masks \(\mathbf{M}\), enabling models using LION to <strong>TRAIN IN PARALLEL</strong> using Full Attention and <strong>INFER EFFICIENTLY</strong> like an RNN.</p> </li> <li> <p>Finally, we introduce a <strong>chunkwise parallel</strong> variant of LION to balance recurrence and parallelism 🙂.</p> </li> </ul> <p><a href="/blog/2025/lion-part2-theory/">Continue reading to Part II - Bi-directional RNN</a></p> <p><em>Acknowledgement:</em> We appreciate <a href="https://goombalab.github.io/" rel="external nofollow noopener" target="_blank">Albert Gu</a> and <a href="https://tridao.me/blog/" rel="external nofollow noopener" target="_blank">Tri Dao</a> for their insightful blog posts, which have been helpful in shaping our own.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Arshia Afzal. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>