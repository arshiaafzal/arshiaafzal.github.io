<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> LION ü¶Å Part 3 - Chunkwise Parallel from of LION | Arshia Afzal </title> <meta name="author" content="Arshia Afzal"> <meta name="description" content="Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%93%9D&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://arshiaafzal.github.io/blog/2025/mamba2-part3-algorithm/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "LION ü¶Å Part 3 - Chunkwise Parallel from of LION",
            "description": "Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference",
            "published": "February 20, 2025",
            "authors": [
              
              {
                "author": "Arshia Afzal",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Elias Abad Rocamora",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Leyla Naz Candogan",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Pol Puigdemont Plana",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "Writer of blogpost",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Francesco Tonin",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Yongtao Wu",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "",
                    "url": ""
                  }
                ]
              },
              
              {
                "author": "Volkan Cevher",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "All authors are with EPFL",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Arshia</span> Afzal </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>LION ü¶Å Part 3 - Chunkwise Parallel from of LION</h1> <p>Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#lion-chunk">LION-Chunk</a> </div> <ul> <li> <a href="#lion-s-chunk">LION-S Chunk</a> </li> <li> <a href="#lion-d-chunk">LION-D Chunk</a> </li> </ul> </nav> </d-contents> <ol> <li><a href="/blog/2025/mamba2-part1-model/">Part 1 - Full Linear Attention</a></li> <li><a href="/blog/2025/mamba2-part2-theory/">Part 2 - Bi-directional RNN</a></li> <li>Part 3 - Chunkwise Parallel from of LION</li> <li><a href="/blog/2024/mamba2-part4-results/">Part 4 - Results</a></li> </ol> <p>Since we have now established the LION theorem, which maps full linear attention into a bidirectional RNN in <a href="/blog/2025/mamba2-part2-theory/">Part 2</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet and GLA, as well as the SSD algorithm in Mamba2, a chunkwise parallel form of full linear attention could be an effective solution. Additionally, in models like Hydra, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking full linear attention, particularly for LION-S and LION-D, where the decay factor is fixed and the mask $\mathbf{M}$ follows a Toeplitz structure? But it is important that this chunkwise form is particularly useful for <strong>inference</strong>, since during training, as we said, full linear attention will provide the highest throughput, especially for short sequences, which is the case for bidirectional tasks. The aim of chunking full attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does not require chunking during training, unlike SSMs such as Hydra <d-cite key="hwang2025hydra"></d-cite>. Also, <strong>Gated Linear Attention (GLA)</strong> <d-cite key="yang2024gated"></d-cite>, <strong>DeltaNet</strong> <d-cite key="yang2024parallelizing"></d-cite>, and the <strong>SSD algorithm of Mamba2</strong> <d-cite key="dao2024transformers"></d-cite> are chunking methods specifically designed for <strong>causal models</strong>.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>As mentioned above, the main purpose of chunking is to balance the memory-speed tradeoff during inference. The key idea is that instead of processing the entire sequence of length $L$, we divide it into $N$ subsequences of length $C$, where $N \times C = L$.<br> To achieve this, we start with the full linear attention formulation:</p> \[\mathbf{Y} = (\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}) \mathbf{V}\] <p>we first chunk the queries, keys and values into submatrices and chunks as followes</p> \[\mathbf{Q}_{[i]} , \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\] <p>Now, given the masked full linear attention in the form $ (\mathbf{A} \odot \mathbf{M})$ for each chunk of the mask and attention, we need to construct the chunkwise form, which consists of four parts:</p> <ul> <li>Chunkwise form of the attention matrix $\mathbf{A}_{[ij]}$</li> <li>Chunkwise form for the scaling matrix $\mathbf{C}$ which the chunkwise form can be written as $\mathbf{C}_{[ij]}$</li> <li>The chunked hidden state to shape the unscaled output $\mathbf{S}_{[ij-1]}$</li> <li>Finally the output of the chunk $i$ which is $\mathbf{Y}_{[i]}$</li> </ul> <p>using these chunked matrices we shape the full linear atteniton in chunk form as bellow:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \\ \mathbf{Y}_{[i]} &amp; = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. And $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full maks $\mathbf{M}$ at chunk $ij$ like:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Don‚Äôt be intimidated by the above formulation‚Äîit is actually quite intuitive when visualized. It clearly shows the steps taken to achieve the chunkwise form.</p> <p>Let‚Äôs start by chunking the attention matrix $\mathbf{A}$. To better understand this, let‚Äôs examine the full attention for a sequence of $L=9$ with $C=3$ chunk size in detail below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_chunk-480.webp 480w,/assets/img/att_chunk-800.webp 800w,/assets/img/att_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/att_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As seen above, chunking simply involves computing the queries and keys for each boxed sub-matrix, as illustrated for the upper, lower, and diagonal chunks. For every attention matrix chunk $[ij]$, the computation follows the same pattern‚Äîmultiplying the corresponding queries and keys for that chunk.</p> <p>But does the same approach apply to selective and fixed masks?</p> <p>In reality, chunking the attention mask is slightly different and even more critical than chunking attention itself due to its unique structure. Below, we provide a detailed explanation of how to chunk the attention mask for LION-D and LION-S.</p> <p>üöÄ <strong>Note:</strong> Please pay close attention to this section, as the visualization and details of this part of chunking are not included in the paper.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <p>Let‚Äôs start with the decay mask, as it is simpler and easier to visualize. For LION-D with a fixed mask, the final mask is a Toeplitz mask constructed using the scalar decay factor $\lambda$. Now, let‚Äôs examine how the mask is structured.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskdec_chunk-480.webp 480w,/assets/img/maskdec_chunk-800.webp 800w,/assets/img/maskdec_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/maskdec_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As seen, the full mask of LION-D (or full RetNet mask) is constructed simply by the submatrix of $\Gamma$, which is a Toeplitz matrix itself. Regardless of where the chunk is located, whether in the upper or lower part of the mask matrix $\mathbf{M}$, it retains the same property of being a fraction of the Toeplitz matrix $\Gamma$ as bellow:</p> \[\mathbf{M}_{[ij]} = \Gamma \lambda^{|i-j|}\] <p>which can simply be implemented in Pytorch.</p> <h3 id="the-code-for-lion-d-chunk-mask">The code for LION-D Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Mask_Decay_Partial</span><span class="p">(</span><span class="n">a_i</span> <span class="p">,</span> <span class="n">L</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">end</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s-chunk">LION-S Chunk</h3> <p>Despite LION-D the full mask of LION-S is more tricky since the upper lower and the diagonal part of the mask are shaped differently:</p> <ul> <li>The <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Upper</span> part is influenced only by the decay factors applied from the end to the beginning of the sequence.</li> <li>The <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Diagonal</span> part incorporates contributions from both directions, spanning from the start to the end and from the end to the start.</li> <li>The <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Lower</span> part is influenced only by the decay factors applied from the beginning to the end of the sequence.</li> </ul> <p>Let‚Äôs visualize LION-S mask as well:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/masksel_chunk-480.webp 480w,/assets/img/masksel_chunk-800.webp 800w,/assets/img/masksel_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"></source> <img src="/assets/img/masksel_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p>As seen above, the chunk 1,3, for example, has only the cumulative decay factors multiplied from the beginning up to the last three sequence elements, while the chunk 3,1 has only the decay factors multiplied from the end up to the first three sequence elements. And this is the reason for using the matrices $\mathbf{L}^F$ and $\mathbf{L}^B$ to compute the cumulative products of the decay factors, progressing from the beginning to the end of the sequence and in reverse which can be created simply by <code class="language-plaintext highlighter-rouge">L^F = cumprod(a)</code> and <code class="language-plaintext highlighter-rouge">L^B = cumprod(Flip(a))</code>.</p> <h3 id="the-code-for-lion-s-chunk-mask">The code for LION-S Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)[...,</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">:(</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">diagonal</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mask_backward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[...,</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">:(</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">,:]</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span><span class="n">diagonal</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Mask_Selective_Partial</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">L</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_forward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">)</span>
    <span class="n">A_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span><span class="mi">1</span><span class="p">:]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">)</span>
    <span class="n">I</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag_embed</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">L</span><span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)),</span><span class="n">offset</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)[...,:</span><span class="n">A_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">A_for</span> <span class="o">+</span> <span class="n">A_back</span> <span class="o">-</span> <span class="n">I</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now that we have all elemnts in place let‚Äôs see how these models are working in practice on real-world datasets for masked language modeling and image classification.</p> <h2 id="next-up">Next Up</h2> <p>In the <a href="/blog/2024/mamba2-part4-results/">final part of this series</a>, we present the advantages of using LION compared to other methods for training SSMs or Linear Transformers.</p> <p>We will present the trade-offs for different LION ü¶Å models and compare them with other well-known SSMs and Softmax Transformers.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/albert.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> ¬© Copyright 2025 Arshia Afzal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>