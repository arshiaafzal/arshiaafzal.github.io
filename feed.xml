<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arshiaafzal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arshiaafzal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-18T13:20:34+00:00</updated><id>https://arshiaafzal.github.io/feed.xml</id><title type="html">blank</title><subtitle>Arshia Afzal personal page. </subtitle><entry><title type="html">LION ü¶Å Part I - Full Linear Attention</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part1-model/" rel="alternate" type="text/html" title="LION ü¶Å Part I - Full Linear Attention"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part1-model</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lion-480.webp 480w,/assets/img/lion-800.webp 800w,/assets/img/lion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/lion.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>We sincerely appreciate Albert Gu and Tri Dao for their insightful blog posts, which have been invaluable in shaping our own!</strong></p> <ol> <li>Part I - Full Linear Attention</li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - LION: Bi-directional RNN for Full Linear Attention</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - LION Chunk: CHunkwise Parallel of LION</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>Recently, Transformers with Linear Attention and State Space Models (SSMs) have gained significant popularity for causal sequence modeling due to their ability to efficiently support both parallel training and RNN-like inference. These models have demonstrated impressive accuracy in causal tasks, particularly in causal language modeling. However, their evaluation in bi-directional sequence modeling, such as image classification and masked language modeling, has been relatively limited. In contrast, SSMs, particularly Mamba, have been recently evaluated in vision tasks, including models like Vision Mamba and Hydra, which represent official extensions of Mamba for bi-directional sequence modeling.</p> <p>We‚Äôre curious to explore whether Linear Attention Transformers, including the simple Linear Transformer and RetNet or simple selctive varient, can perform effectively on bi-directional sequence modeling. Or more precicley what modifications are needed to adapt them for tasks like image classification and masked language modeling? üòä</p> <p>Let‚Äôs break this down with three key questions:</p> <h3 id="question-1-applicability">Question 1 (Applicability)</h3> <p>Given that Linear Transformers can be formulated as RNNs and offer efficiency benefits during inference, alongside parallel training for causal sequence modeling, can they also exhibit similar benefits for bi-directional processing? If so, what would the parallel form look like, and what would be the equivalent bi-directional RNN form?</p> <h3 id="question-2-performance">Question 2 (Performance)</h3> <p>Assuming we‚Äôve addressed the first question, can simple Linear Transformers‚Äîsuch as Linear Trans (cite), RetNet (cite), or even basic linear attention with a selective decay factor‚Äîperform well on bi-directional tasks, such as image classification or masked language modeling?</p> <h3 id="question-3-training-throughput">Question 3 (Training Throughput)</h3> <p>While bi-directional SSMs like Hydra and Vision Mamba show impressive performance on bi-directional sequence modeling tasks, they tend to be difficult and slow to train compared to Transformers with full attention (e.g., ViT and BERT). If we‚Äôve answered the first two questions affirmatively, can Linear Transformers match the accuracy of deep bi-directional SSMs while maintaining the training throughput of softmax Transformers and the inference efficiency of RNNs/SSMs? Also, maybe we can achive this without need for CUDA kernel programming and simply using torch ;)</p> <h2 id="from-causal-to-full-linear-attention">From Causal to Full Linear Attention</h2> <p>Let‚Äôs start with Linear Attention Reccurence:</p> \[\begin{aligned} &amp; S_i = S_{i-1} + k_i v^\top_i, \quad z_i = z_{i-1} + k_i, \\ &amp; Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Above is the RNN form of the Linear Attention which have the parallel form of:</p> \[\mathbf{Y} = Scale \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>and the mask $\mathbf{M}^C$ is a lower triangular binary matrix. Causal Linear Transformers are a class of models introduced following the development of Linear Transformers as shown above (cite). These models typically define a recurrence of the form:</p> \[\begin{aligned} S_i = \boldsymbol{\Lambda_i} \star S_{i-1} + \gamma_i k_i v^\top_i, \quad z_i = \boldsymbol{\Lambda_i} \star z_{i-1} + \gamma_i k_i, \\ Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Here, \(\boldsymbol{\Lambda_i}\) and \(\gamma_i\) are decay factors introduced after Linear Transformers to enhance their performance. (Spoiler alert ‚ö†Ô∏è: this family of Linear Transformers has strong connections to SSMs, as explored in works like (DeltaNet) and (Mamba-2) üòâ). For simplicity, we consider \(\boldsymbol{\Lambda_i} = \lambda_i\) as a scalar in this study. As shown, this choice is as effective as the full matrix form. We now present the general scaled linear attention in the following form:</p> \[\begin{aligned} S_i &amp;= \lambda_i S_{i-1} + \gamma_i k_i v^\top_i,\\ z_i &amp;= \lambda_i z_{i-1} + \gamma_i k_i, \\ y_i &amp;= \frac{q^\top_i S_i}{q^\top_i z_i} \\ \end{aligned}\] <p>The first goal is to extend the causal linear attention parallel form</p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>to a fully <em>scaled</em> and <em>masked</em> attention mechanism for linear attention.</p> <h2 id="creating-scaled-and-masked-full-attention">Creating Scaled and Masked Full Attention</h2> <p>The first step is quite simple: the masked and scaled attention can naturally take the following form, as suggested by its name:</p> <blockquote class="block-tip"> <p><strong>Full Linear Attention</strong></p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} \right)\] </blockquote> <p>The important part is how to well define the matrix \(\mathbf{M}\). A natural choice is to extend the causal mask \(\mathbf{M^C}\), where the causal mask between tokens \(i,j\) is given by \(\mathbf{M}^C_{ij} = \lambda_{j+1} \lambda_{j+2} \dots \lambda_i\), representing the product of all selective scalers between \(i\) and \(j\). In the bidirectional case, the full mask should preserve this property. Since this is indeed a desirable property, one can interpret it as a form of relative positional encoding between two tokens. Saying so the mask cen be shaped as:</p> \[\begin{aligned} \mathbf{M}_{ij} = \begin{cases} \Pi_{k=j+1}^{i}{\lambda_k}, &amp; i &gt; j \\ 1 &amp; i=j\\ \Pi_{k=i+1}^{j}{\lambda_k}, &amp; i &lt; j. \end{cases} \end{aligned}\] <p>To recap, the full output of full LInear Attention can be presented as:</p> <p><span style="font-size: 0.75em;"> \(\mathbf{Y} = Scale \left( \underbrace{\left( \renewcommand*{\arraystretch} \begin{array}{ccccc} \mathbf{q}_1^{\top}\mathbf{k}_1 &amp; \mathbf{q}_1^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_1^{\top}\mathbf{k}_L \\ \mathbf{q}_2^{\top}\mathbf{k}_1 &amp; \mathbf{q}_2^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_2^{\top}\mathbf{k}_L\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{q}_L^{\top}\mathbf{k}_1 &amp; \mathbf{q}_L^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_L^{\top}\mathbf{k}_L\\ \end{array} \right)}_{\hspace{1mm} \mathbf{A} = \mathbf{Q} \mathbf{K}^{\top}} \odot \underbrace{ \left( \renewcommand*{\arraystretch} \begin{array}{ccccc} 1 &amp; \lambda_2 &amp; \lambda_2 \lambda_3 &amp; \cdots &amp; \lambda_2 \cdots \lambda_L \\ \lambda_1 &amp; 1 &amp; \lambda_3 &amp; \cdots &amp; \lambda_3 \cdots \lambda_L \\ \lambda_1 \lambda_2 &amp; \lambda_2 &amp; 1 &amp; \cdots &amp; \lambda_4 \cdots \lambda_L \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \lambda_{L-1} \cdots \lambda_1 &amp; \lambda_{L-1} \cdots \lambda_2 &amp; \lambda_{L-1} \cdots \lambda_3 &amp; \cdots &amp; 1 \\ \end{array} \right) }_{\hspace{1mm} \mathbf{M}} \right) \left( \renewcommand*{\arraystretch} \begin{array}{c} \mathbf{v}_1^\top \\ \mathbf{v}_2^\top \\ \mathbf{v}_3^\top \\ \vdots \\ \mathbf{v}_L^\top \\ \end{array} \right)\) </span></p> <p>The above represents the full <strong>Li</strong>near Attenti<strong>on</strong> in parallel form, which also inspired the name of our framework, <strong>LION</strong> ü¶Å. Now that we have established full linear attention for bidirectional sequence modeling, it‚Äôs time to derive its equivalent bidirectional RNN.</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>We introduce our framework, <strong>LION</strong>, which derives an equivalent bidirectional RNN for full linear attention.</p> </li> <li> <p>Within this framework, we demonstrate how various Linear Transformers can be extended to their bidirectional counterparts.</p> </li> <li>We explore the construction of stable masks (\mathbf{M}), enabling models using LION to: <ul> <li>Train in parallel using full attention.</li> <li>Infer efficiently like an RNN.</li> </ul> </li> <li>Finally, we introduce a <strong>chunkwise parallel</strong> variant of LION to balance recurrence and parallelism üôÇ.</li> </ul>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part II - The Theory</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part II - The Theory"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li>Part II - The Theory</li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>In <a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series, we defined the state space dual (SSD) <em>model</em>. In isolation, this model is relatively simple to define, and we claimed that it can be computed either as an SSM recurrence or with an attention-like pattern. If you just want to use the model, feel free to skip this post!</p> <p>In this post, we‚Äôll dive into the theory behind the model. We‚Äôll derive the SSD ‚Äúduality‚Äù in two completely separate ways, one starting from the SSM perspective and one from the attention perspective. Each method is actually much more broad than the SSD model itself, and the union of these two strong generalizations is what we call the SSD <em>framework</em>. This framework provides a rich body of connections between state space models, attention, and structured matrices. While the SSD model can be viewed as a specific instantiation of each prong of the framework, the SSD framework is much more general opens up many directions for future work.</p> <h4 id="the-state-space-duality-framework">The State Space Duality framework</h4> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_venn-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_venn-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_venn-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_venn.png" width="100%" height="auto" title="Structured State Space Duality" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">SSD Framework (red, blue): State space models (i.e. semiseparable matrices) and structured masked attention encapsulate large classes of efficient sequence models. Their intersection is the SSD model (purple).</figcaption> </figure> <p>For each of the two parts of this framework, we‚Äôll</p> <ol> <li>Define the general concepts</li> <li>Show how the SSD model is an instantiation, and prove the duality</li> <li>Suggest future directions for how the framework can be used</li> </ol> <p>Note that this theory is <em>not necessary</em> to use the SSD model itself; this part of the series can be safely skipped for the practitioner that just wants to use SSD (Mamba-2).</p> <h2 id="recap-the-ssd-model">Recap: The SSD Model</h2> <p><a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series introduced the SSD layer, which is defined as a selective SSM</p> \[\begin{aligned} h_{t} &amp;= A_t h_{t-1} + B_t x_t \\ y_t &amp;= C_t^{\top} y_t \end{aligned}\] <p>\begin{equation} \label{eq:ssm} (\text{Selective state space model (SSM)}) \end{equation}</p> <p>with scalar-identity structure on $A$.</p> <p>More formally, we view it as a <em>sequence transformation</em> $X \mapsto Y$</p> <p>\begin{equation} \label{eq:ssm-transformation} Y^\mathtt{(T,P)} = \mathsf{SSM}(A^\mathtt{(T)}, B^\mathtt{(T,N)}, C^\mathtt{(T,N)})(X^\mathtt{(T,P)}) \end{equation}</p> <p>The dual attention-like form of the SSD layer is</p> <p>\begin{equation} \label{eq:ssd-attention} M = L \circ C B^\top \in \mathbb{R}^{\mathtt{(T,T)}} \end{equation}</p> <p>Now let‚Äôs see how to prove this!</p> <h2 id="ssd-framework-1-structured-matrix-transformations">SSD Framework 1: Structured Matrix Transformations</h2> <p>The first framing of the duality will be from an SSM-centric perspective, where we‚Äôll prove the duality through the framework of <strong>matrix sequence transformations</strong> or ‚Äúmatrix mixers‚Äù.</p> <h3 id="matrix-transformations">Matrix Transformations</h3> <p>The idea is that many sequence models, i.e. <em>sequence transformations</em> $X \in \mathbb{R}^\mathtt{(T,P)} \mapsto Y \in \mathbb{R}^\mathtt{(T,P)}$, can be written in the form of a single matrix multiplication $Y = M(X) \cdot X$ where $M$ is a matrix which can itself depend on $X$. We call this a <em>matrix sequence transformation</em>, or matrix transformation for short. In the literature sequence transformations have also been referred to as ‚Äúsequence mixers‚Äù or ‚Äútoken mixers‚Äù, and matrix sequence transformations as ‚Äúmatrix mixers‚Äù. There are many examples of these, which are distinguished by the structure of the $M$ matrix. The de facto example is self-attention itself, where $M = \mathsf{softmax}(QK^\top)$ is the attention matrix. Other examples include MLP-Mixer<d-cite key="tolstikhin2021mlp"></d-cite>, FNet<d-cite key="lee2021fnet"></d-cite>, and Monarch Mixer<d-cite key="dao2022monarch"></d-cite><d-cite key="fu2024monarch"></d-cite>.</p> <p>Why do we care about these types of models?</p> <blockquote> <p>Writing a sequence model as a matrix transformation provides a powerful tool to understand the structure and characteristics of the model.</p> </blockquote> <p>And although general non-linear RNNs such as LSTMs <em>cannot</em> be written as matrix mixers, state space models can! In fact, this is pretty easy to see by just unrolling the definition of the SSM recurrence. The upshot is that the SSM \eqref{eq:ssm-transformation} can be written as a matrix transformation</p> \[Y = \mathsf{SSM}(A, B, C)(X) = MX\] <p>where $M_{ij} = 0$ for $i &lt; j$ (i.e. it‚Äôs lower triangular) and otherwise \begin{equation} \label{eq:semiseparable} M_{ij} = C_i^\top A_{i:j}^\times B_j := C_i^\top A_i \dots A_{j+1} B_j \end{equation}</p> <p>Drawing it out, this matrix looks like</p> \[\begin{bmatrix} C_0^\top B_0 &amp; \\ C_1^\top A_1 B_0 &amp; C_1^\top B_1 &amp; \\ C_2^\top A_2A_1 B_0 &amp; C_2^\top A_2 B_1 &amp; C_2^\top B_2 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_1 B_0 &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1}\dots A_2 B_1 &amp; \dots &amp; C_\mathtt{T}^\top A_{\mathtt{T}-1} B_{\mathtt{T}-2} &amp; C_\mathtt{T}^\top B_{\mathtt{T}-1} \\ \end{bmatrix}\] <p>\begin{equation} \label{eq:ssm-matrix} (\text{Matrix Transformation Representation of State Space Models}) \end{equation}</p> <h3 id="semiseparable-matrices">Semiseparable Matrices</h3> <p>This type of matrix in fact has a name: it‚Äôs called a (triangular) <strong>semiseparable matrix</strong>, and has been studied in other fields of engineering and computational linear algebra<d-cite key="vandebril2005bibliography"></d-cite>. These matrices are (IMO) quite fundamental and beautiful, and the full paper talks about more of their properties. For example, an alternative characterization of semiseparable matrices is their <em>structured rank property</em>, which says that every submatrix contained in the lower-triangular portion is low rank.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/semiseparable-480.webp 480w,/assets/img/2024-05-31-mamba-2/semiseparable-800.webp 800w,/assets/img/2024-05-31-mamba-2/semiseparable-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/semiseparable.png" width="100%" height="auto" title="State Space Models are Semiseparable Matrices" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">All submatrices contained on-and-below the diagonal of a semiseparable matrix are low-rank.</figcaption> </figure> <p>For our purposes, we‚Äôll care about this form mainly for the algorithmic considerations. One of the central messages of this SSD paper is that:</p> <blockquote class="block-tip"> <h4 id="takeaway-computing-ssms-through-matrix-multiplication">Takeaway: Computing SSMs Through Matrix Multiplication</h4> <p>All algorithms for computing state space models can be viewed as structured matrix multiplication algorithms on semiseparable matrices.</p> </blockquote> <p>Let‚Äôs see an easy instantiation of this, focusing on our main objective!</p> <h3 id="deriving-the-duality-ssm-to-attention">Deriving the Duality: SSM to Attention</h3> <p>To show that equation \eqref{eq:ssd-attention} follows from equation \eqref{eq:ssm} (in the case of the SSD model, i.e. scalar SSM), we directly use the matrix form of the state space model \eqref{eq:semiseparable}. Because the $A_t$ are all scalars in this case, they can be factored out of the entries</p> \[C_i^\top A_{i:j}^\times B_j = A_{i:j}^\times \cdot (C_i^\top B_j)\] <p>which directly implies equation \eqref{eq:ssd-attention}.</p> <p>In summary:</p> <blockquote class="block-tip"> <h4 id="duality-representation-1-ssm">Duality Representation 1 (SSM)</h4> <p>The duality for the SSD model can be seen as two <strong>different matrix multiplication algorithms</strong> on the semiseparable matrix.</p> </blockquote> <ul> <li>The linear form is a <em>structured matrix multiplication algorithm</em> that computes the outputs $Y_0, Y_1, \dots$ sequentially, leveraging the structure of the semiseparable matrix.</li> <li>The quadratic form is the <em>naive matrix multiplication algorithm</em> that materializes the full matrix.</li> </ul> <h3 id="going-beyond-the-ssd-layer-1">Going Beyond the SSD Layer 1</h3> <p>The power of the semiseparable matrix representation applies to <em>all</em> state space models, with various downstream implications.</p> <h4 id="algorithms">Algorithms</h4> <p>Algorithmically, the Mamba-2 paper explores several consequences, such as:</p> <ol> <li>The above duality result for the SSD model, i.e. a scalar-identity structured SSM.</li> <li>New asymptotic efficiency results for state space models (<a href="https://arxiv.org/abs/2405.21060">Theorem 3.7</a>), which follow from applying known results from the semiseparable matrix literature <d-cite key="pernet2016computing"></d-cite><d-cite key="pernet2018time"></d-cite><d-cite key="pernet2023exact"></d-cite>.</li> <li>A more general hybrid algorithm that can be viewed as combining both the linear and quadratic forms to get the best of both worlds. This can be derived as a new matrix multiplication algorithm utilizing <em>block decompositions</em> of the semiseparable matrix. This is the subject of Part III of this blog series!</li> </ol> <h4 id="understanding">Understanding</h4> <p>Conceptually, the matrix transformation viewpoint helps provide a unifying view of sequence models. Some example downstream ideas include</p> <ul> <li><strong>New sequence models</strong>: Restricting ourselves to matrix transformations reduces the problem of developing new sequence models to that of finding structured matrix classes with target properties. In ongoing work by my students, we study this point of view, and use it to derive the most natural bidirectional extension of Mamba (coming very soon!).</li> <li><strong>Expressivity</strong>: Looking at the matrix transformation representation can help us understand what different models can represent from a linear algebraic perspective. In another ongoing work, we use this as a tool to study which subquadratic models are the most amenable to being distilled from Transformers.</li> <li><strong>Interpretability</strong>: A concurrent work <d-cite key="ali2024hidden"></d-cite> derived the matrix formulation of SSMs and use it to probe the internal representations of Mamba models.</li> </ul> <p>We‚Äôre excited to see what algorithmic and conceptual ideas from the structured matrix literature can be applied to further improve state space models!</p> <h2 id="ssd-framework-2-structured-attention">SSD Framework 2: Structured Attention</h2> <p>The second framing of the duality is from an attention-centric perspective, where we‚Äôll prove the duality through the framework of <strong>tensor contractions</strong>.</p> <p>Note that this is entirely independent of the previous [<a href="#ssd-framework-1-structured-matrix-transformations">matrix transformation viewpoint</a>].</p> <h3 id="warm-up-kernel-attention">Warm-up: Kernel Attention</h3> <p>For our purposes, we‚Äôll define attention as a function</p> \[(Q^\mathtt{(T,N)}, K^\mathtt{(S,N)} , V^\mathtt{(S,P)} ) \mapsto Y^\mathtt{(T,P)}\] <p>given by the pairwise matrix multiplications</p> \[Y = (QK^\top) \cdot V\] <details><summary>On Dimensions</summary> <p>Think of $\mathtt{P} = \mathtt{N}$ as the head dimension; technically speaking, in attention the $V$ head dimension $\mathtt{P}$ can differ from the $QK$ head dimension $\mathtt{N}$. Think of $\mathtt{T}$ as the <em>target</em> sequence dimension and $\mathtt{S}$ as the <em>source</em> sequence dimension. Giving these two axes different names will make the math more clear and also covers more general forms of attention such as cross-attention, where the source and target are separate sequences with different lengths. However, for our purposes we‚Äôll assume the self-attention setting where $\mathtt{S}=\mathtt{T}$.</p> </details> <details><summary>Why can we assume this form?</summary> <p>The usual form of attention $Y = f(QK^\top) \cdot V$ (e.g. where $f$ is the softmax function) can, for essentially all functions $f$<d-footnote>And up to some additional massaging such as row-wise normalization, which is easy to handle</d-footnote>, be written as $Y = \psi(Q)\psi(K)^\top \cdot V$ for some appropriate feature map $\psi$ (which may be infinite dimensional). In this case, we can simply redefine $Q \leftarrow \psi(Q)$ and define $\mathtt{N}$ to be the <strong>feature dimension</strong> of the attention kernel to begin with. Softmax attention, for example, can be represented with a particular infinite-dimensional feature map ($\mathtt{N}=\infty$) which represents the exponential kernel.</p> </details> <p>We‚Äôll restrict ourselves to the case when $\psi$ is finite, which is sometimes called <strong>kernel attention</strong>. Many, many variants have been proposed before!<d-cite key="katharopoulos2020transformers"></d-cite><d-cite key="peng2021random"></d-cite><d-cite key="choromanski2021rethinking"></d-cite><d-cite key="qin2022cosformer"></d-cite><d-cite key="zheng2022linear"></d-cite><d-cite key="wang2020linformer"></d-cite><d-cite key="xiong2021nystromformer"></d-cite></p> <p>Why do we care about this formulation? When the sequence length $\mathtt{T}$ grows and the feature dimension $\mathtt{N}$ is small‚Äîcommonly, in the regime when $\psi$ is simple such as an elementwise transform and so $\mathtt{N}$ is constant‚Äîthen the cost of attention can be reduced from quadratic in $\mathtt{T}$ to linear. This follows from simply computing the matrix multiplications in a different order</p> \[Y = Q \cdot (K^\top V)\] <p>This is a somewhat ‚Äúfolklore‚Äù interpretation of linear attention.<d-footnote>At least, one lineage of efficient attention; other varieties exist, such as those based on sparsity or hashing. We reserve the term "linear attention" to those related to Katharopoulos et al.<d-cite key="katharopoulos2020transformers"></d-cite>, or more broadly low-rank attention.</d-footnote></p> <blockquote> <p>The most common way of linearizing attention is usually viewed as a consequence of the <strong>associativity of matrix multiplication</strong></p> </blockquote> <h3 id="causal-linear-attention">(Causal) Linear Attention</h3> <p>However, once the basic kernel attention is slightly modified, we can no longer use the associativity of matrix multiplication directly.</p> <p>The seminal <strong>Linear Attention (LA)</strong> framework of Katharopoulos et al. <d-cite key="katharopoulos2020transformers"></d-cite> shows that it can still be extended to the important case of incorporating causality into attention, for autoregressive settings such as language modeling.</p> <p>Let‚Äôs be a lot more explicit about how it works. The quadratic form of <strong>causal linear attention</strong> is \begin{equation} \label{eq:quadratic-kernel-attention} Y = (L \circ QK^\top) \cdot V \end{equation} where</p> \[L = \begin{bmatrix} 1 \\ \vdots &amp; \ddots \\ 1 &amp; \dots &amp; 1 \end{bmatrix}\] <p>is the <strong>causal mask</strong> matrix.</p> <p>The issue is: once the $L$ mask is incorporated into \eqref{eq:quadratic-kernel-attention}, we can no longer directly apply matrix associativity! This is the problem that the original Linear Attention paper addresses. What they show is that \eqref{eq:quadratic-kernel-attention} is equivalent to a different form which avoids materializing the quadratic $QK^\top$ attention matrix and has linear time complexity</p> \[Y = Q \cdot \mathsf{cumsum}(K^\top V)\] <p>As far as we‚Äôre aware this wasn‚Äôt explicitly proved in the paper, although it isn‚Äôt too hard to write out the summation to show it.</p> <p>What we‚Äôll do is prove this equivalence in essentially one line, while revealing <em>exactly</em> where the ‚Äúlinear‚Äù part of Linear Attention comes from, and how to strongly generalize it.</p> <p>Spoiler alert:</p> <blockquote class="block-tip"> <h4 id="where-does-the-cumsum-in-linear-attention-come-from">Where does the cumsum in Linear Attention come from?</h4> <p>The appearance of the <em>cumulative sum</em> in linear attention is exactly equivalent to the fact that the causal mask $L$, as a matrix multiplication, encodes cumulative sums:</p> \[y = L \cdot x \iff y = \mathsf{cumsum}(x)\] </blockquote> <h3 id="a-tensor-contraction-proof-of-linear-attention">A Tensor Contraction Proof of Linear Attention</h3> <p>Let‚Äôs write out the quadratic form of linear attention \eqref{eq:quadratic-kernel-attention} very explicitly in <strong>tensor contraction</strong> or <a href="https://numpy.org/doc/stable/reference/generated/numpy.einsum.html">einsum</a> notation, with shape annotations:</p> \[\begin{aligned} G &amp;= \mathsf{contract}(\mathtt{TN, SN} \to \mathtt{TS})(Q, K) \\ M &amp;= \mathsf{contract}(\mathtt{TS, TS} \to \mathtt{TS})(G, L) \\ Y &amp;= \mathsf{contract}(\mathtt{TS, SP} \to \mathtt{TP})(M, V) \end{aligned}\] <p>\begin{equation} \label{eq:sma-quad} (\text{Structured Masked Attention - Quadratic Form}) \end{equation}</p> <p>With this notation, we can notice that this sequence of contractions can be written as a <em>single four-way contraction</em></p> <p>\begin{equation} \label{eq:sma} y = \mathsf{contract}(\mathtt{TN},\mathtt{SN},\mathtt{SP},\mathtt{TS} \to \mathtt{TP})(Q, K, V, L) . \end{equation}</p> <p>And finally, it can be computed with any other contraction ordering. In particular, we can perform pairwise reductions on the order $V, K, L, Q$ instead of $Q, K, L, V$</p> \[\begin{aligned} Z &amp;= \mathsf{contract}(\mathtt{SP},\mathtt{SN} \to \mathtt{SPN})(V, K) \\ H &amp;= \mathsf{contract}(\mathtt{TS},\mathtt{SPN} \to \mathtt{TPN})(L, Z) \\ Y &amp;= \mathsf{contract}(\mathtt{TN},\mathtt{TPN} \to \mathtt{TP})(Q, H) \end{aligned}\] <p>\begin{equation} \label{eq:sma-lin} (\text{Structured Masked Attention - Linear Form}) \end{equation}</p> <p>Now the key observation is that the second line of \eqref{eq:sma-lin} is simply a matrix multiplication by $L$, which can be computed with a cumulative sum.</p> <p>That‚Äôs the entire proof of linear attention! The beauty of it is that we didn‚Äôt have to write out a single summation, which was abstracted out into a tensor contraction combined with the structure of $L$.</p> <p>This immediately proves our claim about the <a href="#where-does-the-cumsum-in-linear-attention-come-from">cumsum in linear attention</a>. Moreover, this immediately reveals that the efficiency of linear attention can be made much more general‚Ä¶</p> <h3 id="structured-masked-attention">Structured Masked Attention</h3> <p>The critical observation is that in order for \eqref{eq:sma-lin} to be fast, all that is necessary is for $L$ to be <em>any structured matrix</em> ‚Äì in other words any matrix that has subquadratic matrix-vector multiplication.</p> <p>This immediately motivates one of the main prongs of the SSD framework, which can be seen as a strong generation of LA.</p> <blockquote class="block-tip"> <h4 id="definition-structured-masked-attention">Definition: Structured Masked Attention</h4> <p><strong>Structured masked attention (SMA)</strong> is defined as the <em>four-way tensor contraction</em> \eqref{eq:sma} using an attention mask $L$ that is a structured matrix.</p> </blockquote> <blockquote class="block-tip"> <h4 id="duality-representation-2-sma">Duality Representation 2 (SMA)</h4> <p>SMA has <strong>dual quadratic and linear</strong><d-footnote>Assuming that the structured matrix $L$ has linear time matrix-vector multiplication</d-footnote> <strong>modes</strong> which are simply <em>two different pairwise reduction orders</em> \eqref{eq:sma-quad} and \eqref{eq:sma-lin}.</p> </blockquote> <p>Finally, let‚Äôs just connect this back to the commonly held view of linear attention as matrix multiplication associativity.</p> <blockquote> <p>Although it is commonly believed that incorporating attention masks $L$ prevents matrix multiplication reordering, it turns out to still be compatible. In particular, <strong>associativity of matrix multiplication</strong> is a special case of <strong>tensor contraction reduction orders</strong>; although the former no longer applies, the latter can integrate the attention mask $L$.</p> </blockquote> <p>Next, let‚Äôs look at some consequences of the structured attention framework.</p> <h3 id="deriving-the-duality-attention-to-ssm">Deriving the Duality: Attention to SSM</h3> <p>Recall that the SSD model is defined as either a scalar-identity SSM in equation \eqref{eq:ssm}, or through the attention-like form in equation \eqref{eq:ssd-attention}.</p> <p>To show the equivalence of these forms, we simply recognize that \eqref{eq:ssd-attention} is a special case of structured masked attention where the mask matrix is</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix} .\] <p>\begin{equation} \label{eq:1-ss} (\text{1-semiseparable (1-SS) matrix}) \end{equation}</p> <p>We call this a <strong>1-semiseparable (1-SS) matrix</strong>, for reasons that are explained in more detail in the Mamba-2 paper.</p> <p>Thus, we can also say that the SSD model is <strong>1-semiseparable masked attention</strong> or <strong>1-SS SMA</strong>.</p> <p>To prove that this can be written as an SSM, we simply appeal to the SMA framework, which says that the dual form of this model can be computed through matrix multiplication by $L$. So how fast is that? It‚Äôs not too hard to see that multiplication $y = Lx$ can be computed in linear time through a scalar recurrence:</p> \[\begin{aligned} y_0 &amp;= x_0 \\ y_1 &amp;= a_1 x_0 + a_1 \\ y_2 &amp;= a_2a_1 x_0 + a_2 x_1 + x_2 = a_2 y_1 + x_2 \\ \vdots &amp; \qquad \vdots \end{aligned}\] <p>This corresponds exactly to the original SSM recurrence!</p> <p>(In fact, multiplication by 1-SS matrices $L$ can be computed in a <em>lot</em> more ways, which we compile in the full paper! Alternative algorithms can reveal more insights: for example, the associative scan algorithm used by S5 <d-cite key="smith2023s5"></d-cite> and Mamba can also be shown to be a structured matrix multiplication algorithm on 1-SS matrices.)</p> <h3 id="going-beyond-the-ssd-layer-2">Going Beyond the SSD Layer 2</h3> <p>Structured masked attention not only helps define the SSD model and prove its duality, but it is a much broader framework of efficient attention models.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/sma-480.webp 480w,/assets/img/2024-05-31-mamba-2/sma-800.webp 800w,/assets/img/2024-05-31-mamba-2/sma-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/sma.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Prior examples include the original linear attention as well as the recent Retentive Network (RetNet) model<d-cite key="sun2023retentive"></d-cite>. These can be viewed as direct special cases of SSD. But beyond SSD, we can define classes of efficient attention by replacing the mask $L$ with <em>any structured matrix</em>. As a suggestion, we think that Toeplitz or Fourier structured attention may be interesting to consider because they might encode different forms of positional information.</p> <p>Additionally, other forms of structure can be incorporated into the $L$ mask. For example, another extension my students are developing is viewing SSD (and recurrences in general) as an algorithm operating on <em>directed line graphs</em>, and generalizing it to incorporate arbitrary graph structures.</p> <h2 id="state-space-duality">State Space Duality</h2> <p>We‚Äôll end this post with a brief recap of what we‚Äôve covered.</p> <p>The <strong>SSD framework</strong> consists of the two broad approaches covered in this post, which is summarized by the two areas of the [<a href="#the-state-space-duality-framework">Venn diagram</a>]:</p> <ol> <li>Viewing state space models through [<a href="#ssd-framework-1-structured-matrix-transformations">structured matrix transformations</a>]</li> <li>Generalizing linear attention through [<a href="#ssd-framework-2-structured-attention">tensor contractions</a>]</li> </ol> <p>The [<a href="#recap-the-ssd-model">SSD layer</a>] is a particular model which is the purple intersection in the figure, which can be viewed as an instance of either part of the SSD framework, and in particular has dual quadratic and linear forms that can be derived from either representation.</p> <table> <thead> <tr> <th><em>SSD Framework</em></th> <th>Structured SSMs</th> <th>Structured Attention</th> </tr> </thead> <tbody> <tr> <td>The main representation is‚Ä¶</td> <td>Structured matrix \eqref{eq:ssm-matrix} <br/> sequence transformations</td> <td>The 4-way \eqref{eq:sma} <br/> tensor contraction</td> </tr> <tr> <td>This generalizes‚Ä¶</td> <td>State space models</td> <td>Linear attention</td> </tr> <tr> <td>The SSD model is <br/> an instantiation as‚Ä¶</td> <td>Scalar state space model <br/> ($A_t$ is a scalar-identity matrix)</td> <td>1-semiseparable masked attention <br/> ($L$ mask is a 1-SS matrix)</td> </tr> <tr> <td>The linear-quadratic duality is <br/> revealed through‚Ä¶</td> <td>Structured matrix <br/> multiplication algorithms</td> <td>Tensor contraction <br/> reduction orderings</td> </tr> </tbody> </table> <h2 id="next-up">Next Up</h2> <p>In <a href="/blog/2024/mamba2-part3-algorithm/">the next part of this series</a>, we‚Äôll see how to use some of the SSD framework (in particular, the <a href="#takeaway-computing-ssms">structured matrix algorithm</a> point of view) to derive the more efficient hybrid SSD algorithm that leverages both of the dual forms.</p>]]></content><author><name>Albert Gu</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part III - The Algorithm</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part III - The Algorithm"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li>Part III - The Algorithm</li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - The Systems</a></li> </ol> <p>The theoretical framework of structured state space duality (see <a href="/blog/2024/mamba2-part1-model/">Part I</a> and <a href="/blog/2024/mamba2-part2-theory/">Part II</a> of this series) connects SSMs and (linear) attention through structured matrices. As mentioned in Part I, this connection allows us to derive new algorithms for selective SSMs that are faster than the parallel associative scan in Mamba-1 by leveraging matrix multiplication as a primitive. Moreover, the connection can bring system optimizations (e.g. tensor parallelism, sequence parallelism, variable sequence length) originally developed for Transformer to SSM-land.</p> <h2 id="the-ssd-algorithm">The SSD Algorithm</h2> <p>Even though we already developed optimized scans implementations for Mamba-1, we were limited to small state expansion (typically $\mathtt{N}=16$) as the algorithm and implementation did not use tensor cores (specialized hardware units that perform matrix multiplication). Typically matrix multiplication (matmul) FLOPs are much faster (up to 16x) than non-matmul FLOPs: the A100 GPU has 312 TFLOPS of BF16 matmul but only 19 TFLOPS of FP32 arithmetics, and the H100 has 989 TFLOPS of BF16 matmul but only 67 TFLOPS of FP32 arithmetics. One of our primary goals with Mamba-2 is to <strong>leverage tensor cores to speed up the SSM</strong>.</p> <p>To recap, after tying parameters and introducing the head structure, the SSM in Mamba-1 turns into SSD, a more restrictive form that has an attention-like formulation. And as SSD connects SSMs and structured matrices, we saw in Part II that efficient algorithms to compute SSMs correspond directly to different decompositions of the ‚Äútoken-mixing‚Äù or ‚Äúsequence-mixing‚Äù matrix $M$.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssd_algorithm-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssd_algorithm-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/ssd_algorithm.png" width="100%" height="auto" title="SSD Algorithm" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>We can therefore create new algorithms to compute SSMs simply by looking for alternative ways to multiply this matrix, for example by decomposing it in various ways. A simple block decomposition of this matrix, with carefully chosen block sizes, turns out to get all the advantages of both the linear-recurrent and quadratic-attention dual forms of SSD. This leads to the SSD algorithm, which has 4 steps. There are two completely different interpretations of this algorithm!</p> <h3 id="ssd-algorithm-block-matrix-decomposition">SSD Algorithm: Block Matrix Decomposition</h3> <p>We first partition the SSM (semiseparable) matrix into blocks of size $\mathtt{Q} \times \mathtt{Q}$. Then, we use the properties of semiseparable matrices to factorize each off-diagonal block, which is low rank.</p> <ol> <li>(<em>Orange</em>) Each diagonal block is a smaller semiseparable matrix; we can compute this multiplication however we like; in particular, using the quadratic (attention-like) form of SSD.</li> <li>(<em>Green</em>) There are only $\mathtt{T} / \mathtt{Q}$ total different green blocks because many of them are shared. These can be computed with a batched matmul.</li> <li>(<em>Yellow</em>) Notice that the yellow terms themselves form a 1-semiseparable matrix; in other words, this step is equivalently to an SSM scan (on some modified $A$ factors)!</li> <li>(<em>Blue</em>) Similar to green, these can be computed with a batched matmul.</li> </ol> <h3 id="ssd-algorithm-chunking-and-state-passing">SSD Algorithm: Chunking and State Passing</h3> <p>An alternative interpretation of the algorithm involves reasoning about how the SSM operates on the actual sequence. We first split the sequence of input into blocks (or chunks) of size $\mathtt{Q}$. The steps then have the interpretation</p> <ol> <li><strong>Intra-chunk outputs</strong>: compute the local output of each chunk (<em>what is the output per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Chunk states</strong>: compute the final state of each chunk (<em>what is the final state per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Pass states</strong>: compute a recurrence on all of the chunks‚Äô final states ‚Äì using any desired algorithm, e.g. parallel or sequential scan (<em>what is the actual final state per chunk taking into account all previous inputs?</em>)</li> <li><strong>Output states</strong>: for each chunk, given its true initial state (computed in Step 3), compute the contribution to the output just from the initial state</li> </ol> <p>Either way, we see that most of the algorithm (Step 1, 2, and 4) leverages matmuls (and hence tensor cores), and also can be computed completely in parallel! Only Step 3 requires a scan, but it operates on a much shorter sequence and usually only takes a small fraction of the time of the full algorithm.</p> <h3 id="special-cases">Special Cases</h3> <p>We note that special cases of this algorithm have been seen before. In particular RetNet<d-cite key="sun2023retentive"></d-cite>, which we showed in Part II to be a special case of SSD, mention a ‚Äúchunkwise‚Äù algorithm which computes the quadratic form on a chunk of the input one-at-a-time and passes the final state to the next chunk. This turns out to be essentially equivalent to the SSD algorithm specialized to a restricted case (i.e. a decay matrix mask $L$). Our derivation comes from a different direction‚Äîthe block matrix decomposition‚Äîwhich also makes it more obvious how to parallelize this algorithm and make it really fast in practice.</p> <p>Other forms of ‚Äúchunkwise‚Äù recurrences have recently become popular, such as in <a href="https://arxiv.org/abs/2312.06635">Gated Linear Attention (GLA)</a><d-cite key="yang2024gated"></d-cite>.</p> <h2 id="the-code">The Code</h2> <p>In the ‚ÄúMinimal SSD‚Äù code that we provide in the paper and the <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py">code release</a>, we delineate each of these four steps. As promised, this algorithm is not only faster but also much easier to implement than the original selective scan of Mamba, coming in at just around 25 lines of code!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span>
</code></pre></div></div> <h2 id="the-details">The Details</h2> <p>Let‚Äôs talk about a couple of additional details in the implementation (these don‚Äôt even appear in the full paper, so pay attention!) that unpack some of the choices in this reference code.</p> <h3 id="the-ssm-scan">The SSM Scan</h3> <p>In the above code, we utilized the connection between scalar SSM recurrences</p> \[h_{t+1} = A_t h_t + B_t x_t\] <p>and matrix multiplication by 1-semiseparable matrices</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix}\] <p>which we covered in Part II (and Section 3.2.2 of the paper). In this minimal implementation, we compute Step 3 of the algorithm, which is computing a scalar SSM by <em>any</em> algorithm of our choice, by explicitly materializing a 1-SS matrix and doing dense matrix multiplication.</p> <p>We use this version for several reasons:</p> <ol> <li>Code-wise, it‚Äôs simpler to materialize and multiply by this matrix than to actually implement a parallel associative scan</li> <li>Because of the block decomposition of the SSM matrix, the sequence length $\mathtt{T}$ is reduced by a factor of $\approx 100$ ‚Äì so doing the scan in time $O(\mathtt{T}^2)$ instead of $O(\mathtt{T})$ isn‚Äôt too bad</li> <li>We have to materialize a 1-SS matrix anyways for Step 1 of the algorithm (the diagonal blocks), so might as well reuse the code ¬Ø\_(„ÉÑ)_/¬Ø</li> </ol> <p>While this example code is simpler and reasonably efficient on GPU (and probably TPU as well!), it‚Äôs no longer truly linear at long sequences. Our more optimized Triton implementation does replace the 1-SS multiplication in Step 3 with an actual associative scan.</p> <h3 id="stability">Stability</h3> <h4 id="attempt-1-ratios-of-cumprods">Attempt 1: Ratios of cumprods</h4> <p>The first naive attempt may be to notice that the entries of this matrix are cumulative products</p> \[a_{i:j}^\times = a_i \times \cdots \times a_{j-1} = \frac{a_{i:\mathtt{T}}^\times}{a_{j:\mathtt{T}}^\times}\] <p>However, this runs into severe numerical issues because these products can get really tiny (imagine $a_t \approx 0.9$ and powering it up for a sequence length $\mathtt{T}$ in the thousands!)</p> <h4 id="fix-1-the-segment-sum-segsum-operation">Fix 1: The Segment Sum (<code class="language-plaintext highlighter-rouge">segsum</code>) Operation</h4> <p>The second attempt would be to do all of this in log-space, because all the $a_t$ are positive; so the products become additions, and instead of <code class="language-plaintext highlighter-rouge">cumprod</code>s to deal with we have <code class="language-plaintext highlighter-rouge">cumsum</code>s instead. Then in order to compute the 1-SS matrix, we just have to compute the sums $\log a_i + \dots + \log a_{j-1}$ for every <em>segment</em> $[i:j]$. We call this the <strong>segment sum (segsum)</strong> primitive, analogous to cumulative sum (cumsum).</p> <h4 id="attempt-2-differences-of-cumsums">Attempt 2: Differences of cumsums</h4> <p>The obvious way to do this again is using the same idea as above, but in log space</p> \[a_{i:j}^\times = \exp\left( \log a_i + \cdots + \log a_{j-1} \right) = \left( (\log a)_{i:\mathtt{T}}^+ - (\log a)_{j:\mathtt{T}}^+ \right)\] <p>where we compute a single cumulative sum of $a$ along the time axis, and then compute all pairwise differences. In code, we can do this with</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum_unstable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>
</code></pre></div></div> <p>(and then the 1-semiseparable matrix is just the exponential of this output).</p> <p>Sums/differences are a lot more stable than products/quotients, so this should work ‚Äì right?</p> <h4 id="fix-2-remove-all-subtractions">Fix 2: Remove All Subtractions</h4> <p>Unfortunately, it turns out this still doesn‚Äôt work. The values of this 1-SS matrix roughly represent the SSM dynamics, which are very sensitive to these values of $a_t$, so we have to be very precise. And even in log space, these cumsums can be fairly large, which runs into <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> when subtracted. So we really have to find a way to compute this matrix with only additions, while still vectorizing everything‚Ä¶</p> <h4 id="attempt-3-stable-segsum">Attempt 3: Stable Segsum</h4> <p>This leads to the helper function in the reference SSD code. Instead of computing a single cumsum and then subtracting, we find a way to use a batch of independent cumsums that immediately produces the right answer without subtraction.</p> <p>These details do matter! Without the right implementation of these primitives, the basic SSD algorithm produces NaNs immediately during training (even with FP32).</p> <h3 id="discretization">Discretization</h3> <p>This lineage of structured state space models developed from <a href="https://arxiv.org/abs/2111.00396">S4</a> and <a href="https://arxiv.org/abs/2110.13985">its</a> <a href="https://arxiv.org/abs/2008.07669">predecessors</a> which were viewed as continuous-time systems.<d-cite key="gu2023thesis"></d-cite><d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2021combining"></d-cite><d-cite key="gu2020hippo"></d-cite></p> <p>In Mamba, however, we don‚Äôt really view the SSM as continuous anymore. In fact, as mentioned in the Discussion (Section 5) of the <a href="https://arxiv.org/abs/2312.00752">original paper</a>, Mamba trades off with S4 on modeling different types of data:</p> <ul> <li>S4 is a continuous-time model that excels at modeling continuous data, e.g. perceptual signals such as audio waveforms and pixel-level vision.</li> <li>Mamba is a discrete-time model that excels at modeling discrete data, e.g. tokenized data such as language.</li> </ul> <p>However, the parameterization of Mamba still used the same discretization step as in prior structured SSMs, where there is another parameter $\Delta$ being modeled. We do this because the discretization step has other side effects such as properly normalizing the activations <d-cite key="gu2023train"></d-cite><d-cite key="orvieto2023resurrecting"></d-cite> which is important for performance.</p> <p>The initializations and parameterizations from the previous <a href="https://arxiv.org/abs/2206.12037">theory on structured SSMs</a> still work out-of-the-box, so why fix what‚Äôs not broken?</p> <p>Despite this, we‚Äôre pretty sure that the discretization step isn‚Äôt really necessary for Mamba. In the Mamba-2 paper, we chose to work directly with the ‚Äúdiscrete parameters‚Äù $A$ and $B$, which in all previous structured SSM papers (including Mamba-1) were denoted $(\bar{A}, \bar{B})$ and defined through an additional transformation</p> \[\begin{align*} \bar{A} &amp;= \exp(e^{\Delta A}) \\ \bar{B} &amp;= (\exp(e^{\Delta A}) - I) A^{-1} B \end{align*}\] <p>This doesn‚Äôt pose any problems: to use the continuous SSM parameterization, simply transform the parameters through the above formulas before plugging into the SSD code above.</p> <p>In the full Mamba-2 code, we also kept the same parameterization and discretization step as in Mamba‚Äîagain, why fix what‚Äôs not broken?‚Äîbut hypothesize that ‚Äúdiscrete-centric‚Äù variants (such as the <em>gamma normalization</em> of <a href="https://arxiv.org/abs/2303.06349">LRU</a><d-cite key="orvieto2023resurrecting"></d-cite> and <a href="https://arxiv.org/abs/2402.19427">Griffin</a><d-cite key="de2024griffin"></d-cite>) should work equally well.</p> <blockquote class="block-tip"> <h4 id="is-discretization-necessary">Is Discretization Necessary?</h4> <p>It‚Äôs useful for other structured SSMs, but perhaps not needed for Mamba. But it‚Äôs just a simple invertible transformation, so use either discrete or continuous parameterizations as you like!</p> </blockquote> <h2 id="whats-next">What‚Äôs Next</h2> <p>In the <a href="/blog/2024/mamba2-part4-systems/">final part of this series</a>, we‚Äôll continue talking about the implementation of Mamba-2, but on a more macroscopic level; about the entire neural network, instead of just details of the core SSD layer.</p> <p>We‚Äôll also talk about the actual speed of the algorithm covered in this post.</p>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part IV - The Systems</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part IV - The Systems"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li>Part IV - The Systems</li> </ol> <p>Transformers have benefited from 7 years of systems optimization from the whole research community and large companies. The SSD framework draws connections between SSMs and attention, and allows us to implement many of these optimizations for models like Mamba-2 as well. We focus on tensor parallel and sequence parallel for large-scale training, as well as variable-length sequences for efficient finetuning and inference.</p> <h2 id="systems-and-scaling-optimizations">Systems and Scaling Optimizations</h2> <h3 id="tensor-parallelism">Tensor Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_tp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_tp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_tp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_tp.png" width="100%" height="auto" title="Mamba-2 Tensor Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>One difficulty with large-scaling training of Mamba-1 using tensor parallelism (TP) is that it requires 2 all-reduces per layer, compared to just 1 all-reduce per attention or MLP layer in Transformer. This is because some of the SSM parameters are functions of the inner activations, not of the input to the layer. In Mamba-2, with the ‚Äúparallel projection‚Äù structure, all SSM parameters are functions of the input to the layer, and we can easily apply TP to the input projection: We split the input projection and output projection matrices into 2, 4, 8 shards, depending on the TP degree. We use a grouped norm with number of groups divisible by the TP degree, so that normalization is done separately per GPU. These changes result in 1 all-reduce per layer, instead of 2.</p> <h3 id="sequence-parallelism">Sequence Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_cp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_cp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_cp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_cp.png" width="100%" height="auto" title="Mamba-2 Sequence Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When training on very long sequence length, we might need to split along the sequence length and assign different parts to different devices. There are two main forms of sequence parallelism (SP): For the residual and normalization operation: this replaces the all-reduce in TP with a reduce-scatter, residual + normalization, then all-gather. Since Mamba-2 uses the same residual and normalization structure as Transformer, this form of SP applies directly with no modification. For the attention or SSM operation, aka context parallelism (CP). For attention, one could use Ring attention to split it up along the sequence dimension. For Mamba-2, the SSD framework comes to our help once again: using the same block decomposition, we can have each GPU computing its local output and its final states, then pass the states between GPUs (using send/receive communication primitives), before updating the final output of each GPU.</p> <h3 id="variable-length">Variable Length</h3> <p>For finetuning and inference, in the same batch we often have sequences of different lengths. For Transformer, one would usually pad so all sequences have the same length (wasting computation), or implement attention specifically for variable length sequences with careful load-balancing. With SSM, we can simply treat the whole batch as a long ‚Äúsequence‚Äù, and avoid passing the states between different sequences in the batch by setting the state transition $A_t$ to 0 for tokens at the end of each sequence.</p> <h2 id="results">Results</h2> <p>How well do these optimizations work? The faster SSD algorithm allows us to increase the state dimension ($\mathtt{N}=64$ or $128$ compared to $\mathtt{N}=16$ in Mamba-1). Even though technically Mamba-2 is more restricted than Mamba-1 for the same $\mathtt{N}$, the larger state dimensions generally improve model quality. Here we show results for models trained on 300B tokens on the Pile, with Mamba-2 outperforming Mamba-1 and Pythia.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_lm_downstream-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/blog_lm_downstream.png" width="100%" height="auto" title="Downstream Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Standard downstream evaluations for open source models trained on the Pile</figcaption> </figure> <p>What about <strong>hybrid models</strong>? We have seen from recent and concurrent work (such as <a href="https://arxiv.org/abs/2403.19887">Jamba</a> and <a href="https://arxiv.org/abs/2405.16712">Zamba</a>) that combining Mamba layers with attention layers can improve over pure Transformer or Mamba. We validate at 2.7B parameters and 300B tokens scale that a hybrid model with just 6 attention blocks (and 58 SSD blocks) outperforms 64 SSD blocks, as well as our standard Transformer++ baseline (32 gated MLP and 32 attention blocks).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_hybrid-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_hybrid-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_hybrid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/blog_hybrid.png" width="100%" height="auto" title="Downstream Evaluations for Hybrid Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Downstream evaluations for hybrid Mamba/attention models</figcaption> </figure> <p>We also validated that the SSD algorithm is significantly faster than the selective scan algorithm from Mamba-1 for the same state dimension, and scales much better computationally to larger state dimensions. Getting those tensor cores to go brrr is the key!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate.png" width="100%" height="auto" title="Mamba-2 Efficiency Benchmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Efficiency benchmarks on sequence length 2K</figcaption> </figure> <h2 id="future-directions">Future Directions</h2> <p>With SSD, we have connected (linear) attention and SSMs, allowing us to design faster algorithms and implement systems optimizations for SSMs. There are still tons of exciting directions that we (and hopefully the community) want to tackle:</p> <ul> <li><strong>Understanding</strong>: hybrid models with a few (4-6) attention layers perform very well, even better than pure Mamba(-2) or Transformer++. What are these attention layers doing? Can they be replaced with another mechanism?</li> <li><strong>Training optimizations</strong>: though SSD might be faster than attention, Mamba-2 as a whole might still be slower than Transformers at short (e.g. 2K) sequence length, since the MLP layers in Transformers are very hardware-friendly. Our implementation of SSD does not specifically take advantage of new features on H100 GPUs, and we look forward to future optimizations that could make SSMs faster to train than Transformers for large-scale pretraining at 2-4K sequence length.</li> <li><strong>Inference optimizations</strong>: there‚Äôs a whole suite of optimizations tailored to Transformers, in particular handling the KV cache (quantization, speculative decoding). How would the inference landscape change if model states (e.g. SSM states) no longer scale with context length, and KV cache is no longer the bottleneck?</li> </ul>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry></feed>