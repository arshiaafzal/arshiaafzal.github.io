<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arshiaafzal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arshiaafzal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-20T10:41:14+00:00</updated><id>https://arshiaafzal.github.io/feed.xml</id><title type="html">blank</title><subtitle>Arshia Afzal personal page. </subtitle><entry><title type="html">LION ü¶Å Part I - Full Linear Attention</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part1-model/" rel="alternate" type="text/html" title="LION ü¶Å Part I - Full Linear Attention"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part1-model</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lion-480.webp 480w,/assets/img/lion-800.webp 800w,/assets/img/lion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/lion.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>We sincerely appreciate Albert Gu and Tri Dao for their insightful blog posts, which have been invaluable in shaping our own!</strong></p> <ol> <li>Part I - Full Linear Attention</li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - Results</a></li> </ol> <p>Recently, Transformers with Linear Attention and State Space Models (SSMs) have gained significant popularity for causal sequence modeling due to their ability to efficiently support both parallel training and RNN-like inference. These models have demonstrated impressive accuracy in causal tasks, particularly in causal language modeling. However, their evaluation in bi-directional sequence modeling, such as image classification and masked language modeling, has been relatively limited. In contrast, SSMs, particularly Mamba, have been recently evaluated in vision tasks, including models like Vision Mamba and Hydra, which represent official extensions of Mamba for bi-directional sequence modeling.</p> <p>We‚Äôre curious to explore whether Linear Attention Transformers, including the simple Linear Transformer and RetNet or simple selctive varient, can perform effectively on bi-directional sequence modeling. Or more precicley what modifications are needed to adapt them for tasks like image classification and masked language modeling? üòä</p> <p>Let‚Äôs break this down with three key questions:</p> <h3 id="question-1-applicability">Question 1 (Applicability)</h3> <p>Given that Linear Transformers can be formulated as RNNs and offer efficiency benefits during inference, alongside parallel training for causal sequence modeling, can they also exhibit similar benefits for bi-directional processing? If so, what would the parallel form look like, and what would be the equivalent bi-directional RNN form?</p> <h3 id="question-2-performance">Question 2 (Performance)</h3> <p>Assuming we‚Äôve addressed the first question, can simple Linear Transformers‚Äîsuch as Linear Trans (cite), RetNet (cite), or even basic linear attention with a selective decay factor‚Äîperform well on bi-directional tasks, such as image classification or masked language modeling?</p> <h3 id="question-3-training-throughput">Question 3 (Training Throughput)</h3> <p>While bi-directional SSMs like Hydra and Vision Mamba show impressive performance on bi-directional sequence modeling tasks, they tend to be difficult and slow to train compared to Transformers with full attention (e.g., ViT and BERT). If we‚Äôve answered the first two questions affirmatively, can Linear Transformers match the accuracy of deep bi-directional SSMs while maintaining the training throughput of softmax Transformers and the inference efficiency of RNNs/SSMs? Also, maybe we can achive this without need for CUDA kernel programming and simply using torch ;)</p> <h2 id="from-causal-to-full-linear-attention">From Causal to Full Linear Attention</h2> <p>Let‚Äôs start with Linear Attention Reccurence:</p> \[\begin{aligned} &amp; S_i = S_{i-1} + k_i v^\top_i, \quad z_i = z_{i-1} + k_i, \\ &amp; Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Above is the RNN form of the Linear Attention which have the parallel form of:</p> \[\mathbf{Y} = Scale \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>and the mask $\mathbf{M}^C$ is a lower triangular binary matrix. Causal Linear Transformers are a class of models introduced following the development of Linear Transformers as shown above (cite). These models typically define a recurrence of the form:</p> \[\begin{aligned} S_i = \boldsymbol{\Lambda_i} \star S_{i-1} + \gamma_i k_i v^\top_i, \quad z_i = \boldsymbol{\Lambda_i} \star z_{i-1} + \gamma_i k_i, \\ Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Here, \(\boldsymbol{\Lambda_i}\) and \(\gamma_i\) are decay factors introduced after Linear Transformers to enhance their performance. (Spoiler alert ‚ö†Ô∏è: this family of Linear Transformers has strong connections to SSMs, as explored in works like (DeltaNet) and (Mamba-2) üòâ). Many models apply a non-linear activation to queries and keys, such that $\mathbf{k}_i = \phi(\mathbf{k}_i)$ and $\mathbf{q}_i = \phi(\mathbf{q}_i)$. To avoid notation clutter, we omit explicitly writing $\phi(.)$ everywhere. By default, we assume that queries and keys are already non-linearized. For simplicity, we consider \(\boldsymbol{\Lambda_i} = \lambda_i\) as a scalar in this study. As shown, this choice is as effective as the full matrix form. We now present the general scaled linear attention in the following form:</p> \[\begin{aligned} S_i &amp;= \lambda_i S_{i-1} + \gamma_i k_i v^\top_i,\\ z_i &amp;= \lambda_i z_{i-1} + \gamma_i k_i, \\ y_i &amp;= \frac{q^\top_i S_i}{q^\top_i z_i} \\ \end{aligned}\] <p>The first goal is to extend the causal linear attention parallel form</p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>to a fully <em>scaled</em> and <em>masked</em> attention mechanism for linear attention.</p> <h2 id="creating-scaled-and-masked-full-attention">Creating Scaled and Masked Full Attention</h2> <p>The first step is quite simple: the masked and scaled attention can naturally take the following form, as suggested by its name:</p> <blockquote class="block-tip"> <p><strong>Full Linear Attention</strong></p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} \right)\] </blockquote> <p>The important part is how to well define the matrix \(\mathbf{M}\). A natural choice is to extend the causal mask \(\mathbf{M^C}\), where the causal mask between tokens \(i,j\) is given by \(\mathbf{M}^C_{ij} = \lambda_{j+1} \lambda_{j+2} \dots \lambda_i\), representing the product of all selective scalers between \(i\) and \(j\). In the bidirectional case, the full mask should preserve this property. Since this is indeed a desirable property, one can interpret it as a form of relative positional encoding between two tokens. Saying so the mask cen be shaped as:</p> \[\begin{aligned} \mathbf{M}_{ij} = \begin{cases} \Pi_{k=j+1}^{i}{\lambda_k}, &amp; i &gt; j \\ 1 &amp; i=j\\ \Pi_{k=i+1}^{j}{\lambda_k}, &amp; i &lt; j. \end{cases} \end{aligned}\] <p>To recap, the full output of full LInear Attention can be presented as:</p> <p><span style="font-size: 0.75em;"> \(\mathbf{Y} = Scale \left( \underbrace{\left( \renewcommand*{\arraystretch} \begin{array}{ccccc} \mathbf{q}_1^{\top}\mathbf{k}_1 &amp; \mathbf{q}_1^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_1^{\top}\mathbf{k}_L \\ \mathbf{q}_2^{\top}\mathbf{k}_1 &amp; \mathbf{q}_2^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_2^{\top}\mathbf{k}_L\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{q}_L^{\top}\mathbf{k}_1 &amp; \mathbf{q}_L^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_L^{\top}\mathbf{k}_L\\ \end{array} \right)}_{\hspace{1mm} \mathbf{A} = \mathbf{Q} \mathbf{K}^{\top}} \odot \underbrace{ \left( \renewcommand*{\arraystretch} \begin{array}{ccccc} 1 &amp; \lambda_2 &amp; \lambda_2 \lambda_3 &amp; \cdots &amp; \lambda_2 \cdots \lambda_L \\ \lambda_1 &amp; 1 &amp; \lambda_3 &amp; \cdots &amp; \lambda_3 \cdots \lambda_L \\ \lambda_1 \lambda_2 &amp; \lambda_2 &amp; 1 &amp; \cdots &amp; \lambda_4 \cdots \lambda_L \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \lambda_{L-1} \cdots \lambda_1 &amp; \lambda_{L-1} \cdots \lambda_2 &amp; \lambda_{L-1} \cdots \lambda_3 &amp; \cdots &amp; 1 \\ \end{array} \right) }_{\hspace{1mm} \mathbf{M}} \right) \left( \renewcommand*{\arraystretch} \begin{array}{c} \mathbf{v}_1^\top \\ \mathbf{v}_2^\top \\ \mathbf{v}_3^\top \\ \vdots \\ \mathbf{v}_L^\top \\ \end{array} \right)\) </span></p> <p>The above represents the full <strong>Li</strong>near Attenti<strong>on</strong> in parallel form, which also inspired the name of our framework, <strong>LION</strong> ü¶Å. Now that we have established full linear attention for bidirectional sequence modeling, it‚Äôs time to derive its equivalent bidirectional RNN.</p> <h3 id="an-important-question"><strong>An Important Question:</strong></h3> <blockquote> <p><strong>Question:</strong> Is it worth using full attention with quadratic memory for bidirectional sequence modeling?</p> </blockquote> <p>The answer is <strong>yes</strong>! For real-world bidirectional tasks such as Vision ($L=196$) and Masked Language Modeling (MLM) ($L=128$), sequence lengths are relatively short. This means that using full attention actually increases the model‚Äôs throughput without a significant trade-off in complexity.</p> <p>Unlike causal language modeling, where sequences can become extremely long, bidirectional tasks in real datasets do not suffer from the same scalability challenges.</p> <p>(That being said, we believe that architectures designed for causal tasks do not directly transfer to bidirectional tasks without proper modifications. ¬Ø\<em>(„ÉÑ)</em>/¬Ø )</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>We introduce our framework, <strong>LION</strong>, which derives an equivalent bidirectional RNN for full linear attention.</p> </li> <li> <p>Within this framework, we demonstrate how various Linear Transformers can be extended to their bidirectional counterparts.</p> </li> <li>We explore the construction of stable masks (\mathbf{M}), enabling models using LION to: <ul> <li>Train in parallel using full attention.</li> <li>Infer efficiently like an RNN.</li> </ul> </li> <li>Finally, we introduce a <strong>chunkwise parallel</strong> variant of LION to balance recurrence and parallelism üôÇ.</li> </ul>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">LION ü¶Å Part II - Bi-directional RNN</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory/" rel="alternate" type="text/html" title="LION ü¶Å Part II - Bi-directional RNN"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li>Part II - Bi-directional RNN</li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - Results</a></li> </ol> <p>In <a href="/blog/2024/mamba2-part1-model/">Part I</a> of this series, we defined full linear attention with masking and scaling.<br/> Similar to all linear transformers designed for causal sequence modeling, we aim to derive an RNN form for efficiency during inference.<br/> In this section, we establish and theoretically demonstrate the equivalent bidirectional RNN for the Linear Transformer.</p> <h2 id="finding-bidirectional-rnn-equal-to-full-linear-attention">Finding Bidirectional RNN Equal to Full Linear Attention</h2> <p>Let‚Äôs start by separating the upper, lower, and diagonal elements of the attention matrix and the mask. Since the idea of a bidirectional RNN is to process the sequence in both the forward order (from first to last) and the reverse order (from last to first), these naturally correspond to the upper and lower parts of the attention matrix and mask.</p> <p>Ideally, we aim to construct an RNN that is equivalent to the masked and scaled Linear Attention. Let‚Äôs start by seperating upper and lower parts of the attention and mask:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_mask_color-480.webp 480w,/assets/img/att_mask_color-800.webp 800w,/assets/img/att_mask_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/att_mask_color.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Note:</strong> We made a strong effort to maintain a consistent color coding for this section of the blog post and throughout our paper :).</p> <ul> <li>Wherever you see a <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Yellow</span> color, it indicates the <strong>upper part of the matrix (non-causal)</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Red</span> color, it represents the <strong>diagonal elements</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Blue</span> color, it corresponds to the <strong>lower triangular (causal) part</strong>.</li> </ul> <p>Let‚Äôs seperate the attention into upper and lower parts:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_sep-480.webp 480w,/assets/img/att_sep-800.webp 800w,/assets/img/att_sep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/att_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This formulation represents both the causal and non-causal forms of attention. Ideally, we aim to model each triangular part using an RNN.Similarly, we can also separate the mask in the same way:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mask_sep-480.webp 480w,/assets/img/mask_sep-800.webp 800w,/assets/img/mask_sep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mask_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let‚Äôs also write the scaling part of the masked attention $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ as:</p> \[\begin{aligned} \mathbf{Y} = \big(\text{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V} = (\mathbf{C}^{-1}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}))\mathbf{V}, \hspace{1mm} \mathbf{C}_i = \mathbf{q}^{\top}_i\sum\limits_{j=1}^{L} \mathbf{M}_{ij}\mathbf{k}_j. \end{aligned}\] <p>Also, we can decompose the scaling matrix \(\mathbf{C}_i\) as:</p> \[\begin{aligned} \mathbf{C}_{i}= \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=1}^{i} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^F_i} + \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=i}^{L} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^B_i} \end{aligned}\] <p>Now we replace tha bove scaling matrix $\mathbf{C}$ in the output of the attention form of $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ .Interestingly, many terms naturally cancel out with each other.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/proofC-480.webp 480w,/assets/img/proofC-800.webp 800w,/assets/img/proofC-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/proofC.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This results in only the forward and backward directions of the RNN remaining. As observed, the forward path aligns with causal linear attention with masking. Now, we need to demonstrate that the backward path follows the same RNN structure in the reverse direction. We can simply flip the upper triangular matrices using the <a href="https://en.wikipedia.org/wiki/Exchange_matrix">exchange matrix</a> \(\mathbf{J}_L\) and the function \(F(X) = \mathbf{J}_L X \mathbf{J}_L\):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flip-480.webp 480w,/assets/img/flip-800.webp 800w,/assets/img/flip-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/flip.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Cool! Now, both the upper part (equivalent to the RNN in the forward direction) and the lower part (equivalent to the RNN in the backward direction) can be formulated as RNNs. This is exactly what we need to construct our bidirectional RNN equivalent to full linear attention.</p> <blockquote class="block-tip"> <p><strong>LION: Reccurence form</strong></p> \[\begin{aligned} \mathbf{S}_i^{F/B} &amp;= \lambda_i \mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\ \mathbf{z}^{F/B}_i &amp;= \lambda_i \mathbf{z}^{F/B}_{i-1} + \mathbf{k}_i, \\ c^{F/B}_i &amp; = \mathbf{q}_i^{\top} \mathbf{z}^{F/B}_{i} - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}, \\ \mathbf{y}^{F/B}_i &amp;= \mathbf{q}_i^{\top} \mathbf{S}^{F/B}_i - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2} \mathbf{v}_i, \\ out&amp;put: \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i}. \\ \end{aligned}\] </blockquote> <p>The terms $\frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}$ and $\frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}$ are subtracted to avoid double counting. This bi-directional RNN is equivalent to scaled and masked linear attention described in previous section of this blogpost.</p> <h2 id="some-important-details-of-our-rnn">Some Important details of our RNN</h2> <blockquote> <p>Only the states \(c^{F/B}_i\) and \(\mathbf{y}^{F/B}_i\) are stored per token, resulting in \(\mathcal{O}(Ld)\) memory usage. In contrast, naively storing full matrix-valued hidden states would require \(\mathcal{O}(Ld^2)\), which becomes infeasible for large models.</p> </blockquote> <blockquote> <p>Forward and backward recurrences run independently, completing in \(L\) time steps with \(L\) memory units, compared to \(2L\) in the naive approach.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/memory-480.webp 480w,/assets/img/memory-800.webp 800w,/assets/img/memory-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/memory.png" width="100%" height="auto" title="Memory Allocation of LION in RNN form" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Memory allocation in LION during Forward and Backward recurrences.</figcaption> </figure> <p>All in one we can visulaize our framework nicely like:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frlion-480.webp 480w,/assets/img/frlion-800.webp 800w,/assets/img/frlion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/frlion.png" width="100%" height="auto" title="LION" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">LION ü¶Å: Our framework for training in parallel using Full Linear Attention which also supports the efficient bi-directional RNN format.</figcaption> </figure> <h2 id="different-masks-of-lion">Different Masks of LION</h2> <p>Now that we have created our framework let‚Äôs see what are the choices of the decay factor \(\lambda_i\) and how they resemble the famous linear Transformer models. Let‚Äôs set:</p> <blockquote> <p>$\lambda_i=1$ this results in mighty simple Linear Transformer (cite) which we refrer to as <span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">LION-üî• </span> which is LION-Lit resembling Linear Transformer.</p> </blockquote> <blockquote> <p>$\lambda_i=\lambda$ this results in mighty RetNet (cite) which we refrer to as <span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">LION-D </span></p> </blockquote> <blockquote> <p>$\lambda_i=\sigma(\mathbf{W}\mathbf{x}_i)$ being input dependent, and bi-directional Linear Transformer inspired by selectivity of Mamba2 (cite) which we refrer to as <span style="background-color: rgb(255, 233, 211) ; padding: 3px; color:black">LION-S </span></p> </blockquote> <p>We evaluate all above models, extended to bidirectional sequence modeling using LION, on several bidirectional tasks. Also as all Linear Transformers use feature mapping $\phi(.)$ to queries and keys we also applied SILU shifted $\phi(x)=$ <code class="language-plaintext highlighter-rouge">(SILU(x)+0.5)/(norm(SILU(x)+0.5))</code> non-linear activation function. Let‚Äôs delve deep in each of these models in LION framework.</p> <h3 id="lion-">LION-üî•</h3> <p>LION-üî• is an extension of the very first Linear Transformer (cite). Without any masking, the bidirectional parallel form can be simply written as:</p> \[\mathbf{Y} = Scale(\mathbf{Q} \mathbf{K}^\top )\mathbf{V}\] <p>and the RNN form of the above parallel full linear attention is simply the RNN form mentioned above in this section in green box just by simply not using any mask.</p> <h3 id="lion-d">LION-D</h3> <p>By fixing \(\lambda_i = \lambda\), the mask \(\mathbf{M}\) has the form:</p> \[\begin{align} \mathbf{M}_{ij} = \lambda^{|i-j|}, \quad \mathbf{D}_{ij} = |i-j|\log(\lambda), \quad \mathbf{M} = \exp(\mathbf{D}). \notag \end{align}\] <p>\(\mathbf{M}\) above is a Toeplitz mask cite(tnn) and therefore, creating the decay mask can be made even faster using simple PyTorch commands. To ensure numerical stability, we bound the parameter \(\lambda\) using the <strong>sigmoid function</strong>, setting \(\lambda = \sigma(a)\). Without this constraint, the scalar \(\lambda^L\) could become excessively large, leading to instability. Additionally, as we all know, summation is generally more numerically stable than multiplication. Therefore, in some cases, instead of multiplying a matrix repeatedly, we can leverage summation for improved stability. However, in practice, for <strong>RetNet-style masks</strong> with a fixed decay, multiplication remains stable. This allows for a more straightforward implementation when generating the mask in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Decay_Mask</span><span class="p">(</span><span class="n">a</span> <span class="p">,</span> <span class="n">L</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">L</span><span class="p">))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s">LION-S</h3> <p>Observing the structure of $\mathbf{M}$, its upper ($\mathbf{M}^B$) and lower ($\mathbf{M}^F$) triangular parts are rank-1 <a href="https://people.cs.kuleuven.be/~raf.vandebril/homepage/publications/papers_html/qrq_07/node16.html">semi-separable matrices</a> (cite), allowing for efficient computation via matrix multiplications.</p> <p>During training, the decay factors $\lambda_i$ are stacked into ${\lambda}^F \in \mathbb{R}^L$, and the cumulative product</p> \[\mathbf{L}^F = cumprod(\lambda^F) = \prod_{k=0}^{i} \lambda^F_k\] <p>is used to generate the lower triangular mask (\mathbf{M}^F). For the upper triangular mask $\mathbf{M}^B$, the input sequence is flipped, and the decay factors are computed as</p> \[\boldsymbol{\lambda}^B = \text{Flip}(\boldsymbol{\lambda}^F), \quad \mathbf{L}^B = cumprod(\boldsymbol{\lambda}^B).\] <p>The masks are then constructed as, $\mathbf{M}^F =$ <code class="language-plaintext highlighter-rouge">tril(LF@inv(LF)^T)</code> for the forward part and $\mathbf{M}^B =$ <code class="language-plaintext highlighter-rouge">triu(LB@inv(LB)^T)</code> for the backward part. where <code class="language-plaintext highlighter-rouge">tril(.)</code> and <code class="language-plaintext highlighter-rouge">trilu(.)</code> extract the lower and upper triangular parts of the input matrix respectively. The full mask is then obtained as</p> \[\mathbf{M} = \mathbf{M}^F + \mathbf{M}^B - \mathbf{I}.\] <p>To improve numerical stability, the selective scalar $\lambda_i$ is designed in exponential form</p> \[\lambda_i = e^{a_i}.\] <p>This results in the cumulative sum:</p> \[\mathbf{D}^F_{ij} = \begin{cases} \sum_{k=i}^{j+1} a_k, &amp; \text{if } i &gt; j, \\ \sum_{k=i+1}^{j} a_k, &amp; \text{if } i &lt; j, \\ 0, &amp; \text{if } i = j, \end{cases}\] \[\mathbf{M^F} = \exp(\mathbf{D^F}),\] <p>where $\exp(\cdot)$ is applied element-wise. The same process applies to $\mathbf{M}^B$ by flipping the input sequence order.</p> <p>Here, $\mathbf{D}^{F/B} = cumsum(\mathbf{a}^{F/B})$, where $\mathbf{a} \in \mathbb{R}^L$ contains the selective exponents $a_i$.</p> <p>Ensuring stability is crucial, as $\mathbf{L}^{F/B}$ can overflow or underflow when forming the full mask without chunking. To mitigate this, we define</p> \[a_i = \log(\sigma(\mathbf{W}_{a}^\top\mathbf{x}_i + b)),\] <p>where $\sigma(.)$ is the sigmoid function. This approach ensures numerical stability by bounding $a_i$ within the interval $[0,1]$.</p> <p><strong>Note:</strong> It is crucial that the activation function is a <strong>sigmoid</strong>, as other activations do not produce stable masks and can lead to NaN values in the loss function. To maintain stability, <strong>chunking</strong> is required during training. This issue has been specifically highlighted in the <strong>Mamba2</strong> blog post.<br/> We provide a detailed explanation in the <strong>Results</strong> section of this blog post, where we discuss why using <strong>full attention</strong> is beneficial for achieving <strong>high throughput</strong> during training.</p> <p>The code for building the mask of LION-S is so simple and flexible even in Pytorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_causal_mask_lions</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">cumsum</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">,</span> <span class="mi">1</span><span class="o">/</span> <span class="p">(</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span> <span class="p">)</span>  <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Selective_Mask</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">vec_shape</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">vec</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">A_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">vec</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">vec</span><span class="p">.</span><span class="n">device</span><span class="p">)),</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,:,</span><span class="mi">1</span><span class="p">:].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">A_for</span> <span class="o">+</span> <span class="n">A_back</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <h2 id="lion-attention-block">LION Attention Block</h2> <p>We can formulate the parallel attention form of LION as shown below, supporting all three extensions of our main experiments:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Class</span> <span class="nc">LION_Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">qk_scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">proj_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">silunorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">Mask_type</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">qk_scale</span> <span class="ow">or</span> <span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="n">self</span><span class="p">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">=</span> <span class="n">Mask_type</span>

        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
            <span class="bp">None</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">non_lin</span> <span class="o">=</span> <span class="n">silu_shifted</span>
        <span class="n">self</span><span class="p">.</span><span class="n">silunorm</span> <span class="o">=</span> <span class="n">silunorm</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">a_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">a_i</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">a_i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Selective_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Decay_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">attn</span>

        <span class="c1"># Scaling
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">attn</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn</span> <span class="o">@</span> <span class="n">v</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <blockquote> <p><strong>Question:</strong> As seen above, the <strong>RNN</strong> is more efficient than the <strong>Transformer</strong> since it only requires storing the output for each token, resulting in a memory complexity of <strong>$\mathcal{O}(Ld)$</strong>, as opposed to storing the full attention matrix, which requires <strong>$\mathcal{O}(L^2 d)$</strong>. Can we achieve a balance between the speed of attention parallelism and the efficiency of an RNN?</p> </blockquote> <p>We will answer this question in our next section by introducing LION-Chunk.</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>In the next section of this series, we will describe how to apply a <strong>chunkwise parallel form</strong> for LION, allowing us to balance between the <em>RNN structure</em> and the <em>attention-based</em> formulation.</p> </li> <li> <p>We show the numercial results and experiments on Imagenet and C4 dataset :)</p> </li> </ul>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Part I - Full Linear Attention Part II - Bi-directional RNN Part III - Chunkwise Parallel from of LION Part IV - Results]]></summary></entry><entry><title type="html">LION ü¶Å Part III - Chunkwise Parallel from of LION</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm/" rel="alternate" type="text/html" title="LION ü¶Å Part III - Chunkwise Parallel from of LION"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part3-algorithm/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li>Part III - Chunkwise Parallel from of LION</li> <li><a href="/blog/2024/mamba2-part4-systems/">Part IV - Results</a></li> </ol> <p>Since we have now established the LION theorem, which maps full linear attention into a bidirectional RNN in <a href="/blog/2024/mamba2-part2-theory/">Part II</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet and GLA, as well as the SSD algorithm in Mamba2, a chunkwise parallel form of full linear attention could be an effective solution. Additionally, in models like Hydra, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking full linear attention, particularly for LION-S and LION-D, where the decay factor is fixed and the mask $\mathbf{M}$ follows a Toeplitz structure? But it is important that this chunkwise form is particularly useful for <strong>inference</strong>, since during training, as we said, full linear attention will provide the highest throughput, especially for short sequences, which is the case for bidirectional tasks. The aim of chunking full attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does not require chunking during training, unlike SSMs such as Hydra.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>Chunkwise parallel form of full linear attention is kinda straight forward lets start by chunking the queries keys and values:</p> \[\mathbf{Q}_{[i]} , \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\] <p>we should now shape the chunkwise form which consist of four parts:</p> <ul> <li>Chunkwise form of the attention matrix $\mathbf{A}_{[ij]}$</li> <li>Chunkwise form for the scaling matrix $\mathbf{C}$ which the chunkwise form can be written as $\mathbf{C}_{[ij]}$</li> <li>The chunked hidden state to shape the unscaled output $\mathbf{S}_{[ij-1]}$</li> <li>Finally the output of the chunk $i$ which is $\mathbf{Y}_{[i]}$</li> </ul> <p>using these chunked matrices we shape the full linear atteniton in chunk form as bellow:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \\ \mathbf{Y}_{[i]} &amp; = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. And $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full maks $\mathbf{M}$ at chunk $ij$ like:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Now that this has been stated and proven, we will describe how to construct the <strong>chunkwise mask</strong> from the attention mask $\mathbf{M}$, particularly for the <strong>fixed</strong> and <strong>selective</strong> masks in our framework. The chunkwise form of the mask for chunk of $i$ and $j$ is annotated as $\mathbf{M}_{[ij]}$.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Casual_Mask_Decay_Partial</span><span class="p">(</span><span class="n">a_i</span> <span class="p">,</span> <span class="n">L</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">end</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s-chunk">LION-S Chunk</h3> <h3 id="ssd-algorithm-chunking-and-state-passing">SSD Algorithm: Chunking and State Passing</h3> <p>An alternative interpretation of the algorithm involves reasoning about how the SSM operates on the actual sequence. We first split the sequence of input into blocks (or chunks) of size $\mathtt{Q}$. The steps then have the interpretation</p> <ol> <li><strong>Intra-chunk outputs</strong>: compute the local output of each chunk (<em>what is the output per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Chunk states</strong>: compute the final state of each chunk (<em>what is the final state per chunk supposing that the initial state (to the chunk) is 0?</em>)</li> <li><strong>Pass states</strong>: compute a recurrence on all of the chunks‚Äô final states ‚Äì using any desired algorithm, e.g. parallel or sequential scan (<em>what is the actual final state per chunk taking into account all previous inputs?</em>)</li> <li><strong>Output states</strong>: for each chunk, given its true initial state (computed in Step 3), compute the contribution to the output just from the initial state</li> </ol> <p>Either way, we see that most of the algorithm (Step 1, 2, and 4) leverages matmuls (and hence tensor cores), and also can be computed completely in parallel! Only Step 3 requires a scan, but it operates on a much shorter sequence and usually only takes a small fraction of the time of the full algorithm.</p> <h3 id="special-cases">Special Cases</h3> <p>We note that special cases of this algorithm have been seen before. In particular RetNet<d-cite key="sun2023retentive"></d-cite>, which we showed in Part II to be a special case of SSD, mention a ‚Äúchunkwise‚Äù algorithm which computes the quadratic form on a chunk of the input one-at-a-time and passes the final state to the next chunk. This turns out to be essentially equivalent to the SSD algorithm specialized to a restricted case (i.e. a decay matrix mask $L$). Our derivation comes from a different direction‚Äîthe block matrix decomposition‚Äîwhich also makes it more obvious how to parallelize this algorithm and make it really fast in practice.</p> <p>Other forms of ‚Äúchunkwise‚Äù recurrences have recently become popular, such as in <a href="https://arxiv.org/abs/2312.06635">Gated Linear Attention (GLA)</a><d-cite key="yang2024gated"></d-cite>.</p> <h2 id="the-code">The Code</h2> <p>In the ‚ÄúMinimal SSD‚Äù code that we provide in the paper and the <a href="https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/ssd_minimal.py">code release</a>, we delineate each of these four steps. As promised, this algorithm is not only faster but also much easier to implement than the original selective scan of Mamba, coming in at just around 25 lines of code!</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation. exp(segsum(A)) produces a 1-SS matrix,
       which is equivalent to a scalar SSM.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>

<span class="k">def</span> <span class="nf">ssd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">block_len</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">initial_states</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Arguments:
        X: (batch, length, n_heads, d_head)
        A: (batch, length, n_heads)
        B: (batch, length, n_heads, d_state)
        C: (batch, length, n_heads, d_state)
    Return:
        Y: (batch, length, n_heads, d_head)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">A</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">B</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">C</span><span class="p">.</span><span class="n">dtype</span>
    <span class="k">assert</span> <span class="n">X</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">%</span> <span class="n">block_len</span> <span class="o">==</span> <span class="mi">0</span>

    <span class="c1"># Rearrange into blocks/chunks
</span>    <span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="p">[</span><span class="nf">rearrange</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="sh">"</span><span class="s">b (c l) ... -&gt; b c l ...</span><span class="sh">"</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="n">block_len</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">)]</span>

    <span class="n">A</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h -&gt; b h c l</span><span class="sh">"</span><span class="p">)</span>
    <span class="n">A_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># 1. Compute the output for each intra-chunk (diagonal blocks)
</span>    <span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
    <span class="n">Y_diag</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bcshn,bhcls,bcshp-&gt;bclhp</span><span class="sh">"</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 2. Compute the state for each intra-chunk
</span>    <span class="c1"># (right term of low-rank factorization of off-diagonal blocks; B terms)
</span>    <span class="n">decay_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">((</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">A_cumsum</span><span class="p">))</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bclhn,bhcl,bclhp-&gt;bchpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">decay_states</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>

    <span class="c1"># 3. Compute the inter-chunk SSM recurrence; produces correct SSM states at chunk boundaries
</span>    <span class="c1"># (middle term of factorization of off-diag blocks; A terms)
</span>    <span class="k">if</span> <span class="n">initial_states</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">initial_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">states</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">([</span><span class="n">initial_states</span><span class="p">,</span> <span class="n">states</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">decay_chunk</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="nf">segsum</span><span class="p">(</span><span class="n">F</span><span class="p">.</span><span class="nf">pad</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">))))</span>
    <span class="n">new_states</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">"</span><span class="s">bhzc,bchpn-&gt;bzhpn</span><span class="sh">"</span><span class="p">,</span> <span class="n">decay_chunk</span><span class="p">,</span> <span class="n">states</span><span class="p">)</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">final_state</span> <span class="o">=</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">new_states</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

    <span class="c1"># 4. Compute state -&gt; output conversion per chunk
</span>    <span class="c1"># (left term of low-rank factorization of off-diagonal blocks; C terms)
</span>    <span class="n">state_decay_out</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">A_cumsum</span><span class="p">)</span>
    <span class="n">Y_off</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">einsum</span><span class="p">(</span><span class="sh">'</span><span class="s">bclhn,bchpn,bhcl-&gt;bclhp</span><span class="sh">'</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">states</span><span class="p">,</span> <span class="n">state_decay_out</span><span class="p">)</span>

    <span class="c1"># Add output of intra-chunk and inter-chunk terms (diagonal and off-diagonal blocks)
</span>    <span class="n">Y</span> <span class="o">=</span> <span class="nf">rearrange</span><span class="p">(</span><span class="n">Y_diag</span><span class="o">+</span><span class="n">Y_off</span><span class="p">,</span> <span class="sh">"</span><span class="s">b c l h p -&gt; b (c l) h p</span><span class="sh">"</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">Y</span><span class="p">,</span> <span class="n">final_state</span>
</code></pre></div></div> <h2 id="the-details">The Details</h2> <p>Let‚Äôs talk about a couple of additional details in the implementation (these don‚Äôt even appear in the full paper, so pay attention!) that unpack some of the choices in this reference code.</p> <h3 id="the-ssm-scan">The SSM Scan</h3> <p>In the above code, we utilized the connection between scalar SSM recurrences</p> \[h_{t+1} = A_t h_t + B_t x_t\] <p>and matrix multiplication by 1-semiseparable matrices</p> \[L = \begin{bmatrix} 1 &amp; \\ a_1 &amp; 1 &amp; \\ a_2a_1 &amp; a_2 &amp; 1 \\ \vdots &amp; \vdots &amp; \ddots &amp; \ddots \\ a_{\mathtt{T}-1}\dots a_1 &amp; a_{\mathtt{T}-1}\dots a_2 &amp; \dots &amp; a_{\mathtt{T}-1} &amp; 1 \\ \end{bmatrix}\] <p>which we covered in Part II (and Section 3.2.2 of the paper). In this minimal implementation, we compute Step 3 of the algorithm, which is computing a scalar SSM by <em>any</em> algorithm of our choice, by explicitly materializing a 1-SS matrix and doing dense matrix multiplication.</p> <p>We use this version for several reasons:</p> <ol> <li>Code-wise, it‚Äôs simpler to materialize and multiply by this matrix than to actually implement a parallel associative scan</li> <li>Because of the block decomposition of the SSM matrix, the sequence length $\mathtt{T}$ is reduced by a factor of $\approx 100$ ‚Äì so doing the scan in time $O(\mathtt{T}^2)$ instead of $O(\mathtt{T})$ isn‚Äôt too bad</li> <li>We have to materialize a 1-SS matrix anyways for Step 1 of the algorithm (the diagonal blocks), so might as well reuse the code ¬Ø\_(„ÉÑ)_/¬Ø</li> </ol> <p>While this example code is simpler and reasonably efficient on GPU (and probably TPU as well!), it‚Äôs no longer truly linear at long sequences. Our more optimized Triton implementation does replace the 1-SS multiplication in Step 3 with an actual associative scan.</p> <h3 id="stability">Stability</h3> <h4 id="attempt-1-ratios-of-cumprods">Attempt 1: Ratios of cumprods</h4> <p>The first naive attempt may be to notice that the entries of this matrix are cumulative products</p> \[a_{i:j}^\times = a_i \times \cdots \times a_{j-1} = \frac{a_{i:\mathtt{T}}^\times}{a_{j:\mathtt{T}}^\times}\] <p>However, this runs into severe numerical issues because these products can get really tiny (imagine $a_t \approx 0.9$ and powering it up for a sequence length $\mathtt{T}$ in the thousands!)</p> <h4 id="fix-1-the-segment-sum-segsum-operation">Fix 1: The Segment Sum (<code class="language-plaintext highlighter-rouge">segsum</code>) Operation</h4> <p>The second attempt would be to do all of this in log-space, because all the $a_t$ are positive; so the products become additions, and instead of <code class="language-plaintext highlighter-rouge">cumprod</code>s to deal with we have <code class="language-plaintext highlighter-rouge">cumsum</code>s instead. Then in order to compute the 1-SS matrix, we just have to compute the sums $\log a_i + \dots + \log a_{j-1}$ for every <em>segment</em> $[i:j]$. We call this the <strong>segment sum (segsum)</strong> primitive, analogous to cumulative sum (cumsum).</p> <h4 id="attempt-2-differences-of-cumsums">Attempt 2: Differences of cumsums</h4> <p>The obvious way to do this again is using the same idea as above, but in log space</p> \[a_{i:j}^\times = \exp\left( \log a_i + \cdots + \log a_{j-1} \right) = \left( (\log a)_{i:\mathtt{T}}^+ - (\log a)_{j:\mathtt{T}}^+ \right)\] <p>where we compute a single cumulative sum of $a$ along the time axis, and then compute all pairwise differences. In code, we can do this with</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">segsum_unstable</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">Naive segment sum calculation.</span><span class="sh">"""</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="p">:,</span> <span class="bp">None</span><span class="p">]</span> <span class="o">-</span> <span class="n">x_cumsum</span><span class="p">[...,</span> <span class="bp">None</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="p">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">),</span> <span class="n">diagonal</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">x_segsum</span> <span class="o">=</span> <span class="n">x_segsum</span><span class="p">.</span><span class="nf">masked_fill</span><span class="p">(</span><span class="o">~</span><span class="n">mask</span><span class="p">,</span> <span class="o">-</span><span class="n">torch</span><span class="p">.</span><span class="n">inf</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x_segsum</span>
</code></pre></div></div> <p>(and then the 1-semiseparable matrix is just the exponential of this output).</p> <p>Sums/differences are a lot more stable than products/quotients, so this should work ‚Äì right?</p> <h4 id="fix-2-remove-all-subtractions">Fix 2: Remove All Subtractions</h4> <p>Unfortunately, it turns out this still doesn‚Äôt work. The values of this 1-SS matrix roughly represent the SSM dynamics, which are very sensitive to these values of $a_t$, so we have to be very precise. And even in log space, these cumsums can be fairly large, which runs into <a href="https://en.wikipedia.org/wiki/Catastrophic_cancellation">catastrophic cancellation</a> when subtracted. So we really have to find a way to compute this matrix with only additions, while still vectorizing everything‚Ä¶</p> <h4 id="attempt-3-stable-segsum">Attempt 3: Stable Segsum</h4> <p>This leads to the helper function in the reference SSD code. Instead of computing a single cumsum and then subtracting, we find a way to use a batch of independent cumsums that immediately produces the right answer without subtraction.</p> <p>These details do matter! Without the right implementation of these primitives, the basic SSD algorithm produces NaNs immediately during training (even with FP32).</p> <h3 id="discretization">Discretization</h3> <p>This lineage of structured state space models developed from <a href="https://arxiv.org/abs/2111.00396">S4</a> and <a href="https://arxiv.org/abs/2110.13985">its</a> <a href="https://arxiv.org/abs/2008.07669">predecessors</a> which were viewed as continuous-time systems.<d-cite key="gu2023thesis"></d-cite><d-cite key="gu2022efficiently"></d-cite><d-cite key="gu2021combining"></d-cite><d-cite key="gu2020hippo"></d-cite></p> <p>In Mamba, however, we don‚Äôt really view the SSM as continuous anymore. In fact, as mentioned in the Discussion (Section 5) of the <a href="https://arxiv.org/abs/2312.00752">original paper</a>, Mamba trades off with S4 on modeling different types of data:</p> <ul> <li>S4 is a continuous-time model that excels at modeling continuous data, e.g. perceptual signals such as audio waveforms and pixel-level vision.</li> <li>Mamba is a discrete-time model that excels at modeling discrete data, e.g. tokenized data such as language.</li> </ul> <p>However, the parameterization of Mamba still used the same discretization step as in prior structured SSMs, where there is another parameter $\Delta$ being modeled. We do this because the discretization step has other side effects such as properly normalizing the activations <d-cite key="gu2023train"></d-cite><d-cite key="orvieto2023resurrecting"></d-cite> which is important for performance.</p> <p>The initializations and parameterizations from the previous <a href="https://arxiv.org/abs/2206.12037">theory on structured SSMs</a> still work out-of-the-box, so why fix what‚Äôs not broken?</p> <p>Despite this, we‚Äôre pretty sure that the discretization step isn‚Äôt really necessary for Mamba. In the Mamba-2 paper, we chose to work directly with the ‚Äúdiscrete parameters‚Äù $A$ and $B$, which in all previous structured SSM papers (including Mamba-1) were denoted $(\bar{A}, \bar{B})$ and defined through an additional transformation</p> \[\begin{align*} \bar{A} &amp;= \exp(e^{\Delta A}) \\ \bar{B} &amp;= (\exp(e^{\Delta A}) - I) A^{-1} B \end{align*}\] <p>This doesn‚Äôt pose any problems: to use the continuous SSM parameterization, simply transform the parameters through the above formulas before plugging into the SSD code above.</p> <p>In the full Mamba-2 code, we also kept the same parameterization and discretization step as in Mamba‚Äîagain, why fix what‚Äôs not broken?‚Äîbut hypothesize that ‚Äúdiscrete-centric‚Äù variants (such as the <em>gamma normalization</em> of <a href="https://arxiv.org/abs/2303.06349">LRU</a><d-cite key="orvieto2023resurrecting"></d-cite> and <a href="https://arxiv.org/abs/2402.19427">Griffin</a><d-cite key="de2024griffin"></d-cite>) should work equally well.</p> <blockquote class="block-tip"> <h4 id="is-discretization-necessary">Is Discretization Necessary?</h4> <p>It‚Äôs useful for other structured SSMs, but perhaps not needed for Mamba. But it‚Äôs just a simple invertible transformation, so use either discrete or continuous parameterizations as you like!</p> </blockquote> <h2 id="whats-next">What‚Äôs Next</h2> <p>In the <a href="/blog/2024/mamba2-part4-systems/">final part of this series</a>, we‚Äôll continue talking about the implementation of Mamba-2, but on a more macroscopic level; about the entire neural network, instead of just details of the core SSD layer.</p> <p>We‚Äôll also talk about the actual speed of the algorithm covered in this post.</p>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Part I - Full Linear Attention Part II - Bi-directional RNN Part III - Chunkwise Parallel from of LION Part IV - Results]]></summary></entry><entry><title type="html">State Space Duality (Mamba-2) Part IV - The Systems</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems/" rel="alternate" type="text/html" title="State Space Duality (Mamba-2) Part IV - The Systems"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part4-systems/"><![CDATA[<ol> <li><a href="/blog/2024/mamba2-part1-model/">Part I - The Model</a></li> <li><a href="/blog/2024/mamba2-part2-theory/">Part II - The Theory</a></li> <li><a href="/blog/2024/mamba2-part3-algorithm/">Part III - The Algorithm</a></li> <li>Part IV - The Systems</li> </ol> <p>Transformers have benefited from 7 years of systems optimization from the whole research community and large companies. The SSD framework draws connections between SSMs and attention, and allows us to implement many of these optimizations for models like Mamba-2 as well. We focus on tensor parallel and sequence parallel for large-scale training, as well as variable-length sequences for efficient finetuning and inference.</p> <h2 id="systems-and-scaling-optimizations">Systems and Scaling Optimizations</h2> <h3 id="tensor-parallelism">Tensor Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_tp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_tp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_tp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_tp.png" width="100%" height="auto" title="Mamba-2 Tensor Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>One difficulty with large-scaling training of Mamba-1 using tensor parallelism (TP) is that it requires 2 all-reduces per layer, compared to just 1 all-reduce per attention or MLP layer in Transformer. This is because some of the SSM parameters are functions of the inner activations, not of the input to the layer. In Mamba-2, with the ‚Äúparallel projection‚Äù structure, all SSM parameters are functions of the input to the layer, and we can easily apply TP to the input projection: We split the input projection and output projection matrices into 2, 4, 8 shards, depending on the TP degree. We use a grouped norm with number of groups divisible by the TP degree, so that normalization is done separately per GPU. These changes result in 1 all-reduce per layer, instead of 2.</p> <h3 id="sequence-parallelism">Sequence Parallelism</h3> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/mamba_cp-480.webp 480w,/assets/img/2024-05-31-mamba-2/mamba_cp-800.webp 800w,/assets/img/2024-05-31-mamba-2/mamba_cp-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/mamba_cp.png" width="100%" height="auto" title="Mamba-2 Sequence Parallelism" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>When training on very long sequence length, we might need to split along the sequence length and assign different parts to different devices. There are two main forms of sequence parallelism (SP): For the residual and normalization operation: this replaces the all-reduce in TP with a reduce-scatter, residual + normalization, then all-gather. Since Mamba-2 uses the same residual and normalization structure as Transformer, this form of SP applies directly with no modification. For the attention or SSM operation, aka context parallelism (CP). For attention, one could use Ring attention to split it up along the sequence dimension. For Mamba-2, the SSD framework comes to our help once again: using the same block decomposition, we can have each GPU computing its local output and its final states, then pass the states between GPUs (using send/receive communication primitives), before updating the final output of each GPU.</p> <h3 id="variable-length">Variable Length</h3> <p>For finetuning and inference, in the same batch we often have sequences of different lengths. For Transformer, one would usually pad so all sequences have the same length (wasting computation), or implement attention specifically for variable length sequences with careful load-balancing. With SSM, we can simply treat the whole batch as a long ‚Äúsequence‚Äù, and avoid passing the states between different sequences in the batch by setting the state transition $A_t$ to 0 for tokens at the end of each sequence.</p> <h2 id="results">Results</h2> <p>How well do these optimizations work? The faster SSD algorithm allows us to increase the state dimension ($\mathtt{N}=64$ or $128$ compared to $\mathtt{N}=16$ in Mamba-1). Even though technically Mamba-2 is more restricted than Mamba-1 for the same $\mathtt{N}$, the larger state dimensions generally improve model quality. Here we show results for models trained on 300B tokens on the Pile, with Mamba-2 outperforming Mamba-1 and Pythia.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_lm_downstream-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_lm_downstream-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/blog_lm_downstream.png" width="100%" height="auto" title="Downstream Evaluations" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Standard downstream evaluations for open source models trained on the Pile</figcaption> </figure> <p>What about <strong>hybrid models</strong>? We have seen from recent and concurrent work (such as <a href="https://arxiv.org/abs/2403.19887">Jamba</a> and <a href="https://arxiv.org/abs/2405.16712">Zamba</a>) that combining Mamba layers with attention layers can improve over pure Transformer or Mamba. We validate at 2.7B parameters and 300B tokens scale that a hybrid model with just 6 attention blocks (and 58 SSD blocks) outperforms 64 SSD blocks, as well as our standard Transformer++ baseline (32 gated MLP and 32 attention blocks).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/blog_hybrid-480.webp 480w,/assets/img/2024-05-31-mamba-2/blog_hybrid-800.webp 800w,/assets/img/2024-05-31-mamba-2/blog_hybrid-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/blog_hybrid.png" width="100%" height="auto" title="Downstream Evaluations for Hybrid Models" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Downstream evaluations for hybrid Mamba/attention models</figcaption> </figure> <p>We also validated that the SSD algorithm is significantly faster than the selective scan algorithm from Mamba-1 for the same state dimension, and scales much better computationally to larger state dimensions. Getting those tensor cores to go brrr is the key!</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-480.webp 480w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-800.webp 800w,/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/2024-05-31-mamba-2/ssm_ssd_dstate.png" width="100%" height="auto" title="Mamba-2 Efficiency Benchmarks" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Efficiency benchmarks on sequence length 2K</figcaption> </figure> <h2 id="future-directions">Future Directions</h2> <p>With SSD, we have connected (linear) attention and SSMs, allowing us to design faster algorithms and implement systems optimizations for SSMs. There are still tons of exciting directions that we (and hopefully the community) want to tackle:</p> <ul> <li><strong>Understanding</strong>: hybrid models with a few (4-6) attention layers perform very well, even better than pure Mamba(-2) or Transformer++. What are these attention layers doing? Can they be replaced with another mechanism?</li> <li><strong>Training optimizations</strong>: though SSD might be faster than attention, Mamba-2 as a whole might still be slower than Transformers at short (e.g. 2K) sequence length, since the MLP layers in Transformers are very hardware-friendly. Our implementation of SSD does not specifically take advantage of new features on H100 GPUs, and we look forward to future optimizations that could make SSMs faster to train than Transformers for large-scale pretraining at 2-4K sequence length.</li> <li><strong>Inference optimizations</strong>: there‚Äôs a whole suite of optimizations tailored to Transformers, in particular handling the KV cache (quantization, speculative decoding). How would the inference landscape change if model states (e.g. SSM states) no longer scale with context length, and KV cache is no longer the bottleneck?</li> </ul>]]></content><author><name>Tri Dao</name></author><summary type="html"><![CDATA[Part I - The Model Part II - The Theory Part III - The Algorithm Part IV - The Systems]]></summary></entry></feed>