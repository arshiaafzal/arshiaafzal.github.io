<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://arshiaafzal.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://arshiaafzal.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-02-20T15:31:51+00:00</updated><id>https://arshiaafzal.github.io/feed.xml</id><title type="html">blank</title><subtitle>Arshia Afzal personal page. </subtitle><entry><title type="html">LION ü¶Å Part I - Full Linear Attention</title><link href="https://arshiaafzal.github.io/blog/2025/mamba2-part1-model/" rel="alternate" type="text/html" title="LION ü¶Å Part I - Full Linear Attention"/><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2025/mamba2-part1-model</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2025/mamba2-part1-model/"><![CDATA[<figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/lion-480.webp 480w,/assets/img/lion-800.webp 800w,/assets/img/lion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/lion.jpg" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>[<a href="https://arxiv.org/abs/2405.21060">Paper</a>] [<a href="https://github.com/state-spaces/mamba">Code</a>]</p> <p><strong>We sincerely appreciate Albert Gu and Tri Dao for their insightful blog posts, which have been invaluable in shaping our own!</strong></p> <ol> <li>Part I - Full Linear Attention</li> <li><a href="/blog/2025/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li><a href="/blog/2025/mamba2-part3-algorithm/">Part III - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2024/mamba2-part4-results/">Part IV - Results</a></li> </ol> <p>Recently, Transformers with Linear Attention and State Space Models (SSMs) have gained significant popularity for causal sequence modeling due to their ability to efficiently support both parallel training and RNN-like inference. These models have demonstrated impressive accuracy in causal tasks, particularly in causal language modeling. However, their evaluation in bi-directional sequence modeling, such as image classification and masked language modeling, has been relatively limited. In contrast, SSMs, particularly Mamba, have been recently evaluated in vision tasks, including models like Vision Mamba and Hydra, which represent official extensions of Mamba for bi-directional sequence modeling.</p> <p>We‚Äôre curious to explore whether Linear Attention Transformers, including the simple Linear Transformer and RetNet or simple selctive varient, can perform effectively on bi-directional sequence modeling. Or more precicley what modifications are needed to adapt them for tasks like image classification and masked language modeling? üòä</p> <p>Let‚Äôs break this down with three key questions:</p> <h3 id="question-1-applicability">Question 1 (Applicability)</h3> <p>Given that Linear Transformers can be formulated as RNNs and offer efficiency benefits during inference, alongside parallel training for causal sequence modeling, can they also exhibit similar benefits for bi-directional processing? If so, what would the parallel form look like, and what would be the equivalent bi-directional RNN form?</p> <h3 id="question-2-performance">Question 2 (Performance)</h3> <p>Assuming we‚Äôve addressed the first question, can simple Linear Transformers‚Äîsuch as Linear Trans (cite), RetNet (cite), or even basic linear attention with a selective decay factor‚Äîperform well on bi-directional tasks, such as image classification or masked language modeling?</p> <h3 id="question-3-training-throughput">Question 3 (Training Throughput)</h3> <p>While bi-directional SSMs like Hydra and Vision Mamba show impressive performance on bi-directional sequence modeling tasks, they tend to be difficult and slow to train compared to Transformers with full attention (e.g., ViT and BERT). If we‚Äôve answered the first two questions affirmatively, can Linear Transformers match the accuracy of deep bi-directional SSMs while maintaining the training throughput of softmax Transformers and the inference efficiency of RNNs/SSMs? Also, maybe we can achive this without need for CUDA kernel programming and simply using torch ;)</p> <h2 id="from-causal-to-full-linear-attention">From Causal to Full Linear Attention</h2> <p>Let‚Äôs start with Linear Attention Reccurence:</p> \[\begin{aligned} &amp; S_i = S_{i-1} + k_i v^\top_i, \quad z_i = z_{i-1} + k_i, \\ &amp; Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Above is the RNN form of the Linear Attention which have the parallel form of:</p> \[\mathbf{Y} = Scale \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>and the mask $\mathbf{M}^C$ is a lower triangular binary matrix. Causal Linear Transformers are a class of models introduced following the development of Linear Transformers as shown above (cite). These models typically define a recurrence of the form:</p> \[\begin{aligned} S_i = \boldsymbol{\Lambda_i} \star S_{i-1} + \gamma_i k_i v^\top_i, \quad z_i = \boldsymbol{\Lambda_i} \star z_{i-1} + \gamma_i k_i, \\ Scaled: y_i = \frac{q^\top_i S_i}{q^\top_i z_i}, \quad Non-Scaled: y_i = q^\top_i S_i \\ \end{aligned}\] <p>Here, \(\boldsymbol{\Lambda_i}\) and \(\gamma_i\) are decay factors introduced after Linear Transformers to enhance their performance. (Spoiler alert ‚ö†Ô∏è: this family of Linear Transformers has strong connections to SSMs, as explored in works like (DeltaNet) and (Mamba-2) üòâ). Many models apply a non-linear activation to queries and keys, such that $\mathbf{k}_i = \phi(\mathbf{k}_i)$ and $\mathbf{q}_i = \phi(\mathbf{q}_i)$. To avoid notation clutter, we omit explicitly writing $\phi(.)$ everywhere. By default, we assume that queries and keys are already non-linearized. For simplicity, we consider \(\boldsymbol{\Lambda_i} = \lambda_i\) as a scalar in this study. As shown, this choice is as effective as the full matrix form. We now present the general scaled linear attention in the following form:</p> \[\begin{aligned} S_i &amp;= \lambda_i S_{i-1} + \gamma_i k_i v^\top_i,\\ z_i &amp;= \lambda_i z_{i-1} + \gamma_i k_i, \\ y_i &amp;= \frac{q^\top_i S_i}{q^\top_i z_i} \\ \end{aligned}\] <p>The first goal is to extend the causal linear attention parallel form</p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}^C \right)\] <p>to a fully <em>scaled</em> and <em>masked</em> attention mechanism for linear attention.</p> <h2 id="creating-scaled-and-masked-full-attention">Creating Scaled and Masked Full Attention</h2> <p>The first step is quite simple: the masked and scaled attention can naturally take the following form, as suggested by its name:</p> <blockquote class="block-tip"> <p><strong>Full Linear Attention</strong></p> \[\mathbf{Y} = \text{Scale} \left(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} \right)\] </blockquote> <p>The important part is how to well define the matrix \(\mathbf{M}\). A natural choice is to extend the causal mask \(\mathbf{M^C}\), where the causal mask between tokens \(i,j\) is given by \(\mathbf{M}^C_{ij} = \lambda_{j+1} \lambda_{j+2} \dots \lambda_i\), representing the product of all selective scalers between \(i\) and \(j\). In the bidirectional case, the full mask should preserve this property. Since this is indeed a desirable property, one can interpret it as a form of relative positional encoding between two tokens. Saying so the mask cen be shaped as:</p> \[\begin{aligned} \mathbf{M}_{ij} = \begin{cases} \Pi_{k=j+1}^{i}{\lambda_k}, &amp; i &gt; j \\ 1 &amp; i=j\\ \Pi_{k=i+1}^{j}{\lambda_k}, &amp; i &lt; j. \end{cases} \end{aligned}\] <p>To recap, the full output of full LInear Attention can be presented as:</p> <p><span style="font-size: 0.7em;"> \(\mathbf{Y} = Scale \left( \underbrace{\left( \renewcommand*{\arraystretch} \begin{array}{ccccc} \mathbf{q}_1^{\top}\mathbf{k}_1 &amp; \mathbf{q}_1^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_1^{\top}\mathbf{k}_L \\ \mathbf{q}_2^{\top}\mathbf{k}_1 &amp; \mathbf{q}_2^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_2^{\top}\mathbf{k}_L\\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \mathbf{q}_L^{\top}\mathbf{k}_1 &amp; \mathbf{q}_L^{\top}\mathbf{k}_2 &amp; \cdots &amp; \mathbf{q}_L^{\top}\mathbf{k}_L\\ \end{array} \right)}_{\hspace{1mm} \mathbf{A} = \mathbf{Q} \mathbf{K}^{\top}} \odot \underbrace{ \left( \renewcommand*{\arraystretch} \begin{array}{ccccc} 1 &amp; \lambda_2 &amp; \lambda_2 \lambda_3 &amp; \cdots &amp; \lambda_2 \cdots \lambda_L \\ \lambda_1 &amp; 1 &amp; \lambda_3 &amp; \cdots &amp; \lambda_3 \cdots \lambda_L \\ \lambda_1 \lambda_2 &amp; \lambda_2 &amp; 1 &amp; \cdots &amp; \lambda_4 \cdots \lambda_L \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ \lambda_{L-1} \cdots \lambda_1 &amp; \lambda_{L-1} \cdots \lambda_2 &amp; \lambda_{L-1} \cdots \lambda_3 &amp; \cdots &amp; 1 \\ \end{array} \right) }_{\hspace{1mm} \mathbf{M}} \right) \left( \renewcommand*{\arraystretch} \begin{array}{c} \mathbf{v}_1^\top \\ \mathbf{v}_2^\top \\ \mathbf{v}_3^\top \\ \vdots \\ \mathbf{v}_L^\top \\ \end{array} \right)\) </span></p> <p>The above represents the full <strong>Li</strong>near Attenti<strong>on</strong> in parallel form, which also inspired the name of our framework, <strong>LION</strong> ü¶Å. Now that we have established full linear attention for bidirectional sequence modeling, it‚Äôs time to derive its equivalent bidirectional RNN.</p> <h3 id="an-important-question"><strong>An Important Question:</strong></h3> <blockquote> <p><strong>Question:</strong> Is it worth using full attention with quadratic memory for bidirectional sequence modeling?</p> </blockquote> <p>The answer is <strong>yes</strong>! For real-world bidirectional tasks such as Vision ($L=196$) and Masked Language Modeling (MLM) ($L=128$), sequence lengths are relatively short. This means that using full attention actually increases the model‚Äôs throughput without a significant trade-off in complexity.</p> <p>Unlike causal language modeling, where sequences can become extremely long, bidirectional tasks in real datasets do not suffer from the same scalability challenges.</p> <p>(That being said, we believe that architectures designed for causal tasks do not directly transfer to bidirectional tasks without proper modifications. ¬Ø\<em>(„ÉÑ)</em>/¬Ø )</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>We introduce our framework, <strong>LION</strong>, which derives an equivalent bidirectional RNN for full linear attention.</p> </li> <li> <p>Within this framework, we demonstrate how various Linear Transformers can be extended to their bidirectional counterparts.</p> </li> <li>We explore the construction of stable masks (\mathbf{M}), enabling models using LION to: <ul> <li>Train in parallel using full attention.</li> <li>Infer efficiently like an RNN.</li> </ul> </li> <li>Finally, we introduce a <strong>chunkwise parallel</strong> variant of LION to balance recurrence and parallelism üôÇ.</li> </ul>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Explaining the full linear attention paradigm for bi-directional sequence modeling]]></summary></entry><entry><title type="html">LION ü¶Å Part II - Bi-directional RNN</title><link href="https://arshiaafzal.github.io/blog/2025/mamba2-part2-theory/" rel="alternate" type="text/html" title="LION ü¶Å Part II - Bi-directional RNN"/><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2025/mamba2-part2-theory</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2025/mamba2-part2-theory/"><![CDATA[<ol> <li><a href="/blog/2025/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li>Part II - Bi-directional RNN</li> <li><a href="/blog/2025/mamba2-part3-algorithm/">Part III - Chunkwise Parallel from of LION</a></li> <li><a href="/blog/2024/mamba2-part4-results/">Part IV - Results</a></li> </ol> <p>In <a href="/blog/2025/mamba2-part1-model/">Part I</a> of this series, we defined full linear attention with masking and scaling.<br/> Similar to all linear transformers designed for causal sequence modeling, we aim to derive an RNN form for efficiency during inference.<br/> In this section, we establish and theoretically demonstrate the equivalent bidirectional RNN for the Linear Transformer.</p> <h2 id="finding-bidirectional-rnn-equal-to-full-linear-attention">Finding Bidirectional RNN Equal to Full Linear Attention</h2> <p>Let‚Äôs start by separating the upper, lower, and diagonal elements of the attention matrix and the mask. Since the idea of a bidirectional RNN is to process the sequence in both the forward order (from first to last) and the reverse order (from last to first), these naturally correspond to the upper and lower parts of the attention matrix and mask.</p> <p>Ideally, we aim to construct an RNN that is equivalent to the masked and scaled Linear Attention. Let‚Äôs start by seperating upper and lower parts of the attention and mask:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_mask_color-480.webp 480w,/assets/img/att_mask_color-800.webp 800w,/assets/img/att_mask_color-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/att_mask_color.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p><strong>Note:</strong> We made a strong effort to maintain a consistent color coding for this section of the blog post and throughout our paper :).</p> <ul> <li>Wherever you see a <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Yellow</span> color, it indicates the <strong>upper part of the matrix (non-causal)</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Red</span> color, it represents the <strong>diagonal elements</strong>.</li> <li>Whenever you see a <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Blue</span> color, it corresponds to the <strong>lower triangular (causal) part</strong>.</li> </ul> <p>Let‚Äôs seperate the attention into upper and lower parts:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_sep-480.webp 480w,/assets/img/att_sep-800.webp 800w,/assets/img/att_sep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/att_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This formulation represents both the causal and non-causal forms of attention. Ideally, we aim to model each triangular part using an RNN.Similarly, we can also separate the mask in the same way:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/mask_sep-480.webp 480w,/assets/img/mask_sep-800.webp 800w,/assets/img/mask_sep-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/mask_sep.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Let‚Äôs also write the scaling part of the masked attention $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ as:</p> \[\begin{aligned} \mathbf{Y} = \big(\text{scale}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M})\big) \mathbf{V} = (\mathbf{C}^{-1}(\mathbf{Q}\mathbf{K}^{\top} \odot \mathbf{M}))\mathbf{V}, \hspace{1mm} \mathbf{C}_i = \mathbf{q}^{\top}_i\sum\limits_{j=1}^{L} \mathbf{M}_{ij}\mathbf{k}_j. \end{aligned}\] <p>Also, we can decompose the scaling matrix \(\mathbf{C}_i\) as:</p> \[\begin{aligned} \mathbf{C}_{i}= \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=1}^{i} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^F_i} + \underbrace{\mathbf{q}^{\top}_i\sum\nolimits_{j=i}^{L} \mathbf{M}_{ij}\mathbf{k}_j - \frac{1}{2} \mathbf{q}^{\top}_i\mathbf{k}_i}_{\mathbf{C}^B_i} \end{aligned}\] <p>Now we replace tha bove scaling matrix $\mathbf{C}$ in the output of the attention form of $\mathbf{Y} = \text{Scale}(\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M} ) \mathbf{V}$ .Interestingly, many terms naturally cancel out with each other.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/proofC-480.webp 480w,/assets/img/proofC-800.webp 800w,/assets/img/proofC-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/proofC.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>This results in only the forward and backward directions of the RNN remaining. As observed, the forward path aligns with causal linear attention with masking. Now, we need to demonstrate that the backward path follows the same RNN structure in the reverse direction. We can simply flip the upper triangular matrices using the <a href="https://en.wikipedia.org/wiki/Exchange_matrix">exchange matrix</a> \(\mathbf{J}_L\) and the function \(F(X) = \mathbf{J}_L X \mathbf{J}_L\):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/flip-480.webp 480w,/assets/img/flip-800.webp 800w,/assets/img/flip-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/flip.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>Cool! Now, both the upper part (equivalent to the RNN in the forward direction) and the lower part (equivalent to the RNN in the backward direction) can be formulated as RNNs. This is exactly what we need to construct our bidirectional RNN equivalent to full linear attention.</p> <blockquote class="block-tip"> <p><strong>LION: Reccurence form</strong></p> \[\begin{aligned} \mathbf{S}_i^{F/B} &amp;= \lambda_i \mathbf{S}^{F/B}_{i-1} + \mathbf{k}_i \mathbf{v}_i^{\top}, \\ \mathbf{z}^{F/B}_i &amp;= \lambda_i \mathbf{z}^{F/B}_{i-1} + \mathbf{k}_i, \\ c^{F/B}_i &amp; = \mathbf{q}_i^{\top} \mathbf{z}^{F/B}_{i} - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}, \\ \mathbf{y}^{F/B}_i &amp;= \mathbf{q}_i^{\top} \mathbf{S}^{F/B}_i - \frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2} \mathbf{v}_i, \\ out&amp;put: \mathbf{y}_i = \frac{\mathbf{y}^{F}_i + \mathbf{y}^{B}_i}{c^F_i + c^B_i}. \\ \end{aligned}\] </blockquote> <p>The terms $\frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}$ and $\frac{\mathbf{q}_i^{\top} \mathbf{k}_i}{2}$ are subtracted to avoid double counting. This bi-directional RNN is equivalent to scaled and masked linear attention described in previous section of this blogpost.</p> <h2 id="some-important-details-of-our-rnn">Some Important details of our RNN</h2> <blockquote> <p>Only the states \(c^{F/B}_i\) and \(\mathbf{y}^{F/B}_i\) are stored per token, resulting in \(\mathcal{O}(Ld)\) memory usage. In contrast, naively storing full matrix-valued hidden states would require \(\mathcal{O}(Ld^2)\), which becomes infeasible for large models.</p> </blockquote> <blockquote> <p>Forward and backward recurrences run independently, completing in \(L\) time steps with \(L\) memory units, compared to \(2L\) in the naive approach.</p> </blockquote> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/memory-480.webp 480w,/assets/img/memory-800.webp 800w,/assets/img/memory-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/memory.png" width="100%" height="auto" title="Memory Allocation of LION in RNN form" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Memory allocation in LION during Forward and Backward recurrences.</figcaption> </figure> <p>All in one we can visulaize our framework nicely like:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/frlion-480.webp 480w,/assets/img/frlion-800.webp 800w,/assets/img/frlion-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/frlion.png" width="100%" height="auto" title="LION" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">LION ü¶Å: Our framework for training in parallel using Full Linear Attention which also supports the efficient bi-directional RNN format.</figcaption> </figure> <h2 id="different-masks-of-lion">Different Masks of LION</h2> <p>Now that we have created our framework let‚Äôs see what are the choices of the decay factor \(\lambda_i\) and how they resemble the famous linear Transformer models. Let‚Äôs set:</p> <blockquote> <p>$\lambda_i=1$ this results in mighty simple Linear Transformer (cite) which we refrer to as <span style="background-color: rgb(230, 255, 230); padding: 3px; color:black">LION-üî• </span> which is LION-Lit resembling Linear Transformer.</p> </blockquote> <blockquote> <p>$\lambda_i=\lambda$ this results in mighty RetNet (cite) which we refrer to as <span style="background-color: rgb(229, 204, 230); padding: 3px; color:black">LION-D </span></p> </blockquote> <blockquote> <p>$\lambda_i=\sigma(\mathbf{W}\mathbf{x}_i)$ being input dependent, and bi-directional Linear Transformer inspired by selectivity of Mamba2 (cite) which we refrer to as <span style="background-color: rgb(255, 233, 211) ; padding: 3px; color:black">LION-S </span></p> </blockquote> <p>We evaluate all above models, extended to bidirectional sequence modeling using LION, on several bidirectional tasks. Also as all Linear Transformers use feature mapping $\phi(.)$ to queries and keys we also applied SILU shifted $\phi(x)=$ <code class="language-plaintext highlighter-rouge">(SILU(x)+0.5)/(norm(SILU(x)+0.5))</code> non-linear activation function. Let‚Äôs delve deep in each of these models in LION framework.</p> <h3 id="lion-">LION-üî•</h3> <p>LION-üî• is an extension of the very first Linear Transformer (cite). Without any masking, the bidirectional parallel form can be simply written as:</p> \[\mathbf{Y} = Scale(\mathbf{Q} \mathbf{K}^\top )\mathbf{V}\] <p>and the RNN form of the above parallel full linear attention is simply the RNN form mentioned above in this section in green box just by simply not using any mask.</p> <h3 id="lion-d">LION-D</h3> <p>By fixing \(\lambda_i = \lambda\), the mask \(\mathbf{M}\) has the form:</p> \[\begin{align} \mathbf{M}_{ij} = \lambda^{|i-j|}, \quad \mathbf{D}_{ij} = |i-j|\log(\lambda), \quad \mathbf{M} = \exp(\mathbf{D}). \notag \end{align}\] <p>\(\mathbf{M}\) above is a Toeplitz mask cite(tnn) and therefore, creating the decay mask can be made even faster using simple PyTorch commands. To ensure numerical stability, we bound the parameter \(\lambda\) using the <strong>sigmoid function</strong>, setting \(\lambda = \sigma(a)\). Without this constraint, the scalar \(\lambda^L\) could become excessively large, leading to instability. Additionally, as we all know, summation is generally more numerically stable than multiplication. Therefore, in some cases, instead of multiplying a matrix repeatedly, we can leverage summation for improved stability. However, in practice, for <strong>RetNet-style masks</strong> with a fixed decay, multiplication remains stable. This allows for a more straightforward implementation when generating the mask in code:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Decay_Mask</span><span class="p">(</span><span class="n">a</span> <span class="p">,</span> <span class="n">L</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="n">L</span><span class="p">))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s">LION-S</h3> <p>Observing the structure of $\mathbf{M}$, its upper ($\mathbf{M}^B$) and lower ($\mathbf{M}^F$) triangular parts are rank-1 <a href="https://people.cs.kuleuven.be/~raf.vandebril/homepage/publications/papers_html/qrq_07/node16.html">semi-separable matrices</a> (cite), allowing for efficient computation via matrix multiplications.</p> <p>During training, the decay factors $\lambda_i$ are stacked into ${\lambda}^F \in \mathbb{R}^L$, and the cumulative product</p> \[\mathbf{L}^F = cumprod(\lambda^F) = \prod_{k=0}^{i} \lambda^F_k\] <p>is used to generate the lower triangular mask (\mathbf{M}^F). For the upper triangular mask $\mathbf{M}^B$, the input sequence is flipped, and the decay factors are computed as</p> \[\boldsymbol{\lambda}^B = \text{Flip}(\boldsymbol{\lambda}^F), \quad \mathbf{L}^B = cumprod(\boldsymbol{\lambda}^B).\] <p>The masks are then constructed as, $\mathbf{M}^F =$ <code class="language-plaintext highlighter-rouge">tril(LF@inv(LF)^T)</code> for the forward part and $\mathbf{M}^B =$ <code class="language-plaintext highlighter-rouge">triu(LB@inv(LB)^T)</code> for the backward part. where <code class="language-plaintext highlighter-rouge">tril(.)</code> and <code class="language-plaintext highlighter-rouge">trilu(.)</code> extract the lower and upper triangular parts of the input matrix respectively. The full mask is then obtained as</p> \[\mathbf{M} = \mathbf{M}^F + \mathbf{M}^B - \mathbf{I}.\] <p>To improve numerical stability, the selective scalar $\lambda_i$ is designed in exponential form</p> \[\lambda_i = e^{a_i}.\] <p>This results in the cumulative sum:</p> \[\mathbf{D}^F_{ij} = \begin{cases} \sum_{k=i}^{j+1} a_k, &amp; \text{if } i &gt; j, \\ \sum_{k=i+1}^{j} a_k, &amp; \text{if } i &lt; j, \\ 0, &amp; \text{if } i = j, \end{cases}\] \[\mathbf{M^F} = \exp(\mathbf{D^F}),\] <p>where $\exp(\cdot)$ is applied element-wise. The same process applies to $\mathbf{M}^B$ by flipping the input sequence order.</p> <p>Here, $\mathbf{D}^{F/B} = cumsum(\mathbf{a}^{F/B})$, where $\mathbf{a} \in \mathbb{R}^L$ contains the selective exponents $a_i$.</p> <p>Ensuring stability is crucial, as $\mathbf{L}^{F/B}$ can overflow or underflow when forming the full mask without chunking. To mitigate this, we define</p> \[a_i = \log(\sigma(\mathbf{W}_{a}^\top\mathbf{x}_i + b)),\] <p>where $\sigma(.)$ is the sigmoid function. This approach ensures numerical stability by bounding $a_i$ within the interval $[0,1]$.</p> <p><strong>Note:</strong> It is crucial that the activation function is a <strong>sigmoid</strong>, as other activations do not produce stable masks and can lead to NaN values in the loss function. To maintain stability, <strong>chunking</strong> is required during training. This issue has been specifically highlighted in the <strong>Mamba2</strong> blog post.<br/> We provide a detailed explanation in the <strong>Results</strong> section of this blog post, where we discuss why using <strong>full attention</strong> is beneficial for achieving <strong>high throughput</strong> during training.</p> <p>The code for building the mask of LION-S is so simple and flexible even in Pytorch:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_causal_mask_lions</span><span class="p">(</span><span class="n">tensor</span><span class="p">):</span>
    <span class="n">cumsum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span> <span class="p">)</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">cumsum</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="p">,</span> <span class="mi">1</span><span class="o">/</span> <span class="p">(</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span> <span class="p">)</span>  <span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
</code></pre></div></div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Selective_Mask</span><span class="p">(</span><span class="n">vec</span><span class="p">):</span>
    <span class="n">vec_shape</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">vec</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="n">A_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">vec</span><span class="p">,</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span><span class="n">vec_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span><span class="mi">1</span><span class="p">),</span><span class="n">device</span><span class="o">=</span><span class="n">vec</span><span class="p">.</span><span class="n">device</span><span class="p">)),</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[:,:,</span><span class="mi">1</span><span class="p">:].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">).</span><span class="nf">squeeze</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">A_for</span> <span class="o">+</span> <span class="n">A_back</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">eye</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]).</span><span class="nf">to</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <h2 id="lion-attention-block">LION Attention Block</h2> <p>We can formulate the parallel attention form of LION as shown below, supporting all three extensions of our main experiments:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Class</span> <span class="nc">LION_Attention</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">qkv_bias</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">qk_scale</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">attn_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">proj_drop</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">silunorm</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">Mask_type</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">().</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="n">head_dim</span> <span class="o">=</span> <span class="n">dim</span> <span class="o">//</span> <span class="n">num_heads</span>
        <span class="n">self</span><span class="p">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">qk_scale</span> <span class="ow">or</span> <span class="n">head_dim</span> <span class="o">**</span> <span class="o">-</span><span class="mf">0.5</span>

        <span class="n">self</span><span class="p">.</span><span class="n">qkv</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span> <span class="o">*</span> <span class="mi">3</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">qkv_bias</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">attn_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">attn_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">proj_drop</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Dropout</span><span class="p">(</span><span class="n">proj_drop</span><span class="p">)</span>
        <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">=</span> <span class="n">Mask_type</span>

        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
            <span class="bp">None</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">self</span><span class="p">.</span><span class="n">a_i</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="n">num_heads</span><span class="p">))</span>

        <span class="n">self</span><span class="p">.</span><span class="n">non_lin</span> <span class="o">=</span> <span class="n">silu_shifted</span>
        <span class="n">self</span><span class="p">.</span><span class="n">silunorm</span> <span class="o">=</span> <span class="n">silunorm</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
        <span class="n">qkv</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">qkv</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">C</span> <span class="o">//</span> <span class="n">self</span><span class="p">.</span><span class="n">num_heads</span><span class="p">).</span><span class="nf">permute</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">)</span>
        <span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">self</span><span class="p">.</span><span class="nf">non_lin</span><span class="p">(</span><span class="n">qkv</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">silunorm</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">silunorm</span><span class="p">),</span> <span class="n">qkv</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>

        <span class="n">a_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">a_i</span><span class="p">(</span><span class="n">x</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">a_i</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Selective</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Selective_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Decay</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="nc">Decay_Mask</span><span class="p">(</span><span class="n">a_i</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">Mask_type</span> <span class="o">==</span> <span class="sh">'</span><span class="s">Lit</span><span class="sh">'</span><span class="p">:</span>
          <span class="n">M</span> <span class="o">=</span> <span class="mi">1</span>

        <span class="n">attn</span> <span class="o">=</span> <span class="p">(</span><span class="n">q</span> <span class="o">@</span> <span class="n">k</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="n">self</span><span class="p">.</span><span class="n">scale</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">M</span> <span class="o">*</span> <span class="n">attn</span>

        <span class="c1"># Scaling
</span>        <span class="n">attn</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">attn</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">attn</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attn</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">attn_drop</span><span class="p">(</span><span class="n">attn</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn</span> <span class="o">@</span> <span class="n">v</span><span class="p">).</span><span class="nf">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">).</span><span class="nf">reshape</span><span class="p">(</span><span class="n">B</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">)</span>

        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">proj_drop</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
</code></pre></div></div> <blockquote> <p><strong>Question:</strong> As seen above, the <strong>RNN</strong> is more efficient than the <strong>Transformer</strong> since it only requires storing the output for each token, resulting in a memory complexity of <strong>$\mathcal{O}(Ld)$</strong>, as opposed to storing the full attention matrix, which requires <strong>$\mathcal{O}(L^2 d)$</strong>. Can we achieve a balance between the speed of attention parallelism and the efficiency of an RNN?</p> </blockquote> <p>We will answer this question in our next section by introducing LION-Chunk.</p> <h2 id="next-up">Next Up</h2> <ul> <li> <p>In the next section of this series, we will describe how to apply a <strong>chunkwise parallel form</strong> for LION, allowing us to balance between the <em>RNN structure</em> and the <em>attention-based</em> formulation.</p> </li> <li> <p>We show the numercial results and experiments on Imagenet and C4 dataset :)</p> </li> </ul>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Deriving equivalent bidirectional RNN for linear Attention]]></summary></entry><entry><title type="html">LION ü¶Å Part III - Chunkwise Parallel from of LION</title><link href="https://arshiaafzal.github.io/blog/2025/mamba2-part3-algorithm/" rel="alternate" type="text/html" title="LION ü¶Å Part III - Chunkwise Parallel from of LION"/><published>2025-02-20T00:00:00+00:00</published><updated>2025-02-20T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2025/mamba2-part3-algorithm</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2025/mamba2-part3-algorithm/"><![CDATA[<ol> <li><a href="/blog/2025/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/blog/2025/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li>Part III - Chunkwise Parallel from of LION</li> <li><a href="/blog/2024/mamba2-part4-results/">Part IV - Results</a></li> </ol> <p>Since we have now established the LION theorem, which maps full linear attention into a bidirectional RNN in <a href="/blog/2025/mamba2-part2-theory/">Part II</a> of this series, a key question arises:</p> <p>Given that RNNs are efficient and attention is fast, can we strike a balance between them?</p> <p>For causal Transformers like DeltaNet and GLA, as well as the SSD algorithm in Mamba2, a chunkwise parallel form of full linear attention could be an effective solution. Additionally, in models like Hydra, this balance is achieved by applying two SSD algorithms. However, can we derive a unified framework for chunking full linear attention, particularly for LION-S and LION-D, where the decay factor is fixed and the mask $\mathbf{M}$ follows a Toeplitz structure? But it is important that this chunkwise form is particularly useful for <strong>inference</strong>, since during training, as we said, full linear attention will provide the highest throughput, especially for short sequences, which is the case for bidirectional tasks. The aim of chunking full attention in LION is to maintain a balance between efficiency and speed, particularly during inference. Since LION benefits from stable masks, it does not require chunking during training, unlike SSMs such as Hydra.</p> <h2 id="lion-chunk">LION-Chunk</h2> <p>As mentioned above, the main purpose of chunking is to balance the memory-speed tradeoff during inference. The key idea is that instead of processing the entire sequence of length $L$, we divide it into $N$ subsequences of length $C$, where $N \times C = L$.<br/> To achieve this, we start with the full linear attention formulation:</p> \[\mathbf{Y} = (\mathbf{Q} \mathbf{K}^\top \odot \mathbf{M}) \mathbf{V}\] <p>we first chunk the queries, keys and values into submatrices and chunks as followes</p> \[\mathbf{Q}_{[i]} , \mathbf{K}_{[i]}, \mathbf{V}_{[i]} \in \mathbb{R}^{C \times d}\] <p>Now, given the masked full linear attention in the form $ (\mathbf{A} \odot \mathbf{M})$ for each chunk of the mask and attention, we need to construct the chunkwise form, which consists of four parts:</p> <ul> <li>Chunkwise form of the attention matrix $\mathbf{A}_{[ij]}$</li> <li>Chunkwise form for the scaling matrix $\mathbf{C}$ which the chunkwise form can be written as $\mathbf{C}_{[ij]}$</li> <li>The chunked hidden state to shape the unscaled output $\mathbf{S}_{[ij-1]}$</li> <li>Finally the output of the chunk $i$ which is $\mathbf{Y}_{[i]}$</li> </ul> <p>using these chunked matrices we shape the full linear atteniton in chunk form as bellow:</p> <blockquote class="block-tip"> <p><strong>LION Chunk</strong></p> \[\begin{aligned} \mathbf{A}_{[ij]} &amp; = \mathbf{Q}_{[i]}\mathbf{K}_{[j]}^\top \odot \mathbf{M}_{[ij]}, \\ \mathbf{C}_{[ij]} &amp;= \mathbf{C}_{[ij-1]} + \text{Sum} (\mathbf{A}_{[ij]}), \\ \mathbf{S}_{[ij]} &amp; =\mathbf{S}_{[ij-1]} + \mathbf{A}_{[ij]} \mathbf{V}_{[j]} , \\ \mathbf{Y}_{[i]} &amp; = \frac{\mathbf{S}_{[iN]}}{\mathbf{C}_{[iN]}} \end{aligned}\] </blockquote> <p>where $\text{Sum}$ operations applies summation over the row of the input matrix. And $\mathbf{M}_{[ij]}$ corresponds to a submatrix of the full maks $\mathbf{M}$ at chunk $ij$ like:</p> \[\mathbf{M}_{[ij]} = \mathbf{M}_{iC+1:i(C+1),jC+1:j(C+1)} \in \mathbb{R}^{C \times C}.\] <p>Don‚Äôt be intimidated by the above formulation‚Äîit is actually quite intuitive when visualized. It clearly shows the steps taken to achieve the chunkwise form.</p> <p>Let‚Äôs start by chunking the attention matrix $\mathbf{A}$. To better understand this, let‚Äôs examine the full attention for a sequence of $L=9$ with $C=3$ chunk size in detail below:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/att_chunk-480.webp 480w,/assets/img/att_chunk-800.webp 800w,/assets/img/att_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/att_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As seen above, chunking simply involves computing the queries and keys for each boxed sub-matrix, as illustrated for the upper, lower, and diagonal chunks. For every attention matrix chunk $[ij]$, the computation follows the same pattern‚Äîmultiplying the corresponding queries and keys for that chunk.</p> <p>But does the same approach apply to selective and fixed masks?</p> <p>In reality, chunking the attention mask is slightly different and even more critical than chunking attention itself due to its unique structure. Below, we provide a detailed explanation of how to chunk the attention mask for LION-D and LION-S.</p> <p>üöÄ <strong>Note:</strong> Please pay close attention to this section, as the visualization and details of this part of chunking are not included in the paper.</p> <h3 id="lion-d-chunk">LION-D Chunk</h3> <p>Let‚Äôs start with the decay mask, as it is simpler and easier to visualize. For LION-D with a fixed mask, the final mask is a Toeplitz mask constructed using the scalar decay factor $\lambda$. Now, let‚Äôs examine how the mask is structured.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/maskdec_chunk-480.webp 480w,/assets/img/maskdec_chunk-800.webp 800w,/assets/img/maskdec_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/maskdec_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As seen, the full mask of LION-D (or full RetNet mask) is constructed simply by the submatrix of $\Gamma$, which is a Toeplitz matrix itself. Regardless of where the chunk is located, whether in the upper or lower part of the mask matrix $\mathbf{M}$, it retains the same property of being a fraction of the Toeplitz matrix $\Gamma$ as bellow:</p> \[\mathbf{M}_{[ij]} = \Gamma \lambda^{|i-j|}\] <p>which can simply be implemented in Pytorch.</p> <h3 id="the-code-for-lion-d-chunk-mask">The code for LION-D Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">Mask_Decay_Partial</span><span class="p">(</span><span class="n">a_i</span> <span class="p">,</span> <span class="n">L</span><span class="p">,</span><span class="n">start</span><span class="p">,</span><span class="n">end</span><span class="p">):</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">L</span><span class="p">,</span><span class="n">device</span><span class="o">=</span><span class="n">a_i</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">I</span><span class="p">,</span> <span class="n">J</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">meshgrid</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">],</span> <span class="n">indexing</span><span class="o">=</span><span class="sh">'</span><span class="s">ij</span><span class="sh">'</span><span class="p">)</span>
    <span class="n">E</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">abs</span><span class="p">((</span><span class="n">I</span><span class="o">-</span><span class="n">J</span><span class="p">)).</span><span class="nf">float</span><span class="p">().</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="n">L</span><span class="p">,</span><span class="nf">len</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="n">end</span><span class="p">])))</span>
    <span class="n">M</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span><span class="n">a_i</span><span class="p">).</span><span class="nf">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">)</span><span class="o">**</span><span class="n">E</span>
    <span class="k">return</span> <span class="n">M</span>
</code></pre></div></div> <h3 id="lion-s-chunk">LION-S Chunk</h3> <p>Despite LION-D the full mask of LION-S is more tricky since the upper lower and the diagonal part of the mask are shaped differently:</p> <ul> <li>The <span style="background-color: rgb(255, 248, 203); padding: 3px; color:black">Upper</span> part is influenced only by the decay factors applied from the end to the beginning of the sequence.</li> <li>The <span style="background-color: rgb(254, 200, 201); padding: 3px; color:black">Diagonal</span> part incorporates contributions from both directions, spanning from the start to the end and from the end to the start.</li> <li>The <span style="background-color: rgb(208, 243, 248); padding: 3px; color:black">Lower</span> part is influenced only by the decay factors applied from the beginning to the end of the sequence.</li> </ul> <p>Let‚Äôs visualize LION-S mask as well:</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/masksel_chunk-480.webp 480w,/assets/img/masksel_chunk-800.webp 800w,/assets/img/masksel_chunk-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/masksel_chunk.png" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>As seen above, the chunk 1,3, for example, has only the cumulative decay factors multiplied from the beginning up to the last three sequence elements, while the chunk 3,1 has only the decay factors multiplied from the end up to the first three sequence elements. And this is the reason for using the matrices $\mathbf{L}^F$ and $\mathbf{L}^B$ to compute the cumulative products of the decay factors, progressing from the beginning to the end of the sequence and in reverse which can be created simply by <code class="language-plaintext highlighter-rouge">L^F = cumprod(a)</code> and <code class="language-plaintext highlighter-rouge">L^B = cumprod(Flip(a))</code>.</p> <h3 id="the-code-for-lion-s-chunk-mask">The code for LION-S Chunk Mask</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">mask_forward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)[...,</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">:(</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tril</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">diagonal</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">mask_backward</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">cumprod</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">clamp</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="mf">1e-6</span><span class="p">)</span>
    <span class="n">A</span> <span class="o">=</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)[...,</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">:(</span><span class="n">chunk_index</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">,:]</span> <span class="o">/</span> <span class="n">cumprod</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">triu</span><span class="p">(</span><span class="n">A</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">2</span><span class="p">),</span><span class="n">diagonal</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">Mask_Selective_Partial</span><span class="p">(</span><span class="n">vec</span><span class="p">,</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">):</span>
    <span class="n">B</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">L</span> <span class="o">=</span> <span class="n">vec</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">A_for</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_forward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">)</span>
    <span class="n">A_back</span> <span class="o">=</span> <span class="nf">create_matrix_from_tensor_backward</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">((</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones_like</span><span class="p">(</span><span class="n">vec</span><span class="p">[...,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]),</span> <span class="n">vec</span><span class="p">[...,</span><span class="mi">1</span><span class="p">:]),</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span><span class="n">chunk_index</span><span class="p">,</span><span class="n">chunk_len</span><span class="p">)</span>
    <span class="n">I</span>  <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">diag_embed</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">((</span><span class="n">B</span><span class="p">,</span><span class="n">H</span><span class="p">,</span><span class="n">L</span><span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)),</span><span class="n">offset</span> <span class="o">=</span> <span class="o">-</span><span class="n">chunk_index</span><span class="o">*</span><span class="n">chunk_len</span><span class="p">)[...,:</span><span class="n">A_for</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">return</span> <span class="n">A_for</span> <span class="o">+</span> <span class="n">A_back</span> <span class="o">-</span> <span class="n">I</span><span class="p">.</span><span class="nf">to</span><span class="p">(</span><span class="n">A_for</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
</code></pre></div></div> <p>Now that we have all elemnts in place let‚Äôs see how these models are working in practice on real-world datasets for masked language modeling and image classification.</p> <h2 id="next-up">Next Up</h2> <p>In the <a href="/blog/2024/mamba2-part4-results/">final part of this series</a>, we present the advantages of using LION compared to other methods for training SSMs or Linear Transformers.</p> <p>We will present the trade-offs for different LION models and compare them with other well-known SSMs and Softmax Transformers.</p>]]></content><author><name>Arshia Afzal</name></author><summary type="html"><![CDATA[Explaining LION-Chunk for Balancing Memory-Speed Tradeoffs During Inference]]></summary></entry><entry><title type="html">LION ü¶Å Part IV - Results</title><link href="https://arshiaafzal.github.io/blog/2024/mamba2-part4-results/" rel="alternate" type="text/html" title="LION ü¶Å Part IV - Results"/><published>2024-05-31T00:00:00+00:00</published><updated>2024-05-31T00:00:00+00:00</updated><id>https://arshiaafzal.github.io/blog/2024/mamba2-part4-results</id><content type="html" xml:base="https://arshiaafzal.github.io/blog/2024/mamba2-part4-results/"><![CDATA[<ol> <li><a href="/blog/2025/mamba2-part1-model/">Part I - Full Linear Attention</a></li> <li><a href="/blog/2025/mamba2-part2-theory/">Part II - Bi-directional RNN</a></li> <li><a href="/blog/2025/mamba2-part3-algorithm/">Part III - Chunkwise Parallel from of LION</a></li> <li>Part IV - Results</li> </ol> <p>In this fourth part of our LION series, we will present and discuss a selection of experimental results across various domains, including vision tasks, masked language modeling (MLM), and different LION architectures. These results not only highlight LION‚Äôs versatility and efficiency across diverse applications but also serve as a preview of the comprehensive findings detailed in the full paper.</p> <h2 id="image-classification-performance-overview">Image Classification Performance Overview</h2> <h3 id="model-comparisons">Model Comparisons</h3> <p>We evaluated LION‚Äôs performance, efficiency, and training times against state-of-the-art SSMs and Transformers for image classification. The results demonstrate that LION achieves competitive performance while offering significant advantages in training speed and efficiency.</p> <table> <thead> <tr> <th>Model</th> <th>#Param</th> <th>Imagenet Top-1 Acc.</th> <th>Train. time</th> </tr> </thead> <tbody> <tr> <td>ViT</td> <td>86M</td> <td>$77.9$</td> <td>$\times 1$</td> </tr> <tr> <td>DeiT</td> <td>86M</td> <td>$\underline{81.8}$</td> <td>$\times 1$</td> </tr> <tr> <td>Hydra</td> <td>104M</td> <td>$81.0$</td> <td>$\times 2.51$</td> </tr> <tr> <td>Vim</td> <td>98M</td> <td>$\mathbf{81.9}$</td> <td>$\times 10.86$</td> </tr> <tr> <td>LION-lit</td> <td>86M</td> <td>$74.7$</td> <td>$\mathbf{\times 0.73}$</td> </tr> <tr> <td>LION-D</td> <td>86M</td> <td>$77.8$</td> <td>$\times \underline{1.39}$</td> </tr> <tr> <td>$LION-D^{\natural}$</td> <td>86M</td> <td>$80.2$</td> <td>$\times 1.48$</td> </tr> <tr> <td>$LION-S$</td> <td>86M</td> <td>$76.3$</td> <td>$\times 1.46$</td> </tr> <tr> <td>$LION-S^{\natural}$</td> <td>86M</td> <td>$79.9$</td> <td>$\times 1.68$</td> </tr> </tbody> </table> <p>As shown in the table above, LION models achieve competitive performance with vision-specific SSMs like Vim, while being significantly faster during training. LION-D performs comparably to Vim and surpasses Hydra, while training approximately 7x faster than Vim. Notably, LION-lit demonstrates the highest training speed across all models, showing that training with full linear attention is significantly faster than chunkwise parallel training (used in Hydra) and considerably faster than the scan algorithm, even with optimized GPU kernels (as used in Vim).</p> <h3 id="memory-efficiency">Memory Efficiency</h3> <p>The LION family demonstrates excellent memory efficiency across both vision and language tasks. Figure below shows inference memory usage with a batch size of 64 across different image resolutions, LION models maintain reasonable memory consumption even at high resolutions up to 2496 pixels, while adding minimal training overhead in BERT-style language modeling scenarios. In contrast, baseline models like ViT and DeiT run out of memory (OOM) at much lower resolutions, highlighting LION‚Äôs memory scaling capabilities regardless of the application domain.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig1_plot.svg" sizes="95vw"/> <img src="/assets/img/fig1_plot.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Memory Usage Comparison" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Memory usage during inference across different architectures with batch size 64. LION models maintain reasonable memory consumption at high resolutions while other models run out of memory.</figcaption> </figure> </div> </div> <h3 id="training-time-analysis">Training Time Analysis</h3> <p>The LION family demonstrates remarkable training efficiency across both vision and language tasks. As shown in the table below, LION variants add minimal training overhead compared to standard Transformers, with some variants even training faster.</p> <table class="table-caption"> <thead> <tr> <th>Task</th> <th>LION-lit</th> <th>LION-retnet</th> <th>LION-S</th> <th>Hydra</th> <th>Vim</th> </tr> </thead> <tbody> <tr> <td>Vision</td> <td>$\times 0.73$</td> <td>$\times 1.39$</td> <td>$\times 1.46$</td> <td>$\times 2.51$</td> <td>$\times 10.86$</td> </tr> <tr> <td>MLM</td> <td>$\times 0.95$</td> <td>$\times 1.10$</td> <td>$\times 1.32$</td> <td>$\times 3.13$</td> <td>‚úó</td> </tr> </tbody> </table> <p><em>Training Times (relative to Transformer) ‚Üì</em></p> <p>For vision tasks, LION-lit achieves remarkable speed, training 27% faster than standard Transformers. Even the more complex LION variants maintain competitive training times, with LION-retnet and LION-S training only ~1.4x slower than Transformers. This is significantly better than competing approaches like Hydra (2.51x slower) and Vim (10.86x slower).</p> <p>In MLM tasks, the efficiency gains are even more pronounced. LION-lit nearly matches Transformer training speed at just 0.95x, while LION-retnet adds only 10% overhead. Even LION-S remains efficient at 1.32x. All LION variants significantly outperform Hydra‚Äôs 3.13x slowdown, while Vim is not applicable to MLM tasks (marked as ‚úó).</p> <h2 id="mlm-results">MLM Results</h2> <p>For masked language modeling (MLM) tasks, we evaluated LION models against BERT and Hydra on both MLM pretraining and GLUE benchmark finetuning. The results show that LION variants achieve competitive performance while maintaining excellent training efficiency.</p> <table class="table-caption"> <thead> <tr> <th>Model</th> <th>MLM Acc.</th> <th>GLUE</th> <th>Train. time</th> </tr> </thead> <tbody> <tr> <td>BERT</td> <td>$\underline{69.88}$</td> <td>$\mathbf{82.95}$</td> <td>$\times 1$</td> </tr> <tr> <td>Hydra</td> <td>$\mathbf{71.18}$</td> <td>$\underline{81.77}$</td> <td>$\times 3.13$</td> </tr> <tr> <td>LION-lit</td> <td>$67.11$</td> <td>$80.76$</td> <td>$\times \mathbf{0.95}$</td> </tr> <tr> <td>LION-retnet</td> <td>$68.64$</td> <td>$81.34$</td> <td>$\times \underline{1.10}$</td> </tr> <tr> <td>LION-S</td> <td>$69.16$</td> <td>$81.58$</td> <td>$\times 1.32$</td> </tr> </tbody> </table> <p><em>C4 MLM and GLUE results for the LARGE scale ($334$M). For each dataset, the best and second best results are highlighted with bold and underline respectively.</em></p> <h2 id="lion-architecture-variants-and-trade-offs">LION Architecture Variants and Trade-offs</h2> <p>Let‚Äôs explore how different LION variants handle the trade-off between memory usage and inference speed. We‚Äôll look at three key approaches:</p> <ol> <li>Full Linear Attention - The standard approach using the full attention matrix</li> <li>Bidirectional RNN - Our memory-efficient RNN formulation</li> <li>LION Chunk - A balanced approach using chunked computation</li> </ol> <h3 id="memory-vs-speed-trade-offs">Memory vs Speed Trade-offs</h3> <p>The first plot below shows how these approaches compare in terms of memory efficiency and inference speed. The RNN approach proves to be the most memory-efficient, while full attention uses the most memory. LION chunk provides a nice middle ground - it uses less memory than full attention while actually achieving faster inference speeds than both alternatives. This makes it particularly attractive when you need to balance performance with resource constraints.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fig3_plot.svg" sizes="95vw"/> <img src="/assets/img/fig3_plot.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Impact of Chunk Size" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Analysis of how chunk size affects model performance across different LION variants.</figcaption> </figure> </div> </div> <h3 id="detailed-performance-analysis">Detailed Performance Analysis</h3> <p>Looking more closely at the memory-time trade-off across different LION variants, we can see some interesting patterns. While RNN remains the most memory-efficient across all models, both chunking and full attention hit memory limits much sooner. The chunking approach matches or beats full attention‚Äôs inference speed for simpler variants like LION-RetNet. However, with more complex variants like LION-S, chunking is only faster at lower resolutions - at higher resolutions, the overhead from mask calculations starts to slow it down.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/linear_chunking.svg" sizes="95vw"/> <img src="/assets/img/linear_chunking.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Linear Chunking Analysis" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Evaluation of linear chunking strategies and their impact on model efficiency.</figcaption> </figure> </div> </div> <h3 id="selective-chunking-results">Selective Chunking Results</h3> <p>The final analysis examines how different chunking strategies perform across sequence lengths. This helps inform which approach is best for different scenarios - chunking tends to be optimal for LION-lit and LION-RetNet when memory allows, while RNN can be preferable for handling complex masks at high resolutions.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/selective_chunking.svg" sizes="95vw"/> <img src="/assets/img/selective_chunking.svg" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="Selective Chunking Analysis" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Performance comparison of selective chunking approaches across different sequence lengths.</figcaption> </figure> </div> </div>]]></content><author><name>Arshia Afzal</name></author><category term="math"/><category term="ai"/><summary type="html"><![CDATA[Comprehensive results of LION on Vision, MLM and LION variants]]></summary></entry></feed>